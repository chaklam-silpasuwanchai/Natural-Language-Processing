{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Natural Language Processing\n",
    "\n",
    "## Part 3: SpaCy + Custom Pipeline + Regex\n",
    "\n",
    "Here, let's see how we can create some custom pipeline.  We shall also visit Regex."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Custom Pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "87dfacd9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy\n",
    "from spacy.language import Language"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "4f856104",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Doc length: 3\n"
     ]
    }
   ],
   "source": [
    "# Create the nlp object\n",
    "nlp = spacy.load('en_core_web_sm')\n",
    "\n",
    "# Define a custom component\n",
    "@Language.component(\"show_length\")\n",
    "def show_length(doc):\n",
    "\n",
    "    # Print the doc's length\n",
    "    print('Doc length:', len(doc))\n",
    "\n",
    "    # Return the doc object\n",
    "    return doc\n",
    "\n",
    "# Add the component first in the pipeline\n",
    "nlp.add_pipe(\"show_length\", first=True)\n",
    "\n",
    "# Process a text\n",
    "doc = nlp(\"Hello world!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0d924208",
   "metadata": {},
   "source": [
    "Let's try another one"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "8af8f908",
   "metadata": {},
   "outputs": [],
   "source": [
    "nlp = spacy.load(\"en_core_web_sm\")\n",
    "doc = nlp(\"Britain is a place.  Mary is a doctor.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "2073e569",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Britain GPE\n",
      "Mary PERSON\n"
     ]
    }
   ],
   "source": [
    "for ent in doc.ents:\n",
    "    print (ent.text, ent.label_)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "56fa3ddc",
   "metadata": {},
   "source": [
    "Let's create a sample pipe that remove all `GPE` from our ents.\n",
    "\n",
    "I know this does not make sense, but only for the sake of simplicity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "37f0822b",
   "metadata": {},
   "outputs": [],
   "source": [
    "@Language.component(\"remove_gpe\")\n",
    "def remove_gpe(doc):\n",
    "    original_ents = list(doc.ents)  #convert generator to list\n",
    "    for ent in doc.ents:\n",
    "        if ent.label_ == \"GPE\":\n",
    "            original_ents.remove(ent)\n",
    "    doc.ents = original_ents\n",
    "    return doc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "b213d744",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<function __main__.remove_gpe(doc)>"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#add this\n",
    "nlp.add_pipe(\"remove_gpe\", after=\"ner\")  #you don't need \"after\"...this is just for the sake of demonstration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "ed7152db",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mary PERSON\n"
     ]
    }
   ],
   "source": [
    "doc = nlp(\"Britain is a place.  Mary is a doctor.\")\n",
    "for ent in doc.ents:\n",
    "    print (ent.text, ent.label_)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5c60897f",
   "metadata": {},
   "source": [
    "### A more complex one"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "c292d817",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Golden Retriever, cat, turtle, Rattus norvegicus]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "animals = [\"Golden Retriever\", \"cat\", \"turtle\", \"Rattus norvegicus\"]\n",
    "list(nlp.pipe(animals))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "d4854610",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "animal_patterns: [Golden Retriever, cat, turtle, Rattus norvegicus]\n",
      "['tok2vec', 'tagger', 'parser', 'attribute_ruler', 'lemmatizer', 'ner', 'animal_component']\n",
      "[('cat', 'ANIMAL'), ('Golden Retriever', 'ANIMAL')]\n"
     ]
    }
   ],
   "source": [
    "import spacy\n",
    "from spacy.matcher import PhraseMatcher\n",
    "from spacy.tokens import Span\n",
    "\n",
    "nlp = spacy.load(\"en_core_web_sm\")\n",
    "animals = [\"Golden Retriever\", \"cat\", \"turtle\", \"Rattus norvegicus\"]\n",
    "animal_patterns = list(nlp.pipe(animals))\n",
    "print(\"animal_patterns:\", animal_patterns)\n",
    "matcher = PhraseMatcher(nlp.vocab)\n",
    "matcher.add(\"ANIMAL\", None, *animal_patterns)\n",
    "\n",
    "@Language.component(\"animal_component\")\n",
    "# Define the custom component\n",
    "def animal_component(doc):\n",
    "    # Apply the matcher to the doc\n",
    "    matches = matcher(doc)\n",
    "    # Create a Span for each match and assign the label 'ANIMAL'\n",
    "    spans = [Span(doc, start, end, label=\"ANIMAL\") for match_id, start, end in matches]\n",
    "    # Overwrite the doc.ents with the matched spans\n",
    "    doc.ents = spans\n",
    "    return doc\n",
    "\n",
    "\n",
    "# Add the component to the pipeline after the 'ner' component\n",
    "nlp.add_pipe(\"animal_component\", after=\"ner\")\n",
    "print(nlp.pipe_names)\n",
    "\n",
    "# Process the text and print the text and label for the doc.ents\n",
    "doc = nlp(\"I have a cat and a Golden Retriever\")\n",
    "print([(ent.text, ent.label_) for ent in doc.ents])"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "a57cadfc",
   "metadata": {},
   "source": [
    "## 2. RegEx\n",
    "\n",
    "**Strengths**:  good for complex syntax; fast, and universally supported\n",
    "\n",
    "**Weakness**:   it's not easy!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "a3fcda2f",
   "metadata": {},
   "outputs": [],
   "source": [
    "#import the (famous) regular expression library\n",
    "import re"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5b13378b",
   "metadata": {},
   "source": [
    "Now that we have it imported, we can begin to write out some RegEx rules. Let's say we want to find an occurrence of a date in a text. As noted in an earlier notebook, there are a finite number of ways this can be represented. Let's try to grab all instances of a day followed by a month first."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "6a613212",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('2 February', '2', 'February'), ('14 August', '4', 'August')]\n"
     ]
    }
   ],
   "source": [
    "#(\\d) means any digit (0-9)\n",
    "#{1, 2} means 1 or 2 times\n",
    "#| means or\n",
    "pattern = r\"((\\d){1,2} (January|February|March|April|May|June|July|August|September|October|November|December))\"\n",
    "\n",
    "text = \"This is a date 2 February. Another date would be 14 August.\"\n",
    "matches = re.findall(pattern, text)\n",
    "print (matches) #return individual and groups as well..."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "c3bde781",
   "metadata": {},
   "source": [
    "`findall` gives you a list of matches.  Let's use `finditer` which return an iterator that we can loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "67660edc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<callable_iterator object at 0x13af48e50>\n",
      "<re.Match object; span=(15, 25), match='2 February'>\n",
      "<re.Match object; span=(49, 58), match='14 August'>\n"
     ]
    }
   ],
   "source": [
    "text = \"This is a date 2 February. Another date would be 14 August.\"\n",
    "iter_matches = re.finditer(pattern, text)\n",
    "print (iter_matches)\n",
    "for hit in iter_matches:\n",
    "    print (hit)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "acd6677a",
   "metadata": {},
   "source": [
    "Within each of these is some very salient information, such as the start and end location (inside the span) and the text itself (match). We can use the start and end location to grab the text within the string."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "eccdda1a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2 February\n",
      "14 August\n"
     ]
    }
   ],
   "source": [
    "text = \"This is a date 2 February. Another date would be 14 August.\"\n",
    "iter_matches = re.finditer(pattern, text)\n",
    "for hit in iter_matches:\n",
    "    start = hit.start()\n",
    "    end = hit.end()\n",
    "    print (text[start:end])"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "a517482a",
   "metadata": {},
   "source": [
    "### RegEx + spaCy"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "2520434e",
   "metadata": {},
   "source": [
    "spaCy has easy ways to implement RegEx in three pipes: Matcher, PhraseMatcher, and EntityRuler. One of the major drawbacks to the Matcher and PhraseMatcher, is that they do not align the matches as `doc.ents`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "949a85e3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "555-5555 PHONE_NUMBER\n"
     ]
    }
   ],
   "source": [
    "import spacy\n",
    "\n",
    "#Sample text\n",
    "text = \"This is a sample number 555-5555.\"\n",
    "\n",
    "#Build upon the spaCy Small Model\n",
    "nlp = spacy.blank(\"en\")\n",
    "\n",
    "#Create the Ruler and Add it\n",
    "ruler = nlp.add_pipe(\"entity_ruler\")\n",
    "\n",
    "#List of Entities and Patterns (source: https://spacy.io/usage/rule-based-matching)\n",
    "patterns = [\n",
    "                {\"label\": \"PHONE_NUMBER\", \"pattern\": [{\"SHAPE\": \"ddd\"},\n",
    "                {\"ORTH\": \"-\", \"OP\": \"?\"}, {\"SHAPE\": \"dddd\"}]}\n",
    "            ]\n",
    "#add patterns to ruler\n",
    "ruler.add_patterns(patterns)\n",
    "\n",
    "#create the doc\n",
    "doc = nlp(text)\n",
    "\n",
    "#extract entities\n",
    "for ent in doc.ents:\n",
    "    print (ent.text, ent.label_)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d5fab88b",
   "metadata": {},
   "source": [
    "This method worked well for grabbing the phone number. But what if we wanted to use RegEx as opposed to linguistic features, such as shape? First, let's write some RegEx to capturee 555-5555."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "72c0698f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('555-5555', '5', '5')]\n"
     ]
    }
   ],
   "source": [
    "pattern = r\"((\\d){3}-(\\d){4})\"\n",
    "text = \"This is a sample number 555-5555.\"\n",
    "matches = re.findall(pattern, text)\n",
    "print (matches)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4739a2d0",
   "metadata": {},
   "source": [
    "Okay. So, now we know that we have a RegEx pattern that works. Let's try and implement it in the spaCy EntityRuler. We can do that with the code below. When we execute the code below, we have no output."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "e328ac82",
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy\n",
    "\n",
    "text = \"This is a sample number (555) 555-5555.\"\n",
    "\n",
    "#Build upon the spaCy small model\n",
    "nlp = spacy.blank(\"en\")\n",
    "\n",
    "#Create the ruler and add it\n",
    "ruler = nlp.add_pipe(\"entity_ruler\")\n",
    "\n",
    "#List of Entities and Patterns (source: https://spacy.io/usage/rule-based-matching)\n",
    "patterns = [\n",
    "                {\n",
    "                    \"label\": \"PHONE_NUMBER\", \n",
    "                    \"pattern\": [{\"TEXT\": {\"REGEX\": \"((\\d){3}-(\\d){4})\"}}]\n",
    "                }\n",
    "            ]\n",
    "#add patterns to ruler\n",
    "ruler.add_patterns(patterns)\n",
    "\n",
    "#create the doc\n",
    "doc = nlp(text)\n",
    "\n",
    "#extract entities\n",
    "for ent in doc.ents:\n",
    "    print (ent.text, ent.label_)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fde6930b",
   "metadata": {},
   "source": [
    "This is for one very important reason. SpaCy's EntityRuler cannot use RegEx to pattern match across tokens. The dash in the phone number throws off the EntityRuler. So, what are we to do in this scenario? Well, we have a few different options that we will explore in the next notebook. But before we get to that, let's try and use RegEx to capture the phone number with no hyphen."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "876f28e1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5555555 PHONE_NUMBER\n"
     ]
    }
   ],
   "source": [
    "import spacy\n",
    "\n",
    "#Sample text\n",
    "text = \"This is a sample number 5555555.\"\n",
    "\n",
    "#Build upon the spaCy small model\n",
    "nlp = spacy.blank(\"en\")\n",
    "\n",
    "#add the pipe\n",
    "ruler = nlp.add_pipe(\"entity_ruler\")\n",
    "\n",
    "#List of Entities and Patterns (source: https://spacy.io/usage/rule-based-matching)\n",
    "patterns = [\n",
    "                {\n",
    "                    \"label\": \"PHONE_NUMBER\", \n",
    "                    \"pattern\": [{\"TEXT\": {\"REGEX\": \"((\\d){7})\"}}]\n",
    "                }\n",
    "            ]\n",
    "#add patterns to ruler\n",
    "ruler.add_patterns(patterns)\n",
    "\n",
    "\n",
    "#create the doc\n",
    "doc = nlp(text)\n",
    "\n",
    "#extract entities\n",
    "for ent in doc.ents:\n",
    "    print (ent.text, ent.label_)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "d19f0bbc",
   "metadata": {},
   "source": [
    "This is rather silly that SpaCy cannot do this....let's do a more elegant way...using `Span`"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "6d6a7b59",
   "metadata": {},
   "source": [
    "## 3. Span"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "ddf63627",
   "metadata": {},
   "source": [
    "wWe are going to try and grab a multi-word token whose first name begins with Paul."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "bd4118f4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<re.Match object; span=(0, 11), match='Paul Newman'>\n",
      "<re.Match object; span=(39, 53), match='Paul Hollywood'>\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "\n",
    "text = \"Paul Newman was an American actor, but Paul Hollywood is a British TV Host. The name Paul is quite common.\"\n",
    "\n",
    "#w+ --> another word\n",
    "pattern = r\"Paul [A-Z]\\w+\"\n",
    "\n",
    "matches = re.finditer(pattern, text)\n",
    "\n",
    "for match in matches:\n",
    "    print (match)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "6249e36f",
   "metadata": {},
   "source": [
    "### Reconstruct Spans\n",
    "\n",
    "We can use spaCy `Span` to contain the results return by `re`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "a1df3908",
   "metadata": {},
   "outputs": [],
   "source": [
    "from spacy.tokens import Span"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c8fe3216",
   "metadata": {},
   "source": [
    "Here, we will create a blank spaCy English model and create the doc object of the text. It will have no entities in it because we are working with a blank model that does not have an \"ner\" component."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "cb3b2086",
   "metadata": {},
   "outputs": [],
   "source": [
    "nlp = spacy.blank(\"en\")\n",
    "doc = nlp(text)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fecb45a1",
   "metadata": {},
   "source": [
    "Even though this part is unnecessary, it is good to do it here because in other situations you will have entities. If you do, you need to store them as a separate list to which we will append things."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "a3ef31c7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[]"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "original_ents = list(doc.ents)\n",
    "original_ents"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "c2a8c2dc",
   "metadata": {},
   "source": [
    "Now, let's iterate over the results from `re.finditer()`. In this cell, we are going to grab the start and end from each match. we will then create a temporary span that will be equal to where the characters start and end in the doc object. This is important because tokens and characters do not always align correctly. Finally, we append to `my_ents`, the `start`, `end`, and `text`. The `text` is not necessary but it will help with debugging."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "bcaf984c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(0, 2, 'Paul Newman'), (8, 10, 'Paul Hollywood')]"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "my_ents = []\n",
    "for match in re.finditer(pattern, doc.text):\n",
    "    start, end = match.span()\n",
    "    span = doc.char_span(start, end)\n",
    "    if span is not None:\n",
    "        my_ents.append((span.start, span.end, span.text))\n",
    "        \n",
    "my_ents"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "09c44705",
   "metadata": {},
   "source": [
    "### Inject the Spans into the `doc.ents`"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "39a957e8",
   "metadata": {},
   "source": [
    "With that data, we can iterate over each entity and identify where it begins and ends in spaCy. Note, we are using the spaCy Span class. This allows us to create a span object and assign it a custom label. With this data, we can append each Span to original_ents."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "4ab0f747",
   "metadata": {},
   "outputs": [],
   "source": [
    "for ent in my_ents:\n",
    "    start, end, name = ent\n",
    "    per_ent = Span(doc, start, end, label=\"PERSON\")\n",
    "    original_ents.append(per_ent)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "b3ae4703",
   "metadata": {},
   "source": [
    "And finally, we set `doc.ents` equal to `original_ents`. This effectively loads the spans back into the spaCy `doc.ents`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "ddc73958",
   "metadata": {},
   "outputs": [],
   "source": [
    "doc.ents = original_ents"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1eb1efb4",
   "metadata": {},
   "source": [
    "Let's iterate over the ents as we normally would."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "edaa343a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Paul Newman PERSON\n",
      "Paul Hollywood PERSON\n"
     ]
    }
   ],
   "source": [
    "for ent in doc.ents:\n",
    "    print (ent.text, ent.label_)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "55a5e06e",
   "metadata": {},
   "source": [
    "Note that these are now properly identified entities in our `doc.ents` class.\n",
    "\n",
    "The next thing we want is to create a custom pipeline of this....how to do....very simple!  Just copy everything...."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "de1975f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "from spacy.language import Language\n",
    "\n",
    "@Language.component(\"paul_ner\")\n",
    "def paul_ner(doc):\n",
    "    pattern = r\"Paul [A-Z]\\w+\"\n",
    "    nlp = spacy.blank(\"en\")\n",
    "    doc = nlp(text)\n",
    "    original_ents = list(doc.ents)\n",
    "    my_ents = []\n",
    "    for match in re.finditer(pattern, doc.text):\n",
    "        start, end = match.span()\n",
    "        span = doc.char_span(start, end)\n",
    "        if span is not None:\n",
    "            my_ents.append((span.start, span.end, span.text))\n",
    "    for ent in my_ents:\n",
    "        start, end, name = ent\n",
    "        per_ent = Span(doc, start, end, label=\"PERSON\")\n",
    "        original_ents.append(per_ent)\n",
    "    doc.ents = original_ents\n",
    "    return doc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "c7a6830f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(Paul Newman, Paul Hollywood)\n"
     ]
    }
   ],
   "source": [
    "nlp = spacy.blank(\"en\")\n",
    "nlp.add_pipe(\"paul_ner\")\n",
    "doc = nlp(text)\n",
    "print(doc.ents)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "9afe79d2",
   "metadata": {},
   "source": [
    "## 4. Give priority to longer spans"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "20b20b72",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Paul Newman PERSON\n",
      "American NORP\n",
      "Paul Hollywood PERSON\n",
      "British NORP\n"
     ]
    }
   ],
   "source": [
    "text = \"Paul Newman was an American actor, but Paul Hollywood is a British TV Host.\"\n",
    "\n",
    "nlp = spacy.load(\"en_core_web_sm\")\n",
    "\n",
    "doc = nlp(text)\n",
    "for ent in doc.ents:\n",
    "    print (ent.text, ent.label_)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "5da7ae66",
   "metadata": {},
   "source": [
    "Let’s say that we create a new entity. Maybe words associated with Cinema. So, we want to classify Hollywood as a tag “CINEMA”. Now, in the above text, Hollywood is clearly associated with Paul Hollywood, but let’s imagine for a moment that it is not. Let’s try and run the same code as above. If we do, we notice that we get an error."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "b203b999",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<re.Match object; span=(44, 53), match='Hollywood'>\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "[E1010] Unable to set entity information for token 9 which is included in more than one span in entities, blocked, missing or outside.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-29-ef250c71a507>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     13\u001b[0m     \u001b[0moriginal_ents\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mper_ent\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     14\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 15\u001b[0;31m \u001b[0mdoc\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0ments\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0moriginal_ents\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m~/DSAI/Environments/teaching_env/lib/python3.8/site-packages/spacy/tokens/doc.pyx\u001b[0m in \u001b[0;36mspacy.tokens.doc.Doc.ents.__set__\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;32m~/DSAI/Environments/teaching_env/lib/python3.8/site-packages/spacy/tokens/doc.pyx\u001b[0m in \u001b[0;36mspacy.tokens.doc.Doc.set_ents\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: [E1010] Unable to set entity information for token 9 which is included in more than one span in entities, blocked, missing or outside."
     ]
    }
   ],
   "source": [
    "my_ents = []\n",
    "pattern = r\"Hollywood\"\n",
    "original_ents = list(doc.ents)\n",
    "for match in re.finditer(pattern, doc.text):\n",
    "    print (match)\n",
    "    start, end = match.span()\n",
    "    span = doc.char_span(start, end)\n",
    "    if span is not None:\n",
    "        my_ents.append((span.start, span.end, span.text))\n",
    "for ent in my_ents:\n",
    "    start, end, name = ent\n",
    "    per_ent = Span(doc, start, end, label=\"CINEMA\")\n",
    "    original_ents.append(per_ent)\n",
    "\n",
    "doc.ents = original_ents"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "0e6e25ba",
   "metadata": {},
   "source": [
    "This error tells us that one of our tokens from the `finditer()` overlapped with one that our `ner` component found. This is a problem that can be rectified with spaCy’s `filter_spans`. This gives primacy to **longer spans**. Notice how we have allowed the Paul Hollywood entity to be a PERSON, rather than CINEMA. This is because Hollywood is shorter than Paul Hollywood."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "231535f8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Paul Newman, American, Paul Hollywood, British, Hollywood]"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "original_ents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "3fdcafb7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Paul Newman PERSON\n",
      "American NORP\n",
      "Paul Hollywood PERSON\n",
      "British NORP\n"
     ]
    }
   ],
   "source": [
    "from spacy.util import filter_spans\n",
    "filtered = filter_spans(original_ents)\n",
    "doc.ents = filtered\n",
    "for ent in doc.ents:\n",
    "    print (ent.text, ent.label_)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.8.6 ('teaching_env')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.6"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "becc4c8e5ad229b2591d820334d85e3db0111492344629bf57f272470dce75a5"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
