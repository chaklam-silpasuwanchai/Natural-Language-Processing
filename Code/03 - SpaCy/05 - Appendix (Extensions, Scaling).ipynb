{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "1d4fcc43",
   "metadata": {},
   "source": [
    "## Appendix - Extension Attributes\n",
    "\n",
    "Custom attributes let you add any meta data to Docs, Tokens and Spans. The data can be added once, or it can be computed dynamically.\n",
    "\n",
    "Custom attributes are available via the dot-underscore property. This makes it clear that they were added by the user, and not built into spaCy, like token dot text.\n",
    "\n",
    "Attributes need to be registered on the global Doc, Token and Span classes you can import from spacy dot tokens. You've already worked with those in the previous chapters. To register a custom attribute on the Doc, Token or Span, you can use the set extension method.\n",
    "\n",
    "The first argument is the attribute name. Keyword arguments let you define how the value should be computed. In this case, it has a default value and can be overwritten."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8c9823be",
   "metadata": {},
   "source": [
    "Registered on the global Doc, Token or Span using the set_extension method"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy\n",
    "nlp = spacy.blank(\"en\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "bca13b2d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import global classes\n",
    "from spacy.tokens import Doc, Token, Span\n",
    "\n",
    "# Set extensions on the Doc, Token and Span\n",
    "Doc.set_extension('title', default=None)\n",
    "Token.set_extension('is_color', default=False)\n",
    "Span.set_extension('has_color', default=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "48362367",
   "metadata": {},
   "source": [
    "There are three types of extensions: \n",
    "    * attribute extensions, \n",
    "    * property extensions and \n",
    "    * method extensions."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "98166b65",
   "metadata": {},
   "source": [
    "### Attribute extensions\n",
    "\n",
    "Attribute extensions set a default value that can be overwritten.\n",
    "\n",
    "For example, a custom \"is color\" attribute on the token that defaults to False.\n",
    "\n",
    "On individual tokens, its value can be changed by overwriting it – in this case, True for the token \"blue\"."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "3bddea42",
   "metadata": {},
   "outputs": [],
   "source": [
    "from spacy.tokens import Token\n",
    "\n",
    "# Set extension on the Token with default value\n",
    "Token.set_extension('is_color', default=False, force=True)\n",
    "\n",
    "doc = nlp(\"The sky is blue.\")\n",
    "\n",
    "# Overwrite extension attribute value\n",
    "doc[3]._.is_color = True"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1776191b",
   "metadata": {},
   "source": [
    "### Property extensions\n",
    "\n",
    "* Define a getter and an optional setter function\n",
    "* Getter only called when you retrieve the attribute value\n",
    "\n",
    "Property extensions work like properties in Python: they can define a getter function and an optional setter.\n",
    "\n",
    "The getter function is only called when you retrieve the attribute. This lets you compute the value dynamically, and even take other custom attributes into account.\n",
    "\n",
    "Getter functions take one argument: the object, in this case, the token. In this example, the function returns whether the token text is in our list of colors.\n",
    "\n",
    "We can then provide the function via the getter keyword argument when we register the extension.\n",
    "\n",
    "The token \"blue\" now returns True for \"is color\".\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "05d503e0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True - blue\n"
     ]
    }
   ],
   "source": [
    "from spacy.tokens import Token\n",
    "\n",
    "# Define getter function\n",
    "def get_is_color(token):\n",
    "    colors = ['red', 'yellow', 'blue']\n",
    "    return token.text in colors\n",
    "\n",
    "# Set extension on the Token with getter\n",
    "Token.set_extension('is_color', getter=get_is_color, force=True)\n",
    "\n",
    "doc = nlp(\"The sky is blue.\")\n",
    "print(doc[3]._.is_color, '-', doc[3].text)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2abf9f29",
   "metadata": {},
   "source": [
    "* Span extensions should almost always use a getter\n",
    "\n",
    "If you want to set extension attributes on a Span, you almost always want to use a property extension with a getter. Otherwise, you'd have to update every possible span ever by hand to set all the values.\n",
    "\n",
    "In this example, the \"get has color\" function takes the span and returns whether the text of any of the tokens is in the list of colors.\n",
    "\n",
    "After we've processed the doc, we can check different slices of the doc and the custom \"has color\" property returns whether the span contains a color token or not."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "9f638b2c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True - sky is blue\n",
      "False - The sky\n"
     ]
    }
   ],
   "source": [
    "from spacy.tokens import Span\n",
    "\n",
    "# Define getter function\n",
    "def get_has_color(span):\n",
    "    colors = ['red', 'yellow', 'blue']\n",
    "    return any(token.text in colors for token in span)\n",
    "\n",
    "# Set extension on the Span with getter\n",
    "Span.set_extension('has_color', getter=get_has_color, force=True)\n",
    "\n",
    "doc = nlp(\"The sky is blue.\")\n",
    "print(doc[1:4]._.has_color, '-', doc[1:4].text)\n",
    "print(doc[0:2]._.has_color, '-', doc[0:2].text)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f15f020d",
   "metadata": {},
   "source": [
    "### Method extensions\n",
    "\n",
    "* Assign a function that becomes available as an object method\n",
    "* Lets you pass arguments to the extension function\n",
    "\n",
    "Method extensions make the extension attribute a callable method.\n",
    "\n",
    "You can then pass one or more arguments to it, and compute attribute values dynamically – for example, based on a certain argument or setting.\n",
    "\n",
    "In this example, the method function checks whether the doc contains a token with a given text. The first argument of the method is always the object itself – in this case, the Doc. It's passed in automatically when the method is called. All other function arguments will be arguments on the method extension. In this case, \"token text\".\n",
    "\n",
    "Here, the custom \"has token\" method returns True for the word \"blue\" and False for the word \"cloud\"."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "1a84d2a2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True - blue\n",
      "False - cloud\n"
     ]
    }
   ],
   "source": [
    "from spacy.tokens import Doc\n",
    "\n",
    "# Define method with arguments\n",
    "def has_token(doc, token_text):\n",
    "    in_doc = token_text in [token.text for token in doc]\n",
    "    return in_doc\n",
    "\n",
    "# Set extension on the Doc with method\n",
    "Doc.set_extension('has_token', method=has_token)\n",
    "\n",
    "doc = nlp(\"The sky is blue.\")\n",
    "print(doc._.has_token('blue'), '- blue')\n",
    "print(doc._.has_token('cloud'), '- cloud')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a3136119",
   "metadata": {},
   "source": [
    "More example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "9f7085f8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('I', False), ('live', False), ('in', False), ('Spain', True), ('.', False)]\n"
     ]
    }
   ],
   "source": [
    "from spacy.lang.en import English\n",
    "from spacy.tokens import Token\n",
    "\n",
    "nlp = English()\n",
    "\n",
    "# Register the Token extension attribute 'is_country' with the default value False\n",
    "Token.set_extension(\"is_country\", default=False)\n",
    "\n",
    "# Process the text and set the is_country attribute to True for the token \"Spain\"\n",
    "doc = nlp(\"I live in Spain.\")\n",
    "doc[3]._.is_country = True\n",
    "\n",
    "# Print the token text and the is_country attribute for all tokens\n",
    "print([(token.text, token._.is_country) for token in doc])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c71b1957",
   "metadata": {},
   "source": [
    "Another example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "5fc26d3b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "reversed: llA\n",
      "reversed: snoitazilareneg\n",
      "reversed: era\n",
      "reversed: eslaf\n",
      "reversed: ,\n",
      "reversed: gnidulcni\n",
      "reversed: siht\n",
      "reversed: eno\n",
      "reversed: .\n"
     ]
    }
   ],
   "source": [
    "from spacy.lang.en import English\n",
    "from spacy.tokens import Token\n",
    "\n",
    "nlp = English()\n",
    "\n",
    "# Define the getter function that takes a token and returns its reversed text\n",
    "def get_reversed(token):\n",
    "    return token.text[::-1]\n",
    "\n",
    "# Register the Token property extension 'reversed' with the getter get_reversed\n",
    "Token.set_extension(\"reversed\", getter=get_reversed)\n",
    "\n",
    "# Process the text and print the reversed attribute for each token\n",
    "doc = nlp(\"All generalizations are false, including this one.\")\n",
    "for token in doc:\n",
    "    print(\"reversed:\", token._.reversed)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "9e4be387",
   "metadata": {},
   "source": [
    "## Appendix - Scaling and performance\n",
    "\n",
    "### Processing large volumes of text\n",
    "\n",
    "* Use `nlp.pipe` method\n",
    "* Processes texts as a stream, yields Doc objects\n",
    "* Much faster than calling nlp on each text\n",
    "\n",
    "**BAD**:\n",
    "\n",
    "`docs = [nlp(text) for text in LOTS_OF_TEXTS]`\n",
    "\n",
    "**GOOD**:\n",
    "\n",
    "`docs = list(nlp.pipe(LOTS_OF_TEXTS))`\n",
    "\n",
    "If you need to process a lot of texts and create a lot of Doc objects in a row, the `nlp.pipe` method can speed this up significantly.\n",
    "\n",
    "It processes the texts as a stream and yields Doc objects.\n",
    "\n",
    "It is much faster than just calling nlp on each text, because it batches up the texts.\n",
    "\n",
    "`nlp.pipe` is a generator that yields Doc objects, so in order to get a list of Docs, remember to call the list method around it.\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "83e4f122",
   "metadata": {},
   "source": [
    "### Passing in context\n",
    "\n",
    "* Setting `as_tuples=True` on `nlp.pipe` lets you pass in `(text, context)` tuples\n",
    "* Yields `(doc, context)` tuples\n",
    "* Useful for associating metadata with the doc\n",
    "\n",
    "This is useful for passing in additional metadata, like an ID associated with the text, or a page number."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "cea71888",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "This is a text 15\n",
      "And another text 16\n"
     ]
    }
   ],
   "source": [
    "data = [\n",
    "    ('This is a text', {'id': 1, 'page_number': 15}),\n",
    "    ('And another text', {'id': 2, 'page_number': 16}),\n",
    "]\n",
    "\n",
    "for doc, context in nlp.pipe(data, as_tuples=True):\n",
    "    print(doc.text, context['page_number'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ee0e3678",
   "metadata": {},
   "source": [
    "You can even add the context meta data to custom attributes.\n",
    "\n",
    "In this example, we're registering two extensions, \"id\" and \"page number\", which default to None.\n",
    "\n",
    "After processing the text and passing through the context, we can overwrite the doc extensions with our context metadata."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "29fd8cea",
   "metadata": {},
   "outputs": [],
   "source": [
    "from spacy.tokens import Doc\n",
    "\n",
    "Doc.set_extension('id', default=None)\n",
    "Doc.set_extension('page_number', default=None)\n",
    "\n",
    "data = [\n",
    "    ('This is a text', {'id': 1, 'page_number': 15}),\n",
    "    ('And another text', {'id': 2, 'page_number': 16}),\n",
    "]\n",
    "\n",
    "for doc, context in nlp.pipe(data, as_tuples=True):\n",
    "    doc._.id = context['id']\n",
    "    doc._.page_number = context['page_number']"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "8fbf3e88",
   "metadata": {},
   "source": [
    "### Using only the tokenizer\n",
    "\n",
    "Another common scenario: Sometimes you already have a model loaded to do other processing, but you only need the tokenizer for one particular text.\n",
    "\n",
    "Running the whole pipeline is unnecessarily slow, because you'll be getting a bunch of predictions from the model that you don't need.\n",
    "\n",
    "If you only need a tokenized Doc object, you can use the `nlp.make doc` method instead, which takes a text and returns a Doc before the pipeline components are called.\n",
    "\n",
    "**BAD**:\n",
    "\n",
    "`doc = nlp(\"Hello world\")`\n",
    "\n",
    "**GOOD**:\n",
    "\n",
    "`doc = nlp.make_doc(\"Hello world!\")`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "5bdf399b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hello\n",
      "world\n",
      "!\n"
     ]
    }
   ],
   "source": [
    "doc = nlp.make_doc(\"Hello world!\")\n",
    "for token in doc:\n",
    "    print(token)  #if you only need tokenizer"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c48d890c",
   "metadata": {},
   "source": [
    "### Disabling pipeline components\n",
    "\n",
    "spaCy also allows you to temporarily disable pipeline components using the nlp dot disable pipes context manager.\n",
    "\n",
    "It takes a variable number of arguments, the string names of the pipeline components to disable. For example, if you only want to use the entity recognizer to process a document, you can temporarily disable the tagger and parser.\n",
    "\n",
    "After the with block, the disabled pipeline components are automatically restored.\n",
    "\n",
    "In the with block, spaCy will only run the remaining components.\n",
    "\n",
    "* Use nlp.disable_pipes to temporarily disable one or more pipes\n",
    "* Restores them after the with block\n",
    "* Only runs the remaining components"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "082d7950",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(Peter,)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/chaklam/DSAI/Environments/teaching_env/lib/python3.8/site-packages/spacy/pipeline/lemmatizer.py:211: UserWarning: [W108] The rule-based lemmatizer did not find POS annotation for one or more tokens. Check that your pipeline includes components that assign token.pos, typically 'tagger'+'attribute_ruler' or 'morphologizer'.\n",
      "  warnings.warn(Warnings.W108)\n"
     ]
    }
   ],
   "source": [
    "# Disable tagger and parser\n",
    "nlp = spacy.load(\"en_core_web_md\")\n",
    "\n",
    "text = \"Peter loves food and swimming.\"\n",
    "with nlp.disable_pipes('tagger', 'parser'):\n",
    "    # Process the text and print the entities\n",
    "    doc = nlp(text)\n",
    "    print(doc.ents)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.8.6 ('teaching_env')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.6"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "becc4c8e5ad229b2591d820334d85e3db0111492344629bf57f272470dce75a5"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
