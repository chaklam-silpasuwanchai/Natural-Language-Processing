{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Natural Language Processing\n",
    "\n",
    "## Part 4: SpaCy + Training Neural Network\n",
    "\n",
    "We'll write our own training loop from scratch, and understand the basics of how training works, along with tips and tricks that can make your custom NLP projects more successful.\n",
    "\n",
    "Why updating the model?\n",
    "\n",
    "* Better results on your specific domain\n",
    "* Learn classification schemes specifically for your problem\n",
    "* Essential for text classification\n",
    "* Very useful for named entity recognition\n",
    "* Less critical for part-of-speech tagging and dependency parsing\n",
    "\n",
    "### How training works\n",
    "\n",
    "1. Initialize the model weights randomly\n",
    "2. Predict a few examples with the current weights\n",
    "3. Compare prediction with true labels\n",
    "4. Calculate how to change weights to improve predictions\n",
    "5. Update weights slightly\n",
    "6. Go back to 2\n",
    "\n",
    "### Training the entity recognizer\n",
    "\n",
    "Let's look at an example for a specific component: the entity recognizer.\n",
    "\n",
    "The entity recognizer takes a document and predicts phrases and their labels. This means that the training data needs to include texts, the entities they contain, and the entity labels.\n",
    "\n",
    "Entities can't overlap, so each token can only be part of one entity.\n",
    "\n",
    "Because the entity recognizer predicts entities in context, it also needs to be trained on entities and their surrounding context."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "from spacy.tokens import Span\n",
    "\n",
    "doc = nlp(\"iPhone X is coming\")\n",
    "doc.ents = [Span(doc, 0, 2, label=\"GADGET\")]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It's also very important for the model to learn words that aren't entities."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "doc = nlp(\"I need a new phone! Any tips?\")\n",
    "doc.ents = []"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Creating training data\n",
    "\n",
    "The training data tells the model what we want it to predict. This could be texts and named entities we want to recognize, or tokens and their correct part-of-speech tags.  To update an existing model, we can start with a few hundred to a few thousand examples.  To train a new category we may need up to a million.  spaCy's pre-trained English models for instance were trained on **2 million words!** labelled with part-of-speech tags, dependencies and named entities.\n",
    "\n",
    "Training data is usually created by humans who assign labels to texts.  This is a lot of work, but can be semi-automated – for example, using spaCy's `Matcher`\n",
    "\n",
    "spaCy’s rule-based `Matcher`  is a great way to quickly create training data for named entity models. A list of sentences is available as the variable *text*.  We want to find all mentions of different iPhone models, so we can create training data to teach a model to recognize them as 'GADGET'."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "How to preorder the iPhone X\n",
      "[iPhone X]\n",
      "iPhone X is coming\n",
      "[iPhone X]\n",
      "Should I pay $1,000 for the iPhone X?\n",
      "[iPhone X]\n",
      "The iPhone 8 reviews are here\n",
      "[iPhone 8]\n",
      "iPhone 11 vs iPhone 8: What's the difference?\n",
      "[iPhone 11, iPhone 8]\n",
      "I need a new phone! Any tips?\n",
      "[]\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "import spacy\n",
    "from spacy.matcher import Matcher\n",
    "from spacy.tokens import Span\n",
    "\n",
    "with open(\"data/iphone.json\", encoding=\"utf8\") as f:\n",
    "    text = json.loads(f.read())\n",
    "\n",
    "nlp = spacy.blank(\"en\")\n",
    "matcher = Matcher(nlp.vocab)\n",
    "\n",
    "# Two tokens whose lowercase forms match \"iphone\" and \"x\"\n",
    "pattern1 = [{\"LOWER\": \"iphone\"}, {\"LOWER\": \"x\"}]\n",
    "\n",
    "# Token whose lowercase form matches \"iphone\" and a digit\n",
    "pattern2 = [{\"LOWER\": \"iphone\"}, {\"IS_DIGIT\": True}]\n",
    "\n",
    "# Add patterns to the matcher and create docs with matched entities\n",
    "matcher.add(\"GADGET\", [pattern1, pattern2])\n",
    "docs = []\n",
    "for doc in nlp.pipe(text):\n",
    "    print(doc)\n",
    "    matches = matcher(doc)\n",
    "    spans = [Span(doc, start, end, label=match_id) for match_id, start, end in matches]\n",
    "    print(spans)\n",
    "    doc.ents = spans\n",
    "    docs.append(doc)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After creating the data for our corpus, we need to save it out to a `.spacy` file.\n",
    "\n",
    "Instantiate the `DocBin` with the list of docs.  A container to efficiently store and save `Doc` objects.\n",
    "\n",
    "Save the `DocBin` to a file called `train.spacy` and `dev.spacy`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "from spacy.tokens import DocBin\n",
    "\n",
    "train_docs = docs[:len(docs) // 2]\n",
    "dev_docs   = docs[len(docs) // 2:]\n",
    "\n",
    "train_doc_bin = DocBin(docs=train_docs)\n",
    "train_doc_bin.to_disk(\"docs/train.spacy\")\n",
    "\n",
    "dev_doc_bin = DocBin(docs=dev_docs)\n",
    "dev_doc_bin.to_disk(\"docs/dev.spacy\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Configuring\n",
    "\n",
    "Training in SpaCy is very simple.  All you need is `config.cfg` and the training/dev data."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Generating a config\n",
    "\n",
    "- spaCy can auto-generate a default config file for you\n",
    "- interactive quickstart widget (https://spacy.io/usage/training#quickstart) in the docs\n",
    "- `init config` command on the CLI\n",
    "    \n",
    "    python -m spacy init config configs/config.cfg --lang en --pipeline ner\n",
    "\n",
    "Note: `configs` is the root folder I created; you can use whatever you want.  Similarly, you can name `config.cfg` to whatever you want, e.g., `my_config.cfg`\n",
    "\n",
    "Note: `--lang`: language class of the pipeline, e.g. en for English; \n",
    "      `--pipeline`: comma-separated names of components to include\n",
    "      \n",
    "Note: Because we’re executing the command in a Jupyter environment in this course, we’re using the prefix !. If you’re running the command in your local terminal, you can leave this out."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[38;5;3m⚠ To generate a more effective transformer-based config (GPU-only),\n",
      "install the spacy-transformers package and re-run this command. The config\n",
      "generated now does not use transformers.\u001b[0m\n",
      "\u001b[38;5;4mℹ Generated config template specific for your use case\u001b[0m\n",
      "- Language: en\n",
      "- Pipeline: ner\n",
      "- Optimize for: efficiency\n",
      "- Hardware: CPU\n",
      "- Transformer: None\n",
      "\u001b[38;5;2m✔ Auto-filled config with all values\u001b[0m\n",
      "\u001b[38;5;2m✔ Saved config\u001b[0m\n",
      "configs/config.cfg\n",
      "You can now add your data and train your pipeline:\n",
      "python -m spacy train config.cfg --paths.train ./train.spacy --paths.dev ./dev.spacy\n"
     ]
    }
   ],
   "source": [
    "!python -m spacy init config configs/config.cfg --lang en --pipeline ner"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let’s take a look at the config spaCy just generated! You can run the command below to print the config to the terminal and inspect it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[paths]\n",
      "train = null\n",
      "dev = null\n",
      "vectors = null\n",
      "init_tok2vec = null\n",
      "\n",
      "[system]\n",
      "gpu_allocator = null\n",
      "seed = 0\n",
      "\n",
      "[nlp]\n",
      "lang = \"en\"\n",
      "pipeline = [\"tok2vec\",\"ner\"]\n",
      "batch_size = 1000\n",
      "disabled = []\n",
      "before_creation = null\n",
      "after_creation = null\n",
      "after_pipeline_creation = null\n",
      "tokenizer = {\"@tokenizers\":\"spacy.Tokenizer.v1\"}\n",
      "\n",
      "[components]\n",
      "\n",
      "[components.ner]\n",
      "factory = \"ner\"\n",
      "incorrect_spans_key = null\n",
      "moves = null\n",
      "scorer = {\"@scorers\":\"spacy.ner_scorer.v1\"}\n",
      "update_with_oracle_cut_size = 100\n",
      "\n",
      "[components.ner.model]\n",
      "@architectures = \"spacy.TransitionBasedParser.v2\"\n",
      "state_type = \"ner\"\n",
      "extra_state_tokens = false\n",
      "hidden_width = 64\n",
      "maxout_pieces = 2\n",
      "use_upper = true\n",
      "nO = null\n",
      "\n",
      "[components.ner.model.tok2vec]\n",
      "@architectures = \"spacy.Tok2VecListener.v1\"\n",
      "width = ${components.tok2vec.model.encode.width}\n",
      "upstream = \"*\"\n",
      "\n",
      "[components.tok2vec]\n",
      "factory = \"tok2vec\"\n",
      "\n",
      "[components.tok2vec.model]\n",
      "@architectures = \"spacy.Tok2Vec.v2\"\n",
      "\n",
      "[components.tok2vec.model.embed]\n",
      "@architectures = \"spacy.MultiHashEmbed.v2\"\n",
      "width = ${components.tok2vec.model.encode.width}\n",
      "attrs = [\"NORM\",\"PREFIX\",\"SUFFIX\",\"SHAPE\"]\n",
      "rows = [5000,1000,2500,2500]\n",
      "include_static_vectors = false\n",
      "\n",
      "[components.tok2vec.model.encode]\n",
      "@architectures = \"spacy.MaxoutWindowEncoder.v2\"\n",
      "width = 96\n",
      "depth = 4\n",
      "window_size = 1\n",
      "maxout_pieces = 3\n",
      "\n",
      "[corpora]\n",
      "\n",
      "[corpora.dev]\n",
      "@readers = \"spacy.Corpus.v1\"\n",
      "path = ${paths.dev}\n",
      "max_length = 0\n",
      "gold_preproc = false\n",
      "limit = 0\n",
      "augmenter = null\n",
      "\n",
      "[corpora.train]\n",
      "@readers = \"spacy.Corpus.v1\"\n",
      "path = ${paths.train}\n",
      "max_length = 0\n",
      "gold_preproc = false\n",
      "limit = 0\n",
      "augmenter = null\n",
      "\n",
      "[training]\n",
      "dev_corpus = \"corpora.dev\"\n",
      "train_corpus = \"corpora.train\"\n",
      "seed = ${system.seed}\n",
      "gpu_allocator = ${system.gpu_allocator}\n",
      "dropout = 0.1\n",
      "accumulate_gradient = 1\n",
      "patience = 1600\n",
      "max_epochs = 0\n",
      "max_steps = 20000\n",
      "eval_frequency = 200\n",
      "frozen_components = []\n",
      "annotating_components = []\n",
      "before_to_disk = null\n",
      "\n",
      "[training.batcher]\n",
      "@batchers = \"spacy.batch_by_words.v1\"\n",
      "discard_oversize = false\n",
      "tolerance = 0.2\n",
      "get_length = null\n",
      "\n",
      "[training.batcher.size]\n",
      "@schedules = \"compounding.v1\"\n",
      "start = 100\n",
      "stop = 1000\n",
      "compound = 1.001\n",
      "t = 0.0\n",
      "\n",
      "[training.logger]\n",
      "@loggers = \"spacy.ConsoleLogger.v1\"\n",
      "progress_bar = false\n",
      "\n",
      "[training.optimizer]\n",
      "@optimizers = \"Adam.v1\"\n",
      "beta1 = 0.9\n",
      "beta2 = 0.999\n",
      "L2_is_weight_decay = true\n",
      "L2 = 0.01\n",
      "grad_clip = 1.0\n",
      "use_averages = false\n",
      "eps = 0.00000001\n",
      "learn_rate = 0.001\n",
      "\n",
      "[training.score_weights]\n",
      "ents_f = 1.0\n",
      "ents_p = 0.0\n",
      "ents_r = 0.0\n",
      "ents_per_type = null\n",
      "\n",
      "[pretraining]\n",
      "\n",
      "[initialize]\n",
      "vectors = ${paths.vectors}\n",
      "init_tok2vec = ${paths.init_tok2vec}\n",
      "vocab_data = null\n",
      "lookups = null\n",
      "before_init = null\n",
      "after_init = null\n",
      "\n",
      "[initialize.components]\n",
      "\n",
      "[initialize.tokenizer]"
     ]
    }
   ],
   "source": [
    "!cat configs/config.cfg"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Training the pipeline\n",
    "\n",
    "Training can be done via command line as code below\n",
    "\n",
    "    python -m spacy train ./config.cfg --output ./output --paths.train train.spacy --paths.dev dev.spacy\n",
    "\n",
    "Note: `train`: the command to run; `config.cfg`: the path to the config file; \n",
    "      `--output`: the path to the output directory to save the trained pipeline; \n",
    "      `--paths.train`: override with path to the training data, \n",
    "      `--paths.dev`: override with path to the evaluation data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[38;5;2m✔ Created output directory: output\u001b[0m\n",
      "\u001b[38;5;4mℹ Saving to output directory: output\u001b[0m\n",
      "\u001b[38;5;4mℹ Using CPU\u001b[0m\n",
      "\u001b[1m\n",
      "=========================== Initializing pipeline ===========================\u001b[0m\n",
      "[2022-12-14 07:37:21,815] [INFO] Set up nlp object from config\n",
      "[2022-12-14 07:37:21,826] [INFO] Pipeline: ['tok2vec', 'ner']\n",
      "[2022-12-14 07:37:21,830] [INFO] Created vocabulary\n",
      "[2022-12-14 07:37:21,831] [INFO] Finished initializing nlp object\n",
      "[2022-12-14 07:37:21,912] [INFO] Initialized pipeline components: ['tok2vec', 'ner']\n",
      "\u001b[38;5;2m✔ Initialized pipeline\u001b[0m\n",
      "\u001b[1m\n",
      "============================= Training pipeline =============================\u001b[0m\n",
      "\u001b[38;5;4mℹ Pipeline: ['tok2vec', 'ner']\u001b[0m\n",
      "\u001b[38;5;4mℹ Initial learn rate: 0.001\u001b[0m\n",
      "E    #       LOSS TOK2VEC  LOSS NER  ENTS_F  ENTS_P  ENTS_R  SCORE \n",
      "---  ------  ------------  --------  ------  ------  ------  ------\n",
      "  0       0          0.00     11.17    0.00    0.00    0.00    0.00\n",
      "200     200          0.95    139.37  100.00  100.00  100.00    1.00\n",
      "400     400          0.00      0.00  100.00  100.00  100.00    1.00\n",
      "600     600          0.00      0.00  100.00  100.00  100.00    1.00\n",
      "800     800          0.00      0.00  100.00  100.00  100.00    1.00\n",
      "1000    1000          0.00      0.00  100.00  100.00  100.00    1.00\n",
      "1200    1200          0.00      0.00  100.00  100.00  100.00    1.00\n",
      "1400    1400          0.00      0.00  100.00  100.00  100.00    1.00\n",
      "1600    1600          0.00      0.00  100.00  100.00  100.00    1.00\n",
      "1800    1800          0.00      0.00  100.00  100.00  100.00    1.00\n",
      "\u001b[38;5;2m✔ Saved pipeline to output directory\u001b[0m\n",
      "output/model-last\n"
     ]
    }
   ],
   "source": [
    "!python -m spacy train configs/config.cfg --output ./output --paths.train docs/train.spacy --paths.dev docs/dev.spacy"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Loading the pipeline\n",
    "\n",
    "Output after training is a regular loadable spaCy pipeline where two pipelines are returned: `model-last`: last trained pipeline and `model-best`: best trained pipeline.   We can load this with `spacy.load`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(iPhone 11, iPhone 8)\n"
     ]
    }
   ],
   "source": [
    "import spacy\n",
    "\n",
    "nlp = spacy.load(\"output/model-best\")\n",
    "doc = nlp(\"iPhone 11 vs iPhone 8: What's the difference?\")\n",
    "print(doc.ents)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Packaging the pipeline\n",
    "\n",
    "`SpaCy package`: create an installable Python package containing your pipeline; easy to version and deploy\n",
    "\n",
    "    python -m spacy package /path/to/output/model-best ./packages --name my_pipeline --version 1.0.0\n",
    "\n",
    "To install simply:\n",
    "\n",
    "    cd ./packages/en_my_pipeline-1.0.0\n",
    "    pip install dist/en_my_pipeline-1.0.0.tar.gz\n",
    "\n",
    "Then load it and use\n",
    "\n",
    "    nlp = spacy.load(\"en_my_pipeline\")\n",
    "\n",
    "Since I don't want to do it, I will skip this part."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Appendix - Training best practices"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When you start running your own experiments, you might find that a lot of things just don't work the way you want them to. And that's okay.\n",
    "\n",
    "Training models is an iterative process, and you have to try different things until you find out what works best.\n",
    "\n",
    "Here we will be sharing some best practices and things to keep in mind when training your own models.\n",
    "\n",
    "Let's take a look at some of the problems you may come across."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Problem 1: Models can \"forget\" things\n",
    "\n",
    "* Existing model can overfit on new data\n",
    "e.g.: if you only update it with WEBSITE, it can \"unlearn\" what a PERSON is\n",
    "\n",
    "* Also known as \"catastrophic forgetting\" problem"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Solution 1: Mix in previously correct predictions\n",
    "\n",
    "To prevent this, make sure to always mix in examples of what the model previously got correct.\n",
    "\n",
    "If you're training a new category \"website\", also include examples of \"person\"."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Problem 2: Models can't learn everything\n",
    "\n",
    "Another common problem is that your model just won't learn what you want it to.\n",
    "\n",
    "spaCy's models make predictions based on the local context – for example, for named entities, the surrounding words are most important.\n",
    "\n",
    "If the decision is difficult to make based on the context, the model can struggle to learn it.\n",
    "\n",
    "The label scheme also needs to be consistent and not too specific.\n",
    "\n",
    "For example, it may be very difficult to teach a model to predict whether something is adult clothing or children's clothing based on the context. However, just predicting the label \"clothing\" may work better."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Solution 2: Plan your label scheme carefully\n",
    "\n",
    "Before you start training and updating models, it's worth taking a step back and planning your label scheme.\n",
    "\n",
    "Try to pick categories that are reflected in the local context and make them more generic if possible.\n",
    "\n",
    "You can always add a **rule-based system** (or `EntityRuler`) later to go from generic to specific.\n",
    "\n",
    "**BAD:**\n",
    "\n",
    "LABELS = ['ADULT_SHOES', 'CHILDRENS_SHOES', 'BANDS_I_LIKE]\n",
    "          \n",
    "**GOOD:**\n",
    "\n",
    "LABELS = ['CLOTHING', 'BAND']"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Appendix - Good data vs. bad data\n",
    "\n",
    "Here’s an excerpt from a training set that labels the entity type TOURIST_DESTINATION in traveler reviews."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "doc1 = nlp(\"i went to amsterdem last year and the canals were beautiful\")\n",
    "doc1.ents = [Span(doc1, 3, 4, label=\"TOURIST_DESTINATION\")]\n",
    "\n",
    "doc2 = nlp(\"You should visit Paris once, but the Eiffel Tower is kinda boring\")\n",
    "doc2.ents = [Span(doc2, 3, 4, label=\"TOURIST_DESTINATION\")]\n",
    "\n",
    "doc3 = nlp(\"There's also a Paris in Arkansas, lol\")\n",
    "doc3.ents = []\n",
    "\n",
    "doc4 = nlp(\"Berlin is perfect for summer holiday: great nightlife and cheap beer!\")\n",
    "doc4.ents = [Span(doc4, 0, 1, label=\"TOURIST_DESTINATION\")]"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Why is this data and label scheme problematic? Whether a place is a tourist destination is a **subjective judgement** and not a definitive category. It will be very difficult for the entity recognizer to learn.\n",
    "\n",
    "A much better approach would be to only label `GPE` (geopolitical entity) or LOCATION and then use a **rule-based system** to determine whether the entity is a tourist destination in this context. For example, you could resolve the entities types back to a knowledge base or look them up in a travel wiki."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here's a much better code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy\n",
    "from spacy.tokens import Span\n",
    "\n",
    "nlp = spacy.blank(\"en\")\n",
    "\n",
    "doc1 = nlp(\"i went to amsterdem last year and the canals were beautiful\")\n",
    "doc1.ents = [Span(doc1, 3, 4, label=\"GPE\")]\n",
    "\n",
    "doc2 = nlp(\"You should visit Paris once, but the Eiffel Tower is kinda boring\")\n",
    "doc2.ents = [Span(doc2, 3, 4, label=\"GPE\")]\n",
    "\n",
    "doc3 = nlp(\"There's also a Paris in Arkansas, lol\")\n",
    "doc3.ents = [Span(doc3, 4, 5, label=\"GPE\"), Span(doc3, 6, 7, label=\"GPE\")]\n",
    "\n",
    "doc4 = nlp(\"Berlin is perfect for summer holiday: great nightlife and cheap beer!\")\n",
    "doc4.ents = [Span(doc4, 0, 1, label=\"GPE\")]"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Appendix - Training multiple labels\n",
    "\n",
    "Here’s a small sample of a dataset created to train a new entity type WEBSITE. The original dataset contains a few thousand sentences. In this exercise, you’ll be doing the labeling by hand. In real life, you probably want to automate this and use an annotation tool – for example, **Brat**, a popular open-source solution, or **Prodigy**, our own annotation tool that integrates with spaCy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy\n",
    "from spacy.tokens import Span\n",
    "\n",
    "nlp = spacy.blank(\"en\")\n",
    "\n",
    "doc1 = nlp(\"Reddit partners with Patreon to help creators build communities\")\n",
    "doc1.ents = [\n",
    "    Span(doc1, 0, 1, label=\"WEBSITE\"),\n",
    "    Span(doc1, 3, 4, label=\"WEBSITE\"),\n",
    "]\n",
    "\n",
    "doc2 = nlp(\"PewDiePie smashes YouTube record\")\n",
    "doc2.ents = [Span(doc2, 2, 3, label=\"WEBSITE\")]\n",
    "\n",
    "doc3 = nlp(\"Reddit founder Alexis Ohanian gave away two Metallica tickets to fans\")\n",
    "doc3.ents = [Span(doc3, 0, 1, label=\"WEBSITE\")]"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A model was trained with the data you just labelled, plus a few thousand similar examples. After training, it’s doing great on WEBSITE, but doesn’t recognize PERSON anymore. Why could this be happening?\n",
    "\n",
    "That is because the training data included no examples of PERSON, so the model learned that this label is incorrect.\n",
    "\n",
    "**If PERSON entities occur in the training data but aren’t labelled, the model will learn that they shouldn’t be predicted.** \n",
    "\n",
    "Similarly, if an existing entity type isn’t present in the training data, the model may ”forget” and stop predicting it."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's update the training data to include annotations for the PERSON entities “PewDiePie” and “Alexis Ohanian”."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy\n",
    "from spacy.tokens import Span\n",
    "\n",
    "nlp = spacy.blank(\"en\")\n",
    "\n",
    "doc1 = nlp(\"Reddit partners with Patreon to help creators build communities\")\n",
    "doc1.ents = [\n",
    "    Span(doc1, 0, 1, label=\"WEBSITE\"),\n",
    "    Span(doc1, 3, 4, label=\"WEBSITE\"),\n",
    "]\n",
    "\n",
    "doc2 = nlp(\"PewDiePie smashes YouTube record\")\n",
    "doc2.ents = [Span(doc2, 0, 1, label=\"PERSON\"), Span(doc2, 2, 3, label=\"WEBSITE\")]\n",
    "\n",
    "doc3 = nlp(\"Reddit founder Alexis Ohanian gave away two Metallica tickets to fans\")\n",
    "doc3.ents = [Span(doc3, 0, 1, label=\"WEBSITE\"), Span(doc3, 2, 4, label=\"PERSON\")]"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.8.6 ('teaching_env')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.6"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  },
  "vscode": {
   "interpreter": {
    "hash": "becc4c8e5ad229b2591d820334d85e3db0111492344629bf57f272470dce75a5"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
