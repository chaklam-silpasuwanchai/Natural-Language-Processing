{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Assignment 1 : Search Engine"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "## Task 1. Preparation and Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "corpus_tokenized sample : 4623\n",
      "voc_size : 812\n",
      " 19%|███████▌                               | 962/5000 [00:02<00:10, 401.43it/s]Epoch   1000 | Loss: 8.695992\n",
      " 40%|███████████████▏                      | 1999/5000 [00:06<00:08, 367.12it/s]Epoch   2000 | Loss: 6.969626\n",
      " 60%|██████████████████████▋               | 2987/5000 [00:10<00:05, 350.37it/s]Epoch   3000 | Loss: 6.696272\n",
      " 80%|██████████████████████████████▏       | 3977/5000 [00:13<00:02, 378.44it/s]Epoch   4000 | Loss: 6.260726\n",
      " 99%|█████████████████████████████████████▋| 4957/5000 [00:16<00:00, 340.94it/s]Epoch   5000 | Loss: 6.617470\n",
      "100%|██████████████████████████████████████| 5000/5000 [00:16<00:00, 300.58it/s]\n"
     ]
    }
   ],
   "source": [
    "!python3 skipgram.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "corpus_tokenized sample : 4623\n",
      "voc_size : 812\n",
      " 19%|███████▍                               | 959/5000 [00:03<00:21, 189.09it/s]Epoch   1000 | Loss: 5.292124\n",
      " 39%|██████████████▊                       | 1952/5000 [00:06<00:08, 357.65it/s]Epoch   2000 | Loss: 1.846857\n",
      " 60%|██████████████████████▋               | 2977/5000 [00:09<00:05, 381.16it/s]Epoch   3000 | Loss: 1.503825\n",
      " 80%|██████████████████████████████▎       | 3994/5000 [00:12<00:02, 346.80it/s]Epoch   4000 | Loss: 2.561615\n",
      " 99%|█████████████████████████████████████▊| 4969/5000 [00:14<00:00, 309.03it/s]Epoch   5000 | Loss: 5.038004\n",
      "100%|██████████████████████████████████████| 5000/5000 [00:14<00:00, 334.07it/s]\n"
     ]
    }
   ],
   "source": [
    "!python3 neg.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "corpus_tokenized sample : 4623\n",
      "330078it [00:00, 609404.45it/s]\n",
      "voc_size : 812\n",
      " 20%|███████▋                               | 989/5000 [00:02<00:09, 402.90it/s]Epoch: 1000 | cost: 7.054244 | time: 0m 2s\n",
      " 40%|███████████████▏                      | 1993/5000 [00:04<00:06, 478.11it/s]Epoch: 2000 | cost: 1.901738 | time: 0m 4s\n",
      " 60%|██████████████████████▊               | 2997/5000 [00:07<00:04, 462.23it/s]Epoch: 3000 | cost: 1.622313 | time: 0m 7s\n",
      " 80%|██████████████████████████████▎       | 3987/5000 [00:09<00:02, 447.01it/s]Epoch: 4000 | cost: 1.335360 | time: 0m 9s\n",
      "100%|█████████████████████████████████████▉| 4990/5000 [00:12<00:00, 434.34it/s]Epoch: 5000 | cost: 0.726250 | time: 0m 12s\n",
      "100%|██████████████████████████████████████| 5000/5000 [00:12<00:00, 412.94it/s]\n"
     ]
    }
   ],
   "source": [
    "!python3 glove.py"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "## Task 2. Model Comparison and Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "corpus_tokenized sample : 4623\n",
      "voc_size : 812\n"
     ]
    }
   ],
   "source": [
    "# Use corpus from nltk\n",
    "import nltk\n",
    "# nltk.download('brown')\n",
    "from nltk.corpus import brown\n",
    "\n",
    "corpus_tokenized = nltk.corpus.brown.sents(categories='news')\n",
    "print('corpus_tokenized sample :',len(corpus_tokenized))\n",
    "\n",
    "#1. tokenization\n",
    "corpus = [[word.lower() for word in sent] for sent in corpus_tokenized]\n",
    "corpus = corpus[:100]\n",
    "\n",
    "#2. numeralization\n",
    "#find unique words\n",
    "flatten = lambda l: [item for sublist in l for item in sublist]\n",
    "#assign unique integer\n",
    "vocabs = list(set(flatten(corpus))) #all the words we have in the system - <UNK>\n",
    "\n",
    "#create handy mapping between integer and word\n",
    "word2index = {v:idx for idx, v in enumerate(vocabs)}\n",
    "\n",
    "#append UNK\n",
    "vocabs.append('<UNK>')\n",
    "word2index['<UNK>'] = len(vocabs) - 1\n",
    "\n",
    "#vocab size\n",
    "voc_size = len(vocabs)\n",
    "print('voc_size :',voc_size)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.1 Compare Skip-gram, Skip-gram with negative sampling, GloVe models on training loss, training time, syntactic and semantic accuracy, similar to the methods in the Word2Vec and GloVe paper. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Skipgram.pt', 'GloVe.pt', 'SkipgramNeg.pt']\n",
      "Skipgram\n",
      "GloVe\n",
      "SkipgramNeg\n",
      "[Skipgram(\n",
      "  (embedding_center): Embedding(812, 2)\n",
      "  (embedding_outside): Embedding(812, 2)\n",
      "), GloVe(\n",
      "  (embedding_center): Embedding(812, 2)\n",
      "  (embedding_outside): Embedding(812, 2)\n",
      "  (v_bias): Embedding(812, 1)\n",
      "  (u_bias): Embedding(812, 1)\n",
      "), SkipgramNeg(\n",
      "  (embedding_center): Embedding(812, 2)\n",
      "  (embedding_outside): Embedding(812, 2)\n",
      "  (logsigmoid): LogSigmoid()\n",
      ")]\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import os\n",
    "from all_models import Skipgram, SkipgramNeg, GloVe\n",
    "\n",
    "list_weight = os.listdir('./models')\n",
    "# list_weight.remove('.ipynb_checkpoints')\n",
    "print(list_weight)\n",
    "\n",
    "embedding_size  = 2\n",
    "model_skipgram  = Skipgram(voc_size, embedding_size)\n",
    "model_neg       = SkipgramNeg(voc_size, embedding_size)\n",
    "model_glove     = GloVe(voc_size, embedding_size)\n",
    "\n",
    "list_model      = [model_skipgram, model_glove, model_neg]\n",
    "\n",
    "for idx, each_weight in enumerate(list_weight):\n",
    "    print(list_model[idx].__class__.__name__)\n",
    "    pretrained_state_dict = torch.load(os.path.join('./models/', each_weight))\n",
    "    # # Load the state dictionary into the new model\n",
    "    list_model[idx].load_state_dict(pretrained_state_dict)\n",
    "\n",
    "print(list_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !python3 -m spacy download en_core_web_sm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>// Copyright 2013 Google Inc. All Rights Reser...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>: capital-common-countries\\n</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Athens Greece Baghdad Iraq\\n</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Athens Greece Bangkok Thailand\\n</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Athens Greece Beijing China\\n</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                   0\n",
       "0  // Copyright 2013 Google Inc. All Rights Reser...\n",
       "1                       : capital-common-countries\\n\n",
       "2                       Athens Greece Baghdad Iraq\\n\n",
       "3                   Athens Greece Bangkok Thailand\\n\n",
       "4                      Athens Greece Beijing China\\n"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Testing Set\n",
    "import spacy\n",
    "import numpy as np\n",
    "import re\n",
    "import pandas as pd\n",
    "\n",
    "def clean_data(df_col):\n",
    "    corpus = []\n",
    "    for item in df_col:\n",
    "        item = re.sub('[^A-Za-z0-9]+', ' ', str(item)) # remove special characters\n",
    "        item = item.lower() # lower all characters\n",
    "        item = item.split() # split data\n",
    "        corpus.append(' '.join(str(x) for x in item))\n",
    "    return corpus\n",
    "    \n",
    "nlp = spacy.load('en_core_web_sm')\n",
    "text = open('./data/word-test.txt',mode='r')\n",
    "df = pd.DataFrame(text.readlines())\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1, 508, 5033, 5900, 8368, 8875, 9868, 10681, 12014, 13137, 14194, 15794, 17355, 18688]\n"
     ]
    }
   ],
   "source": [
    "#Check Header \n",
    "header = df[0].str.startswith(':')\n",
    "index_list = np.where(header)[0].tolist()\n",
    "print(index_list)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Semantic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>: capital-common-countries\\n</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Athens Greece Baghdad Iraq\\n</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Athens Greece Bangkok Thailand\\n</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Athens Greece Beijing China\\n</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>Athens Greece Berlin Germany\\n</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                  0\n",
       "1      : capital-common-countries\\n\n",
       "2      Athens Greece Baghdad Iraq\\n\n",
       "3  Athens Greece Bangkok Thailand\\n\n",
       "4     Athens Greece Beijing China\\n\n",
       "5    Athens Greece Berlin Germany\\n"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Semantic\n",
    "#capital-common-countries \n",
    "df_capital_common_countries = df[1:508]\n",
    "df_capital_common_countries.head()\n",
    "#i pick only captial-common-countries to test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[['athens', 'greece', 'baghdad', 'iraq'],\n",
       " ['athens', 'greece', 'bangkok', 'thailand'],\n",
       " ['athens', 'greece', 'beijing', 'china'],\n",
       " ['athens', 'greece', 'berlin', 'germany'],\n",
       " ['athens', 'greece', 'bern', 'switzerland']]"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#1. tokenize\n",
    "#data cleaned\n",
    "corpus_test = clean_data(df_capital_common_countries[0])\n",
    "#data tokenized\n",
    "semantic_corpus_tokenized_test = [sent.split(\" \") for sent in corpus_test]\n",
    "semantic_corpus_tokenized_test.pop(0)\n",
    "semantic_corpus_tokenized_test[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "506"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(semantic_corpus_tokenized_test)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Syntactic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>15794</th>\n",
       "      <td>: gram7-past-tense\\n</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15795</th>\n",
       "      <td>dancing danced decreasing decreased\\n</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15796</th>\n",
       "      <td>dancing danced describing described\\n</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15797</th>\n",
       "      <td>dancing danced enhancing enhanced\\n</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15798</th>\n",
       "      <td>dancing danced falling fell\\n</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                           0\n",
       "15794                   : gram7-past-tense\\n\n",
       "15795  dancing danced decreasing decreased\\n\n",
       "15796  dancing danced describing described\\n\n",
       "15797    dancing danced enhancing enhanced\\n\n",
       "15798          dancing danced falling fell\\n"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Syntactic\n",
    "#: gram7-past-tense\n",
    "df_past_tense = df[15794:17354]\n",
    "df_past_tense.head()\n",
    "#i pick only past-tense to test"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Cleaning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[['dancing', 'danced', 'decreasing', 'decreased'],\n",
       " ['dancing', 'danced', 'describing', 'described'],\n",
       " ['dancing', 'danced', 'enhancing', 'enhanced'],\n",
       " ['dancing', 'danced', 'falling', 'fell'],\n",
       " ['dancing', 'danced', 'feeding', 'fed']]"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#1. tokenize\n",
    "#data cleaned\n",
    "corpus_test = clean_data(df_past_tense[0])\n",
    "#data tokenized\n",
    "syntactic_corpus_tokenized_test = [sent.split(\" \") for sent in corpus_test]\n",
    "syntactic_corpus_tokenized_test.pop(0)\n",
    "syntactic_corpus_tokenized_test[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1559"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(syntactic_corpus_tokenized_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "corpus_tokenized_test = syntactic_corpus_tokenized_test + semantic_corpus_tokenized_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "127"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Put to pandas which easier to select column\n",
    "import pandas as pd\n",
    "data = pd.DataFrame(corpus_tokenized_test, columns=[\"A\", \"B\", \"C\", \"D\"])\n",
    "\n",
    "#2. numericalize (vocab)\n",
    "#2.1 get all the unique words\n",
    "#we want to flatten unit (basically merge all list)\n",
    "flatten = lambda l: [item for sublist in l for item in sublist]\n",
    "vocabs_test = list(set(flatten(corpus_tokenized_test)))\n",
    "\n",
    "#2.2 assign id to all these vocabs\n",
    "word2index = {v: idx for idx, v in enumerate(vocabs_test)}\n",
    "\n",
    "#adding unknown word\n",
    "vocabs_test.append('<UNK>')\n",
    "word2index['<UNK>'] = len(vocabs_test) - 1\n",
    "\n",
    "voc_size_test = len(vocabs_test)\n",
    "voc_size_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('Skipgram', ('saw', 0.47231573), 0)\n",
      "('GloVe', ('saw', 0.9661224), 0)\n",
      "('SkipgramNeg', ('saw', 0.9999987), 0)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.10/dist-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "#testing draft\n",
    "from tqdm.auto import tqdm\n",
    "from numpy import dot\n",
    "from numpy.linalg import norm\n",
    "\n",
    "def cos_sim(a, b):\n",
    "    cos_sim = dot(a, b)/(norm(a)*norm(b))\n",
    "    return cos_sim\n",
    "    \n",
    "#find embedding of fruit, cat\n",
    "def get_embed(word, model, word2index):\n",
    "    try:\n",
    "        index = word2index[word]\n",
    "    except :\n",
    "        index = word2index['<UNK>'] #unknown\n",
    "    word = torch.LongTensor([index])\n",
    "    \n",
    "    embed = (model.embedding_center(word)+model.embedding_outside(word))/2\n",
    "    return np.array(embed[0].detach().numpy())\n",
    "    # return embed[0][0].item(),embed[0][1].item()\n",
    "\n",
    "def find_analogy(word_list, model, vocabs,word2index):\n",
    "    word1, word2, word3, word4 = word_list\n",
    "    emb_a, emb_b, emb_c = get_embed(word1, model, word2index),get_embed(word2, model,word2index),get_embed(word3, model,word2index)\n",
    "    vector = emb_b - emb_a + emb_c\n",
    "    similarity = -1 \n",
    "    \n",
    "    accuracy = 0\n",
    "    for vocab in vocabs:\n",
    "        if vocab not in [word1, word2, word3]: #ignore input words itself\n",
    "            current_sim = cos_sim(vector,get_embed(vocab,model,word2index))\n",
    "            if current_sim > similarity:\n",
    "                similarity = current_sim #update better one\n",
    "                d = (vocab, similarity)\n",
    "                if d == word4:\n",
    "                    accuracy = 1\n",
    "                else:\n",
    "                    accuracy = 0\n",
    "                    \n",
    "    return model.__class__.__name__, d, accuracy\n",
    "\n",
    "#Test each model\n",
    "for each_model in list_model:\n",
    "    print(find_analogy(['dancing', 'danced', 'decreasing', 'decreased'], each_model, vocabs, word2index))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Find Analogy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 160,
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_analogy_sets(analogy_sets, model, word2index, vocab):\n",
    "    total_accuracy = 0\n",
    "    num_sets = len(analogy_sets)\n",
    "\n",
    "    for word_list in tqdm(analogy_sets):\n",
    "        word1, word2, word3, word4 = word_list\n",
    "        emb_a, emb_b, emb_c = get_embed(word1, model, word2index), get_embed(word2, model, word2index), get_embed(word3, model, word2index)\n",
    "        vector = emb_b - emb_a + emb_c\n",
    "        similarity = -1\n",
    "\n",
    "        accuracy = 0\n",
    "        for vocab_word in vocab:\n",
    "            if vocab_word not in [word1, word2, word3]:\n",
    "                current_sim = cos_sim(vector, get_embed(vocab_word, model, word2index))\n",
    "                if current_sim > similarity:\n",
    "                    similarity = current_sim\n",
    "                    predicted_word = vocab_word\n",
    "                    if predicted_word == word4:\n",
    "                        accuracy = 1\n",
    "                    else:\n",
    "                        accuracy = 0\n",
    "\n",
    "        total_accuracy += accuracy\n",
    "\n",
    "    average_accuracy = total_accuracy / num_sets\n",
    "    return model.__class__.__name__, average_accuracy"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Semantic Result from Skipgram, SkipgramNeg, Glove"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 163,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 506/506 [01:56<00:00,  4.36it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('Glove', 0.0)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 506/506 [01:56<00:00,  4.35it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('Skipgram', 0.0)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 506/506 [01:56<00:00,  4.35it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('SkipgramNeg', 0.0)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# Semantic Corpus Result\n",
    "for each_model in list_model:\n",
    "    result = find_analogy_sets(semantic_corpus_tokenized_test, each_model, word2index, vocab)\n",
    "    print(result)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Syntactic Result from Skipgram, SkipgramNeg, Glove"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 164,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1559/1559 [05:58<00:00,  4.34it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('Glove', 0.011545862732520847)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1559/1559 [05:59<00:00,  4.34it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('Skipgram', 0.015394483643361129)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1559/1559 [06:01<00:00,  4.31it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('SkipgramNeg', 0.01603592046183451)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# Syntactic Corpus Result\n",
    "for each_model in list_model:\n",
    "    result = find_analogy_sets(syntactic_corpus_tokenized_test, each_model, word2index, vocab)\n",
    "    print(result)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Glove gensim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip3 install gensim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "from gensim.test.utils import datapath\n",
    "from gensim.models import KeyedVectors\n",
    "from gensim.scripts.glove2word2vec import glove2word2vec\n",
    "\n",
    "#you have to put this file in some python/gensim directory; just run it and it will inform where to put....\n",
    "# glove_file = datapath('glove.6B.100d.txt')  #search on the google\n",
    "glove_file = './data/glove.6B.50d.txt'\n",
    "model = KeyedVectors.load_word2vec_format(glove_file, binary=False, no_header=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "def analogy_gensim(analogy_sets, model):\n",
    "    total_accuracy = 0\n",
    "    num_sets = len(analogy_sets)\n",
    "    \n",
    "    for word_list in tqdm(analogy_sets):\n",
    "        word1, word2, word3, word4 = word_list\n",
    "        result = model.most_similar(positive=[word3, word2], negative=[word1])\n",
    "        if result[0][0] == word4:\n",
    "            accuracy = 1\n",
    "        else:\n",
    "            accuracy = 0\n",
    "        total_accuracy += accuracy\n",
    "    average_accuracy = total_accuracy / num_sets\n",
    "    return average_accuracy"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Semantic/Syntactic Result from Gensim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 506/506 [00:05<00:00, 93.58it/s] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "semantic_result :0.792\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1559/1559 [00:19<00:00, 81.86it/s] "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "syntactic_result :0.375\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "semantic_result = analogy_gensim(semantic_corpus_tokenized_test, model)\n",
    "print(f'semantic_result :{semantic_result:.3f}')\n",
    "\n",
    "syntactic_result = analogy_gensim(syntactic_corpus_tokenized_test, model)\n",
    "print(f'syntactic_result :{syntactic_result:.3f}')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.2 Use the similarity dataset to find the correlation between your models' dot product and the provided similarity metrics. Assess if your embeddings correlate with human judgment."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Gold standard"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>x1</th>\n",
       "      <th>x2</th>\n",
       "      <th>correlation</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>computer</td>\n",
       "      <td>keyboard</td>\n",
       "      <td>7.62</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>jerusalem</td>\n",
       "      <td>israel</td>\n",
       "      <td>8.46</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>planet</td>\n",
       "      <td>galaxy</td>\n",
       "      <td>8.11</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>canyon</td>\n",
       "      <td>landscape</td>\n",
       "      <td>7.53</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>opec</td>\n",
       "      <td>country</td>\n",
       "      <td>5.63</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "          x1         x2 correlation\n",
       "0   computer   keyboard        7.62\n",
       "1  jerusalem     israel        8.46\n",
       "2     planet     galaxy        8.11\n",
       "3     canyon  landscape        7.53\n",
       "4       opec    country        5.63"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "wordsim353 = open('./data/wordsim353_sim_rel/wordsim_relatedness_goldstandard.txt',mode='r')\n",
    "df_wordsim353 = pd.DataFrame(wordsim353.readlines())\n",
    "\n",
    "def clean_data_num(df_col):\n",
    "    corpus = []\n",
    "    for item in df_col:\n",
    "        # item = re.sub('[^A-Za-z0-9]+', ' ', str(item)) # remove special characters\n",
    "        item = item.lower() # lower all characters\n",
    "        item = item.split() # split data\n",
    "        corpus.append(' '.join(str(x) for x in item))\n",
    "    return corpus\n",
    "\n",
    "#1. tokenize\n",
    "#data cleaned\n",
    "corpus_wordsim353 = clean_data_num(df_wordsim353[0])\n",
    "\n",
    "#data tokenized\n",
    "corpus_tokenized_wordsim353 = [sent.split(\" \") for sent in corpus_wordsim353]\n",
    "# corpus_tokenized_wordsim353[:5]\n",
    "wordsim_353 = pd.DataFrame(corpus_tokenized_wordsim353, columns=['x1','x2','correlation'])\n",
    "wordsim_353.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_embed(word, model, word2index):\n",
    "    try:\n",
    "        index = word2index[word]\n",
    "    except :\n",
    "        index = word2index['<UNK>'] #unknown\n",
    "    word = torch.LongTensor([index])\n",
    "    \n",
    "    embed = (model.embedding_center(word)+model.embedding_outside(word))/2\n",
    "    return np.array(embed[0].detach().numpy())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Skipgram(\n",
       "   (embedding_center): Embedding(812, 2)\n",
       "   (embedding_outside): Embedding(812, 2)\n",
       " ),\n",
       " GloVe(\n",
       "   (embedding_center): Embedding(812, 2)\n",
       "   (embedding_outside): Embedding(812, 2)\n",
       "   (v_bias): Embedding(812, 1)\n",
       "   (u_bias): Embedding(812, 1)\n",
       " ),\n",
       " SkipgramNeg(\n",
       "   (embedding_center): Embedding(812, 2)\n",
       "   (embedding_outside): Embedding(812, 2)\n",
       "   (logsigmoid): LogSigmoid()\n",
       " )]"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "list_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "wordsim = {}\n",
    "for idx, model in enumerate(list_model):\n",
    "    wordsim[model.__class__.__name__] = wordsim_353.apply(\n",
    "        lambda row: np.dot(\n",
    "            get_embed(row['x1'], model, word2index),\n",
    "            get_embed(row['x2'], model, word2index)\n",
    "        ), axis=1)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Correlation  \n",
    "- Calculate a Spearman correlation coefficient with associated p-value."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Skipgram:\n",
      "Spearman correlation: nan\n",
      "P-value: nan\n",
      "GloVe:\n",
      "Spearman correlation: nan\n",
      "P-value: nan\n",
      "SkipgramNeg:\n",
      "Spearman correlation: nan\n",
      "P-value: nan\n"
     ]
    }
   ],
   "source": [
    "from scipy.stats import spearmanr\n",
    "\n",
    "# Example data\n",
    "for idx, ws in enumerate(wordsim.keys()):\n",
    "    # Calculate Spearman correlation coefficient\n",
    "    corr_coef, p_value = spearmanr(wordsim_353['correlation'], wordsim[ws])\n",
    "    # Display the result\n",
    "    print(f\"{list_model[idx].__class__.__name__}:\")\n",
    "    print(f\"Spearman correlation: {corr_coef}\")\n",
    "    print(f\"P-value: {p_value}\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Conclusion\n",
    "**Section 1** : I have tried to train the model (skipgram, NEG, GloVe) using only 5000 epoch \n",
    "\n",
    "| Model          | Window Size | Dimension | Training Loss | Syntactic Accuracy | Semantic accuracy |\n",
    "|----------------|-------------|-----------|---------------|--------------------|-------------------|\n",
    "| Skipgram       |      2      |     2     |  9.523348     |         0          |         0         |\n",
    "| Skipgram (NEG) |      2      |     2     |  1.891104     |         0          |         0         |\n",
    "| Glove          |      2      |     2     |  93.934296    |         0          |         0         |\n",
    "| Glove (Gensim) |      10     |    100    |       -       |       0.792        |       0.375       |\n",
    "\n",
    "**Section 2** : using spearman correlation to find similarity\n",
    "\n",
    "| Model           | Skipgram | NEG    | GloVe  | GloVe (gensim) | Y_true |\n",
    "|-----------------|----------|--------|--------|----------------|--------|\n",
    "| MSE             | 26.726   | 26.726 | 26.726 | 5.29           | 5.03   |\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
