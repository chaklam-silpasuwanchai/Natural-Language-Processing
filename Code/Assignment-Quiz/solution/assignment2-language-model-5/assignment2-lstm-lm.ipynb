{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Assignment 2 : Languague Modeling\n",
    "\n",
    "# Recurrent Neural Networks and Language Models\n",
    "\n",
    "You guys probably very excited about ChatGPT.  In today class, we will be implementing a very simple language model, which is basically what ChatGPT is, but with a simple LSTM.  You will be surprised that it is not so difficult at all.\n",
    "\n",
    "Paper that we base on is *Regularizing and Optimizing LSTM Language Models*, https://arxiv.org/abs/1708.02182"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda:0\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "\n",
    "device = torch.device('cuda:0' if torch.cuda.is_available() else 'cpu')\n",
    "print(device)\n",
    "\n",
    "#make our work comparable if restarted the kernel\n",
    "SEED = 1234\n",
    "torch.manual_seed(SEED)\n",
    "torch.backends.cudnn.deterministic = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.10/dist-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "('2.0.1+cu117', '0.6.1', '0.15.2+cpu')"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torchtext, datasets, math\n",
    "from tqdm.auto import tqdm\n",
    "import torchdata\n",
    "torch.__version__, torchdata.__version__, torchtext.__version__"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Task 1. Dataset Acquisition "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Load data - Wiki Text\n",
    "\n",
    "We will be using wikitext which contains a large corpus of text, perfect for language modeling task.  This time, we will use the `datasets` library from HuggingFace to load."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ['http_proxy']  = 'http://192.41.170.23:3128'\n",
    "os.environ['https_proxy'] = 'http://192.41.170.23:3128'\n",
    "# https://huggingface.co/datasets/codeparrot/github-jupyter-code-to-text/tree/main dataset\n",
    "#there are raw and preprocessed version; we used the raw one and preprocessed ourselves for fun"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Downloading readme: 100%|██████████| 872/872 [00:00<00:00, 9.57MB/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading and preparing dataset parquet/codeparrot--github-jupyter-code-to-text to /home/todsavadt/.cache/huggingface/datasets/codeparrot___parquet/codeparrot--github-jupyter-code-to-text-14ed47a6ba32f30c/0.0.0/2a3b91fbd88a2c90d1dbbb32b460cf621d31bd5b05b934492fdef7d8d6f236ec...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Downloading data files:   0%|          | 0/2 [00:00<?, ?it/s]\n",
      "Downloading data:   0%|          | 0.00/227M [00:00<?, ?B/s]\u001b[A\n",
      "Downloading data:   0%|          | 520k/227M [00:00<00:44, 5.14MB/s]\u001b[A\n",
      "Downloading data:   1%|          | 1.71M/227M [00:00<00:24, 9.01MB/s]\u001b[A\n",
      "Downloading data:   1%|▏         | 2.89M/227M [00:00<00:21, 10.2MB/s]\u001b[A\n",
      "Downloading data:   2%|▏         | 4.06M/227M [00:00<00:20, 10.8MB/s]\u001b[A\n",
      "Downloading data:   2%|▏         | 5.24M/227M [00:00<00:19, 11.2MB/s]\u001b[A\n",
      "Downloading data:   3%|▎         | 6.42M/227M [00:00<00:19, 11.4MB/s]\u001b[A\n",
      "Downloading data:   3%|▎         | 7.61M/227M [00:00<00:19, 11.5MB/s]\u001b[A\n",
      "Downloading data:   4%|▍         | 8.78M/227M [00:00<00:18, 11.6MB/s]\u001b[A\n",
      "Downloading data:   4%|▍         | 9.96M/227M [00:00<00:18, 11.6MB/s]\u001b[A\n",
      "Downloading data:   5%|▍         | 11.1M/227M [00:01<00:18, 11.7MB/s]\u001b[A\n",
      "Downloading data:   5%|▌         | 12.3M/227M [00:01<00:18, 11.7MB/s]\u001b[A\n",
      "Downloading data:   6%|▌         | 13.5M/227M [00:01<00:18, 11.7MB/s]\u001b[A\n",
      "Downloading data:   6%|▋         | 14.7M/227M [00:01<00:18, 11.7MB/s]\u001b[A\n",
      "Downloading data:   7%|▋         | 15.9M/227M [00:01<00:17, 11.7MB/s]\u001b[A\n",
      "Downloading data:   8%|▊         | 17.0M/227M [00:01<00:17, 11.7MB/s]\u001b[A\n",
      "Downloading data:   8%|▊         | 18.2M/227M [00:01<00:17, 11.7MB/s]\u001b[A\n",
      "Downloading data:   9%|▊         | 19.4M/227M [00:01<00:17, 11.7MB/s]\u001b[A\n",
      "Downloading data:   9%|▉         | 20.6M/227M [00:01<00:17, 11.7MB/s]\u001b[A\n",
      "Downloading data:  10%|▉         | 21.8M/227M [00:01<00:17, 11.7MB/s]\u001b[A\n",
      "Downloading data:  10%|█         | 22.9M/227M [00:02<00:17, 11.7MB/s]\u001b[A\n",
      "Downloading data:  11%|█         | 24.1M/227M [00:02<00:17, 11.7MB/s]\u001b[A\n",
      "Downloading data:  11%|█         | 25.3M/227M [00:02<00:17, 11.7MB/s]\u001b[A\n",
      "Downloading data:  12%|█▏        | 26.5M/227M [00:02<00:17, 11.7MB/s]\u001b[A\n",
      "Downloading data:  12%|█▏        | 27.7M/227M [00:02<00:16, 11.7MB/s]\u001b[A\n",
      "Downloading data:  13%|█▎        | 28.8M/227M [00:02<00:16, 11.8MB/s]\u001b[A\n",
      "Downloading data:  13%|█▎        | 30.0M/227M [00:02<00:16, 11.7MB/s]\u001b[A\n",
      "Downloading data:  14%|█▍        | 31.2M/227M [00:02<00:16, 11.7MB/s]\u001b[A\n",
      "Downloading data:  14%|█▍        | 32.4M/227M [00:02<00:16, 11.7MB/s]\u001b[A\n",
      "Downloading data:  15%|█▍        | 33.6M/227M [00:02<00:16, 11.7MB/s]\u001b[A\n",
      "Downloading data:  15%|█▌        | 34.7M/227M [00:03<00:16, 11.7MB/s]\u001b[A\n",
      "Downloading data:  16%|█▌        | 35.9M/227M [00:03<00:16, 11.7MB/s]\u001b[A\n",
      "Downloading data:  16%|█▋        | 37.1M/227M [00:03<00:16, 11.7MB/s]\u001b[A\n",
      "Downloading data:  17%|█▋        | 38.3M/227M [00:03<00:16, 11.7MB/s]\u001b[A\n",
      "Downloading data:  17%|█▋        | 39.5M/227M [00:03<00:15, 11.7MB/s]\u001b[A\n",
      "Downloading data:  18%|█▊        | 40.6M/227M [00:03<00:15, 11.7MB/s]\u001b[A\n",
      "Downloading data:  18%|█▊        | 41.8M/227M [00:03<00:15, 11.7MB/s]\u001b[A\n",
      "Downloading data:  19%|█▉        | 43.0M/227M [00:03<00:15, 11.7MB/s]\u001b[A\n",
      "Downloading data:  20%|█▉        | 44.2M/227M [00:03<00:15, 11.8MB/s]\u001b[A\n",
      "Downloading data:  20%|██        | 45.4M/227M [00:03<00:15, 11.7MB/s]\u001b[A\n",
      "Downloading data:  21%|██        | 46.5M/227M [00:04<00:15, 11.7MB/s]\u001b[A\n",
      "Downloading data:  21%|██        | 47.7M/227M [00:04<00:15, 11.7MB/s]\u001b[A\n",
      "Downloading data:  22%|██▏       | 48.9M/227M [00:04<00:15, 11.7MB/s]\u001b[A\n",
      "Downloading data:  22%|██▏       | 50.1M/227M [00:04<00:15, 11.7MB/s]\u001b[A\n",
      "Downloading data:  23%|██▎       | 51.3M/227M [00:04<00:14, 11.7MB/s]\u001b[A\n",
      "Downloading data:  23%|██▎       | 52.4M/227M [00:04<00:14, 11.7MB/s]\u001b[A\n",
      "Downloading data:  24%|██▎       | 53.6M/227M [00:04<00:14, 11.7MB/s]\u001b[A\n",
      "Downloading data:  24%|██▍       | 54.8M/227M [00:04<00:14, 11.7MB/s]\u001b[A\n",
      "Downloading data:  25%|██▍       | 56.0M/227M [00:04<00:14, 11.7MB/s]\u001b[A\n",
      "Downloading data:  25%|██▌       | 57.2M/227M [00:04<00:14, 11.7MB/s]\u001b[A\n",
      "Downloading data:  26%|██▌       | 58.3M/227M [00:05<00:14, 11.7MB/s]\u001b[A\n",
      "Downloading data:  26%|██▋       | 59.5M/227M [00:05<00:14, 11.7MB/s]\u001b[A\n",
      "Downloading data:  27%|██▋       | 60.7M/227M [00:05<00:14, 11.7MB/s]\u001b[A\n",
      "Downloading data:  27%|██▋       | 61.9M/227M [00:05<00:14, 11.7MB/s]\u001b[A\n",
      "Downloading data:  28%|██▊       | 63.1M/227M [00:05<00:13, 11.7MB/s]\u001b[A\n",
      "Downloading data:  28%|██▊       | 64.2M/227M [00:05<00:13, 11.7MB/s]\u001b[A\n",
      "Downloading data:  29%|██▉       | 65.4M/227M [00:05<00:13, 11.7MB/s]\u001b[A\n",
      "Downloading data:  29%|██▉       | 66.6M/227M [00:05<00:13, 11.7MB/s]\u001b[A\n",
      "Downloading data:  30%|██▉       | 67.8M/227M [00:05<00:13, 11.7MB/s]\u001b[A\n",
      "Downloading data:  30%|███       | 69.0M/227M [00:05<00:13, 11.7MB/s]\u001b[A\n",
      "Downloading data:  31%|███       | 70.1M/227M [00:06<00:13, 11.8MB/s]\u001b[A\n",
      "Downloading data:  31%|███▏      | 71.3M/227M [00:06<00:13, 11.8MB/s]\u001b[A\n",
      "Downloading data:  32%|███▏      | 72.5M/227M [00:06<00:13, 11.7MB/s]\u001b[A\n",
      "Downloading data:  33%|███▎      | 73.7M/227M [00:06<00:13, 11.7MB/s]\u001b[A\n",
      "Downloading data:  33%|███▎      | 74.9M/227M [00:06<00:12, 11.7MB/s]\u001b[A\n",
      "Downloading data:  34%|███▎      | 76.0M/227M [00:06<00:12, 11.7MB/s]\u001b[A\n",
      "Downloading data:  34%|███▍      | 77.2M/227M [00:06<00:12, 11.7MB/s]\u001b[A\n",
      "Downloading data:  35%|███▍      | 78.4M/227M [00:06<00:12, 11.7MB/s]\u001b[A\n",
      "Downloading data:  35%|███▌      | 79.6M/227M [00:06<00:12, 11.8MB/s]\u001b[A\n",
      "Downloading data:  36%|███▌      | 80.8M/227M [00:06<00:12, 11.7MB/s]\u001b[A\n",
      "Downloading data:  36%|███▌      | 81.9M/227M [00:07<00:12, 11.7MB/s]\u001b[A\n",
      "Downloading data:  37%|███▋      | 83.1M/227M [00:07<00:12, 11.7MB/s]\u001b[A\n",
      "Downloading data:  37%|███▋      | 84.3M/227M [00:07<00:12, 11.8MB/s]\u001b[A\n",
      "Downloading data:  38%|███▊      | 85.5M/227M [00:07<00:12, 11.7MB/s]\u001b[A\n",
      "Downloading data:  38%|███▊      | 86.7M/227M [00:07<00:11, 11.7MB/s]\u001b[A\n",
      "Downloading data:  39%|███▉      | 87.8M/227M [00:07<00:11, 11.7MB/s]\u001b[A\n",
      "Downloading data:  39%|███▉      | 89.0M/227M [00:07<00:11, 11.7MB/s]\u001b[A\n",
      "Downloading data:  40%|███▉      | 90.2M/227M [00:07<00:11, 11.7MB/s]\u001b[A\n",
      "Downloading data:  40%|████      | 91.4M/227M [00:07<00:11, 11.7MB/s]\u001b[A\n",
      "Downloading data:  41%|████      | 92.6M/227M [00:07<00:11, 11.7MB/s]\u001b[A\n",
      "Downloading data:  41%|████▏     | 93.7M/227M [00:08<00:11, 11.8MB/s]\u001b[A\n",
      "Downloading data:  42%|████▏     | 94.9M/227M [00:08<00:11, 11.7MB/s]\u001b[A\n",
      "Downloading data:  42%|████▏     | 96.1M/227M [00:08<00:11, 11.7MB/s]\u001b[A\n",
      "Downloading data:  43%|████▎     | 97.3M/227M [00:08<00:11, 11.7MB/s]\u001b[A\n",
      "Downloading data:  43%|████▎     | 98.5M/227M [00:08<00:10, 11.8MB/s]\u001b[A\n",
      "Downloading data:  44%|████▍     | 99.6M/227M [00:08<00:10, 11.8MB/s]\u001b[A\n",
      "Downloading data:  45%|████▍     | 101M/227M [00:08<00:10, 11.7MB/s] \u001b[A\n",
      "Downloading data:  45%|████▌     | 102M/227M [00:08<00:10, 11.8MB/s]\u001b[A\n",
      "Downloading data:  46%|████▌     | 103M/227M [00:08<00:10, 11.8MB/s]\u001b[A\n",
      "Downloading data:  46%|████▌     | 104M/227M [00:08<00:10, 11.7MB/s]\u001b[A\n",
      "Downloading data:  47%|████▋     | 106M/227M [00:09<00:10, 11.7MB/s]\u001b[A\n",
      "Downloading data:  47%|████▋     | 107M/227M [00:09<00:10, 11.8MB/s]\u001b[A\n",
      "Downloading data:  48%|████▊     | 108M/227M [00:09<00:10, 11.8MB/s]\u001b[A\n",
      "Downloading data:  48%|████▊     | 109M/227M [00:09<00:09, 11.7MB/s]\u001b[A\n",
      "Downloading data:  49%|████▊     | 110M/227M [00:09<00:09, 11.7MB/s]\u001b[A\n",
      "Downloading data:  49%|████▉     | 111M/227M [00:09<00:09, 11.7MB/s]\u001b[A\n",
      "Downloading data:  50%|████▉     | 113M/227M [00:09<00:09, 11.7MB/s]\u001b[A\n",
      "Downloading data:  50%|█████     | 114M/227M [00:09<00:09, 11.8MB/s]\u001b[A\n",
      "Downloading data:  51%|█████     | 115M/227M [00:09<00:09, 11.7MB/s]\u001b[A\n",
      "Downloading data:  51%|█████▏    | 116M/227M [00:09<00:09, 11.7MB/s]\u001b[A\n",
      "Downloading data:  52%|█████▏    | 117M/227M [00:10<00:09, 11.7MB/s]\u001b[A\n",
      "Downloading data:  52%|█████▏    | 119M/227M [00:10<00:09, 11.7MB/s]\u001b[A\n",
      "Downloading data:  53%|█████▎    | 120M/227M [00:10<00:11, 8.92MB/s]\u001b[A\n",
      "Downloading data:  53%|█████▎    | 121M/227M [00:10<00:11, 9.59MB/s]\u001b[A\n",
      "Downloading data:  54%|█████▍    | 122M/227M [00:10<00:10, 10.1MB/s]\u001b[A\n",
      "Downloading data:  54%|█████▍    | 123M/227M [00:10<00:09, 10.6MB/s]\u001b[A\n",
      "Downloading data:  55%|█████▍    | 124M/227M [00:10<00:09, 10.9MB/s]\u001b[A\n",
      "Downloading data:  55%|█████▌    | 126M/227M [00:10<00:09, 11.1MB/s]\u001b[A\n",
      "Downloading data:  56%|█████▌    | 127M/227M [00:10<00:08, 11.3MB/s]\u001b[A\n",
      "Downloading data:  56%|█████▋    | 128M/227M [00:11<00:08, 11.4MB/s]\u001b[A\n",
      "Downloading data:  57%|█████▋    | 129M/227M [00:11<00:08, 11.5MB/s]\u001b[A\n",
      "Downloading data:  58%|█████▊    | 130M/227M [00:11<00:08, 11.6MB/s]\u001b[A\n",
      "Downloading data:  58%|█████▊    | 131M/227M [00:11<00:08, 11.6MB/s]\u001b[A\n",
      "Downloading data:  59%|█████▊    | 133M/227M [00:11<00:08, 11.7MB/s]\u001b[A\n",
      "Downloading data:  59%|█████▉    | 134M/227M [00:11<00:07, 11.7MB/s]\u001b[A\n",
      "Downloading data:  60%|█████▉    | 135M/227M [00:11<00:07, 11.7MB/s]\u001b[A\n",
      "Downloading data:  60%|██████    | 136M/227M [00:11<00:07, 11.7MB/s]\u001b[A\n",
      "Downloading data:  61%|██████    | 137M/227M [00:11<00:07, 11.8MB/s]\u001b[A\n",
      "Downloading data:  61%|██████    | 139M/227M [00:11<00:07, 11.7MB/s]\u001b[A\n",
      "Downloading data:  62%|██████▏   | 140M/227M [00:12<00:07, 11.7MB/s]\u001b[A\n",
      "Downloading data:  62%|██████▏   | 141M/227M [00:12<00:07, 11.7MB/s]\u001b[A\n",
      "Downloading data:  63%|██████▎   | 142M/227M [00:12<00:07, 11.7MB/s]\u001b[A\n",
      "Downloading data:  63%|██████▎   | 143M/227M [00:12<00:07, 11.7MB/s]\u001b[A\n",
      "Downloading data:  64%|██████▍   | 144M/227M [00:12<00:06, 11.7MB/s]\u001b[A\n",
      "Downloading data:  64%|██████▍   | 146M/227M [00:12<00:06, 11.7MB/s]\u001b[A\n",
      "Downloading data:  65%|██████▍   | 147M/227M [00:12<00:06, 11.7MB/s]\u001b[A\n",
      "Downloading data:  65%|██████▌   | 148M/227M [00:12<00:06, 11.7MB/s]\u001b[A\n",
      "Downloading data:  66%|██████▌   | 149M/227M [00:12<00:06, 11.7MB/s]\u001b[A\n",
      "Downloading data:  66%|██████▋   | 150M/227M [00:12<00:06, 11.7MB/s]\u001b[A\n",
      "Downloading data:  67%|██████▋   | 152M/227M [00:13<00:06, 11.7MB/s]\u001b[A\n",
      "Downloading data:  67%|██████▋   | 153M/227M [00:13<00:06, 11.7MB/s]\u001b[A\n",
      "Downloading data:  68%|██████▊   | 154M/227M [00:13<00:06, 11.7MB/s]\u001b[A\n",
      "Downloading data:  68%|██████▊   | 155M/227M [00:13<00:06, 11.7MB/s]\u001b[A\n",
      "Downloading data:  69%|██████▉   | 156M/227M [00:13<00:05, 11.7MB/s]\u001b[A\n",
      "Downloading data:  70%|██████▉   | 157M/227M [00:13<00:05, 11.7MB/s]\u001b[A\n",
      "Downloading data:  70%|███████   | 159M/227M [00:13<00:05, 11.7MB/s]\u001b[A\n",
      "Downloading data:  71%|███████   | 160M/227M [00:13<00:05, 11.7MB/s]\u001b[A\n",
      "Downloading data:  71%|███████   | 161M/227M [00:13<00:05, 11.7MB/s]\u001b[A\n",
      "Downloading data:  72%|███████▏  | 162M/227M [00:13<00:05, 11.7MB/s]\u001b[A\n",
      "Downloading data:  72%|███████▏  | 163M/227M [00:14<00:05, 11.8MB/s]\u001b[A\n",
      "Downloading data:  73%|███████▎  | 165M/227M [00:14<00:05, 11.7MB/s]\u001b[A\n",
      "Downloading data:  73%|███████▎  | 166M/227M [00:14<00:05, 11.7MB/s]\u001b[A\n",
      "Downloading data:  74%|███████▎  | 167M/227M [00:14<00:05, 11.7MB/s]\u001b[A\n",
      "Downloading data:  74%|███████▍  | 168M/227M [00:14<00:04, 11.7MB/s]\u001b[A\n",
      "Downloading data:  75%|███████▍  | 169M/227M [00:14<00:04, 11.7MB/s]\u001b[A\n",
      "Downloading data:  75%|███████▌  | 170M/227M [00:14<00:04, 11.8MB/s]\u001b[A\n",
      "Downloading data:  76%|███████▌  | 172M/227M [00:14<00:04, 11.8MB/s]\u001b[A\n",
      "Downloading data:  76%|███████▋  | 173M/227M [00:14<00:04, 11.8MB/s]\u001b[A\n",
      "Downloading data:  77%|███████▋  | 174M/227M [00:14<00:04, 11.7MB/s]\u001b[A\n",
      "Downloading data:  77%|███████▋  | 175M/227M [00:15<00:04, 11.6MB/s]\u001b[A\n",
      "Downloading data:  78%|███████▊  | 176M/227M [00:15<00:04, 11.6MB/s]\u001b[A\n",
      "Downloading data:  78%|███████▊  | 178M/227M [00:15<00:04, 11.7MB/s]\u001b[A\n",
      "Downloading data:  79%|███████▉  | 179M/227M [00:15<00:04, 11.6MB/s]\u001b[A\n",
      "Downloading data:  79%|███████▉  | 180M/227M [00:15<00:03, 11.7MB/s]\u001b[A\n",
      "Downloading data:  80%|███████▉  | 181M/227M [00:15<00:03, 11.7MB/s]\u001b[A\n",
      "Downloading data:  80%|████████  | 182M/227M [00:15<00:03, 11.7MB/s]\u001b[A\n",
      "Downloading data:  81%|████████  | 183M/227M [00:15<00:03, 11.7MB/s]\u001b[A\n",
      "Downloading data:  82%|████████▏ | 185M/227M [00:15<00:03, 11.7MB/s]\u001b[A\n",
      "Downloading data:  82%|████████▏ | 186M/227M [00:15<00:03, 11.7MB/s]\u001b[A\n",
      "Downloading data:  83%|████████▎ | 187M/227M [00:16<00:03, 11.7MB/s]\u001b[A\n",
      "Downloading data:  83%|████████▎ | 188M/227M [00:16<00:03, 11.7MB/s]\u001b[A\n",
      "Downloading data:  84%|████████▎ | 189M/227M [00:16<00:03, 11.7MB/s]\u001b[A\n",
      "Downloading data:  84%|████████▍ | 191M/227M [00:16<00:03, 11.7MB/s]\u001b[A\n",
      "Downloading data:  85%|████████▍ | 192M/227M [00:16<00:02, 11.7MB/s]\u001b[A\n",
      "Downloading data:  85%|████████▌ | 193M/227M [00:16<00:02, 11.7MB/s]\u001b[A\n",
      "Downloading data:  86%|████████▌ | 194M/227M [00:16<00:02, 11.6MB/s]\u001b[A\n",
      "Downloading data:  86%|████████▌ | 195M/227M [00:16<00:02, 11.8MB/s]\u001b[A\n",
      "Downloading data:  87%|████████▋ | 196M/227M [00:16<00:02, 11.8MB/s]\u001b[A\n",
      "Downloading data:  87%|████████▋ | 198M/227M [00:16<00:02, 11.8MB/s]\u001b[A\n",
      "Downloading data:  88%|████████▊ | 199M/227M [00:17<00:02, 11.8MB/s]\u001b[A\n",
      "Downloading data:  88%|████████▊ | 200M/227M [00:17<00:02, 11.7MB/s]\u001b[A\n",
      "Downloading data:  89%|████████▉ | 201M/227M [00:17<00:02, 11.7MB/s]\u001b[A\n",
      "Downloading data:  89%|████████▉ | 202M/227M [00:17<00:02, 11.7MB/s]\u001b[A\n",
      "Downloading data:  90%|████████▉ | 204M/227M [00:17<00:01, 11.7MB/s]\u001b[A\n",
      "Downloading data:  90%|█████████ | 205M/227M [00:17<00:01, 11.7MB/s]\u001b[A\n",
      "Downloading data:  91%|█████████ | 206M/227M [00:17<00:01, 11.7MB/s]\u001b[A\n",
      "Downloading data:  91%|█████████▏| 207M/227M [00:17<00:01, 11.7MB/s]\u001b[A\n",
      "Downloading data:  92%|█████████▏| 208M/227M [00:17<00:01, 11.5MB/s]\u001b[A\n",
      "Downloading data:  92%|█████████▏| 210M/227M [00:18<00:01, 11.8MB/s]\u001b[A\n",
      "Downloading data:  93%|█████████▎| 211M/227M [00:18<00:01, 11.8MB/s]\u001b[A\n",
      "Downloading data:  94%|█████████▎| 212M/227M [00:18<00:01, 11.8MB/s]\u001b[A\n",
      "Downloading data:  94%|█████████▍| 213M/227M [00:18<00:01, 11.8MB/s]\u001b[A\n",
      "Downloading data:  95%|█████████▍| 214M/227M [00:18<00:01, 11.8MB/s]\u001b[A\n",
      "Downloading data:  95%|█████████▌| 215M/227M [00:18<00:00, 11.7MB/s]\u001b[A\n",
      "Downloading data:  96%|█████████▌| 217M/227M [00:18<00:00, 11.7MB/s]\u001b[A\n",
      "Downloading data:  96%|█████████▌| 218M/227M [00:18<00:00, 11.7MB/s]\u001b[A\n",
      "Downloading data:  97%|█████████▋| 219M/227M [00:18<00:00, 11.7MB/s]\u001b[A\n",
      "Downloading data:  97%|█████████▋| 220M/227M [00:18<00:00, 11.7MB/s]\u001b[A\n",
      "Downloading data:  98%|█████████▊| 221M/227M [00:19<00:00, 11.7MB/s]\u001b[A\n",
      "Downloading data:  98%|█████████▊| 223M/227M [00:19<00:00, 11.7MB/s]\u001b[A\n",
      "Downloading data:  99%|█████████▊| 224M/227M [00:19<00:00, 11.7MB/s]\u001b[A\n",
      "Downloading data:  99%|█████████▉| 225M/227M [00:19<00:00, 11.7MB/s]\u001b[A\n",
      "Downloading data: 100%|██████████| 227M/227M [00:19<00:00, 11.6MB/s]\u001b[A\n",
      "Downloading data files:  50%|█████     | 1/2 [00:21<00:21, 21.19s/it]\n",
      "Downloading data:   0%|          | 0.00/56.9M [00:00<?, ?B/s]\u001b[A\n",
      "Downloading data:   0%|          | 2.05k/56.9M [00:00<1:49:22, 8.67kB/s]\u001b[A\n",
      "Downloading data:   0%|          | 34.8k/56.9M [00:00<11:05, 85.4kB/s]  \u001b[A\n",
      "Downloading data:   0%|          | 104k/56.9M [00:00<05:13, 181kB/s]  \u001b[A\n",
      "Downloading data:   0%|          | 209k/56.9M [00:00<03:19, 285kB/s]\u001b[A\n",
      "Downloading data:   1%|          | 453k/56.9M [00:01<01:41, 555kB/s]\u001b[A\n",
      "Downloading data:   2%|▏         | 940k/56.9M [00:01<00:52, 1.07MB/s]\u001b[A\n",
      "Downloading data:   3%|▎         | 1.90M/56.9M [00:01<00:26, 2.05MB/s]\u001b[A\n",
      "Downloading data:   5%|▌         | 3.09M/56.9M [00:01<00:14, 3.66MB/s]\u001b[A\n",
      "Downloading data:   7%|▋         | 3.82M/56.9M [00:01<00:12, 4.10MB/s]\u001b[A\n",
      "Downloading data:   9%|▉         | 5.00M/56.9M [00:01<00:09, 5.66MB/s]\u001b[A\n",
      "Downloading data:  10%|█         | 5.72M/56.9M [00:02<00:10, 5.09MB/s]\u001b[A\n",
      "Downloading data:  12%|█▏        | 6.90M/56.9M [00:02<00:07, 6.51MB/s]\u001b[A\n",
      "Downloading data:  14%|█▍        | 8.08M/56.9M [00:02<00:06, 7.73MB/s]\u001b[A\n",
      "Downloading data:  16%|█▌        | 9.25M/56.9M [00:02<00:05, 8.61MB/s]\u001b[A\n",
      "Downloading data:  18%|█▊        | 10.5M/56.9M [00:02<00:04, 9.61MB/s]\u001b[A\n",
      "Downloading data:  21%|██        | 11.7M/56.9M [00:02<00:04, 10.2MB/s]\u001b[A\n",
      "Downloading data:  23%|██▎       | 12.9M/56.9M [00:02<00:04, 10.6MB/s]\u001b[A\n",
      "Downloading data:  25%|██▍       | 14.0M/56.9M [00:02<00:03, 10.9MB/s]\u001b[A\n",
      "Downloading data:  27%|██▋       | 15.2M/56.9M [00:02<00:03, 11.2MB/s]\u001b[A\n",
      "Downloading data:  29%|██▉       | 16.4M/56.9M [00:03<00:03, 11.3MB/s]\u001b[A\n",
      "Downloading data:  31%|███       | 17.6M/56.9M [00:03<00:03, 11.5MB/s]\u001b[A\n",
      "Downloading data:  33%|███▎      | 18.7M/56.9M [00:03<00:03, 11.4MB/s]\u001b[A\n",
      "Downloading data:  35%|███▌      | 20.0M/56.9M [00:03<00:03, 11.6MB/s]\u001b[A\n",
      "Downloading data:  37%|███▋      | 21.1M/56.9M [00:03<00:03, 11.7MB/s]\u001b[A\n",
      "Downloading data:  39%|███▉      | 22.3M/56.9M [00:03<00:02, 11.7MB/s]\u001b[A\n",
      "Downloading data:  41%|████▏     | 23.5M/56.9M [00:03<00:02, 11.7MB/s]\u001b[A\n",
      "Downloading data:  43%|████▎     | 24.7M/56.9M [00:03<00:02, 11.7MB/s]\u001b[A\n",
      "Downloading data:  45%|████▌     | 25.9M/56.9M [00:03<00:02, 11.7MB/s]\u001b[A\n",
      "Downloading data:  48%|████▊     | 27.0M/56.9M [00:03<00:02, 11.7MB/s]\u001b[A\n",
      "Downloading data:  50%|████▉     | 28.2M/56.9M [00:04<00:02, 11.7MB/s]\u001b[A\n",
      "Downloading data:  52%|█████▏    | 29.4M/56.9M [00:04<00:02, 11.8MB/s]\u001b[A\n",
      "Downloading data:  54%|█████▎    | 30.6M/56.9M [00:04<00:02, 11.7MB/s]\u001b[A\n",
      "Downloading data:  56%|█████▌    | 31.8M/56.9M [00:04<00:02, 11.7MB/s]\u001b[A\n",
      "Downloading data:  58%|█████▊    | 32.9M/56.9M [00:04<00:02, 11.7MB/s]\u001b[A\n",
      "Downloading data:  60%|█████▉    | 34.1M/56.9M [00:04<00:01, 11.8MB/s]\u001b[A\n",
      "Downloading data:  62%|██████▏   | 35.3M/56.9M [00:04<00:01, 11.7MB/s]\u001b[A\n",
      "Downloading data:  64%|██████▍   | 36.5M/56.9M [00:04<00:01, 11.7MB/s]\u001b[A\n",
      "Downloading data:  66%|██████▌   | 37.7M/56.9M [00:04<00:01, 11.7MB/s]\u001b[A\n",
      "Downloading data:  68%|██████▊   | 38.9M/56.9M [00:04<00:01, 11.7MB/s]\u001b[A\n",
      "Downloading data:  70%|███████   | 40.0M/56.9M [00:05<00:01, 11.7MB/s]\u001b[A\n",
      "Downloading data:  72%|███████▏  | 41.2M/56.9M [00:05<00:01, 11.7MB/s]\u001b[A\n",
      "Downloading data:  74%|███████▍  | 42.4M/56.9M [00:05<00:01, 11.7MB/s]\u001b[A\n",
      "Downloading data:  77%|███████▋  | 43.6M/56.9M [00:05<00:01, 11.8MB/s]\u001b[A\n",
      "Downloading data:  79%|███████▊  | 44.8M/56.9M [00:05<00:01, 11.7MB/s]\u001b[A\n",
      "Downloading data:  81%|████████  | 45.9M/56.9M [00:05<00:00, 11.7MB/s]\u001b[A\n",
      "Downloading data:  83%|████████▎ | 47.1M/56.9M [00:05<00:00, 11.7MB/s]\u001b[A\n",
      "Downloading data:  85%|████████▍ | 48.3M/56.9M [00:05<00:00, 11.8MB/s]\u001b[A\n",
      "Downloading data:  87%|████████▋ | 49.5M/56.9M [00:05<00:00, 11.7MB/s]\u001b[A\n",
      "Downloading data:  89%|████████▉ | 50.7M/56.9M [00:05<00:00, 11.7MB/s]\u001b[A\n",
      "Downloading data:  91%|█████████ | 51.8M/56.9M [00:06<00:00, 11.7MB/s]\u001b[A\n",
      "Downloading data:  93%|█████████▎| 53.0M/56.9M [00:06<00:00, 11.7MB/s]\u001b[A\n",
      "Downloading data:  95%|█████████▌| 54.2M/56.9M [00:06<00:00, 11.7MB/s]\u001b[A\n",
      "Downloading data:  97%|█████████▋| 55.4M/56.9M [00:06<00:00, 11.7MB/s]\u001b[A\n",
      "Downloading data: 100%|██████████| 56.9M/56.9M [00:06<00:00, 8.72MB/s]\u001b[A\n",
      "Downloading data files: 100%|██████████| 2/2 [00:31<00:00, 15.65s/it]\n",
      "Extracting data files: 100%|██████████| 2/2 [00:00<00:00, 3851.52it/s]\n",
      "                                                                   \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset parquet downloaded and prepared to /home/todsavadt/.cache/huggingface/datasets/codeparrot___parquet/codeparrot--github-jupyter-code-to-text-14ed47a6ba32f30c/0.0.0/2a3b91fbd88a2c90d1dbbb32b460cf621d31bd5b05b934492fdef7d8d6f236ec. Subsequent calls will reuse this data.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 2/2 [00:00<00:00, 1236.71it/s]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train: Dataset({\n",
       "        features: ['repo_name', 'path', 'license', 'content'],\n",
       "        num_rows: 47452\n",
       "    })\n",
       "    test: Dataset({\n",
       "        features: ['repo_name', 'path', 'license', 'content'],\n",
       "        num_rows: 11864\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import datasets \n",
    "dataset = datasets.load_dataset(\"codeparrot/github-jupyter-code-to-text\")\n",
    "dataset"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Preprocessing"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tokenizing\n",
    "\n",
    "Simply tokenize the given text to tokens."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                   \r"
     ]
    },
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train: Dataset({\n",
       "        features: ['tokens'],\n",
       "        num_rows: 47452\n",
       "    })\n",
       "    test: Dataset({\n",
       "        features: ['tokens'],\n",
       "        num_rows: 11864\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer = torchtext.data.utils.get_tokenizer('basic_english')\n",
    "\n",
    "tokenize_data = lambda example, tokenizer: {'tokens': tokenizer(example['content'])}\n",
    "\n",
    "tokenized_dataset = dataset.map(\n",
    "    tokenize_data, \n",
    "    remove_columns=['repo_name', 'path', 'license', 'content'], \n",
    "    fn_kwargs={'tokenizer': tokenizer}\n",
    ")\n",
    "\n",
    "tokenized_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['import', 'numpy', 'as', 'np', 'from', 'tensorflow', 'import', 'keras', 'from', 'tensorflow', '.', 'keras', 'import', 'layers', 'explanation', 'simple', 'mnist', 'convnet', 'author', 'fchollet<br>', 'date', 'created', '2015/06/19<br>', 'last', 'modified', '2020/04/21<br>', 'description', 'a', 'simple', 'convnet', 'that', 'achieves', '~99%', 'test', 'accuracy', 'on', 'mnist', '.', 'setup', 'end', 'of', 'explanation', '#', 'model', '/', 'data', 'parameters', 'num_classes', '=', '10', 'input_shape', '=', '(', '28', ',', '28', ',', '1', ')', '#', 'the', 'data', ',', 'split', 'between', 'train', 'and', 'test', 'sets', '(', 'x_train', ',', 'y_train', ')', ',', '(', 'x_test', ',', 'y_test', ')', '=', 'keras', '.', 'datasets', '.', 'mnist', '.', 'load_data', '(', ')', '#', 'scale', 'images', 'to', 'the', '[0', ',', '1]', 'range', 'x_train', '=', 'x_train', '.', 'astype', '(', 'float32', ')', '/', '255', 'x_test', '=', 'x_test', '.', 'astype', '(', 'float32', ')', '/', '255', '#', 'make', 'sure', 'images', 'have', 'shape', '(', '28', ',', '28', ',', '1', ')', 'x_train', '=', 'np', '.', 'expand_dims', '(', 'x_train', ',', '-1', ')', 'x_test', '=', 'np', '.', 'expand_dims', '(', 'x_test', ',', '-1', ')', 'print', '(', 'x_train', 'shape', ',', 'x_train', '.', 'shape', ')', 'print', '(', 'x_train', '.', 'shape[0]', ',', 'train', 'samples', ')', 'print', '(', 'x_test', '.', 'shape[0]', ',', 'test', 'samples', ')', '#', 'convert', 'class', 'vectors', 'to', 'binary', 'class', 'matrices', 'y_train', '=', 'keras', '.', 'utils', '.', 'to_categorical', '(', 'y_train', ',', 'num_classes', ')', 'y_test', '=', 'keras', '.', 'utils', '.', 'to_categorical', '(', 'y_test', ',', 'num_classes', ')', 'explanation', 'prepare', 'the', 'data', 'end', 'of', 'explanation', 'model', '=', 'keras', '.', 'sequential', '(', '[', 'keras', '.', 'input', '(', 'shape=input_shape', ')', ',', 'layers', '.', 'conv2d', '(', '32', ',', 'kernel_size=', '(', '3', ',', '3', ')', ',', 'activation=relu', ')', ',', 'layers', '.', 'maxpooling2d', '(', 'pool_size=', '(', '2', ',', '2', ')', ')', ',', 'layers', '.', 'conv2d', '(', '64', ',', 'kernel_size=', '(', '3', ',', '3', ')', ',', 'activation=relu', ')', ',', 'layers', '.', 'maxpooling2d', '(', 'pool_size=', '(', '2', ',', '2', ')', ')', ',', 'layers', '.', 'flatten', '(', ')', ',', 'layers', '.', 'dropout', '(', '0', '.', '5', ')', ',', 'layers', '.', 'dense', '(', 'num_classes', ',', 'activation=softmax', ')', ',', ']', ')', 'model', '.', 'summary', '(', ')', 'explanation', 'build', 'the', 'model', 'end', 'of', 'explanation', 'batch_size', '=', '128', 'epochs', '=', '15', 'model', '.', 'compile', '(', 'loss=categorical_crossentropy', ',', 'optimizer=adam', ',', 'metrics=[accuracy]', ')', 'model', '.', 'fit', '(', 'x_train', ',', 'y_train', ',', 'batch_size=batch_size', ',', 'epochs=epochs', ',', 'validation_split=0', '.', '1', ')', 'explanation', 'train', 'the', 'model', 'end', 'of', 'explanation', 'score', '=', 'model', '.', 'evaluate', '(', 'x_test', ',', 'y_test', ',', 'verbose=0', ')', 'print', '(', 'test', 'loss', ',', 'score[0]', ')', 'print', '(', 'test', 'accuracy', ',', 'score[1]', ')', 'explanation', 'evaluate', 'the', 'trained', 'model', 'end', 'of', 'explanation']\n"
     ]
    }
   ],
   "source": [
    "print(tokenized_dataset['train'][0]['tokens'])"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Numericalizing\n",
    "\n",
    "We will tell torchtext to add any word that has occurred at least three times in the dataset to the vocabulary because otherwise it would be too big.  Also we shall make sure to add `unk` and `eos`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab = torchtext.vocab.build_vocab_from_iterator(\n",
    "    tokenized_dataset['train']['tokens'], \n",
    "    min_freq=3,\n",
    "    specials=['<unk>', '<eos>'],\n",
    "    special_first=True\n",
    ")\n",
    "\n",
    "# vocab.insert_token('<unk>', 0)\n",
    "# vocab.insert_token('<eos>', 1)\n",
    "vocab.set_default_index(vocab['<unk>'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "611138\n"
     ]
    }
   ],
   "source": [
    "print(len(vocab))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['<unk>', '<eos>', '.', ',', ')', '(', \"'\", 'the', '=', 'of']\n"
     ]
    }
   ],
   "source": [
    "print(vocab.get_itos()[:10])"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Prepare the batch loader\n",
    "\n",
    "### Prepare data\n",
    "\n",
    "Given \"Chaky loves eating at AIT\", and \"I really love deep learning\", and given batch size = 3, we will get three batches of data \"Chaky loves eating at\", \"AIT `<eos>` I really\", \"love deep learning `<eos>`\".  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def get_data(dataset, vocab, batch_size):\n",
    "    data = []\n",
    "    for example in tqdm(dataset):\n",
    "        if example['tokens']:\n",
    "            tokens = example['tokens'].append('<eos>')\n",
    "            tokens = [vocab[token] for token in example['tokens']]\n",
    "            data.extend(tokens)\n",
    "    data = torch.LongTensor(data)\n",
    "    num_batches = data.shape[0] // batch_size\n",
    "    data = data[:num_batches * batch_size]\n",
    "    data = data.view(batch_size, num_batches) #view vs. reshape (whether data is contiguous)\n",
    "    return data #[batch size, seq len]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenized_dataset_train = tokenized_dataset['train'].select(range(1000))\n",
    "tokenized_dataset_test = tokenized_dataset['test'].select(range(100))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1000/1000 [00:00<00:00, 1027.01it/s]\n",
      "100%|██████████| 100/100 [00:00<00:00, 1107.97it/s]\n"
     ]
    }
   ],
   "source": [
    "batch_size = 32\n",
    "train_data = get_data(tokenized_dataset_train, vocab, batch_size)\n",
    "valid_data = get_data(tokenized_dataset_test, vocab, batch_size)\n",
    "# test_data  = get_data(tokenized_dataset['test'], vocab, batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([32, 68772]), torch.Size([32, 6375]))"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_data.shape, valid_data.shape #[batch_size, all the next length]"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Modeling "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "class LSTMLanguageModel(nn.Module):\n",
    "    def __init__(self, vocab_size, emb_dim, hid_dim, num_layers, dropout_rate):\n",
    "        super().__init__()\n",
    "        self.num_layers = num_layers\n",
    "        self.hid_dim    = hid_dim\n",
    "        self.emb_dim    = emb_dim\n",
    "        \n",
    "        self.embedding  = nn.Embedding(vocab_size, emb_dim)\n",
    "        self.lstm       = nn.LSTM(emb_dim, hid_dim, num_layers=num_layers, dropout=dropout_rate, batch_first=True)\n",
    "        self.dropout    = nn.Dropout(dropout_rate)\n",
    "        self.fc         = nn.Linear(hid_dim, vocab_size)\n",
    "        \n",
    "        self.init_weights()\n",
    "    \n",
    "    def init_weights(self):\n",
    "        init_range_emb = 0.1\n",
    "        init_range_other = 1/math.sqrt(self.hid_dim)\n",
    "        self.embedding.weight.data.uniform_(-init_range_emb, init_range_other)\n",
    "        self.fc.weight.data.uniform_(-init_range_other, init_range_other)\n",
    "        self.fc.bias.data.zero_()\n",
    "        for i in range(self.num_layers):\n",
    "            self.lstm.all_weights[i][0] = torch.FloatTensor(self.emb_dim,\n",
    "                self.hid_dim).uniform_(-init_range_other, init_range_other) #We\n",
    "            self.lstm.all_weights[i][1] = torch.FloatTensor(self.hid_dim,   \n",
    "                self.hid_dim).uniform_(-init_range_other, init_range_other) #Wh\n",
    "    \n",
    "    def init_hidden(self, batch_size, device):\n",
    "        hidden = torch.zeros(self.num_layers, batch_size, self.hid_dim).to(device)\n",
    "        cell   = torch.zeros(self.num_layers, batch_size, self.hid_dim).to(device)\n",
    "        return hidden, cell\n",
    "        \n",
    "    def detach_hidden(self, hidden):\n",
    "        hidden, cell = hidden\n",
    "        hidden = hidden.detach() #not to be used for gradient computation\n",
    "        cell   = cell.detach()\n",
    "        return hidden, cell\n",
    "        \n",
    "    def forward(self, src, hidden):\n",
    "        #src: [batch_size, seq len]\n",
    "        embedding = self.dropout(self.embedding(src)) #harry potter is\n",
    "        #embedding: [batch-size, seq len, emb dim]\n",
    "        output, hidden = self.lstm(embedding, hidden)\n",
    "        #ouput: [batch size, seq len, hid dim]\n",
    "        #hidden: [num_layers * direction, seq len, hid_dim]\n",
    "        output = self.dropout(output)\n",
    "        prediction =self.fc(output)\n",
    "        #prediction: [batch_size, seq_len, vocab_size]\n",
    "        return prediction, hidden"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Task 2. Model Training"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Training \n",
    "\n",
    "Follows very basic procedure.  One note is that some of the sequences that will be fed to the model may involve parts from different sequences in the original dataset or be a subset of one (depending on the decoding length). For this reason we will reset the hidden state every epoch, this is like assuming that the next batch of sequences is probably always a follow up on the previous in the original dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab_size = len(vocab)\n",
    "emb_dim = 1024                # 400 in the paper\n",
    "hid_dim = 1024                # 1150 in the paper\n",
    "num_layers = 2                # 3 in the paper\n",
    "dropout_rate = 0.65              \n",
    "lr = 1e-3                     "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The model has 1,269,015,362 trainable parameters\n"
     ]
    }
   ],
   "source": [
    "model = LSTMLanguageModel(vocab_size, emb_dim, hid_dim, num_layers, dropout_rate).to(device)\n",
    "optimizer = optim.Adam(model.parameters(), lr=lr)\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "num_params = sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
    "print(f'The model has {num_params:,} trainable parameters')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_batch(data, seq_len, idx):\n",
    "    #this data is from get_data()\n",
    "    #train_data.shape # [batch_size, number of batches....]\n",
    "    src    = data[:, idx:idx+seq_len]                   \n",
    "    target = data[:, idx+1:idx+seq_len+1]  #target simply is ahead of src by 1            \n",
    "    return src, target"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(model, data, optimizer, criterion, batch_size, seq_len, clip, device):\n",
    "    \n",
    "    epoch_loss = 0\n",
    "    model.train()\n",
    "    # drop all batches that are not a multiple of seq_len\n",
    "    num_batches = data.shape[-1]\n",
    "    data = data[:, :num_batches - (num_batches -1) % seq_len]\n",
    "    num_batches = data.shape[-1]\n",
    "\n",
    "    hidden = model.init_hidden(batch_size, device)\n",
    "    \n",
    "    for idx in tqdm(range(0, num_batches - 1, seq_len), desc='Training: ',leave=False):\n",
    "        optimizer.zero_grad()\n",
    "        hidden = model.detach_hidden(hidden)\n",
    "\n",
    "        src, target = get_batch(data, seq_len, idx) #src, target: [batch size, seq len]\n",
    "        src, target = src.to(device), target.to(device)\n",
    "        batch_size = src.shape[0]\n",
    "        prediction, hidden = model(src, hidden)               \n",
    "\n",
    "        prediction = prediction.reshape(batch_size * seq_len, -1)  #prediction: [batch size * seq len, vocab size]  \n",
    "        target = target.reshape(-1)\n",
    "        loss = criterion(prediction, target)\n",
    "        \n",
    "        loss.backward()\n",
    "        torch.nn.utils.clip_grad_norm_(model.parameters(), clip) #prevent gradient explosion - clip is basically \n",
    "        optimizer.step()\n",
    "        epoch_loss += loss.detach().item() * seq_len\n",
    "    return epoch_loss / num_batches"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate(model, data, criterion, batch_size, seq_len, device):\n",
    "\n",
    "    epoch_loss = 0\n",
    "    model.eval()\n",
    "    num_batches = data.shape[-1]\n",
    "    data = data[:, :num_batches - (num_batches -1) % seq_len]\n",
    "    num_batches = data.shape[-1]\n",
    "\n",
    "    hidden = model.init_hidden(batch_size, device)\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for idx in range(0, num_batches - 1, seq_len):\n",
    "            hidden = model.detach_hidden(hidden)\n",
    "            src, target = get_batch(data, seq_len, idx)\n",
    "            src, target = src.to(device), target.to(device)\n",
    "            batch_size= src.shape[0]\n",
    "\n",
    "            prediction, hidden = model(src, hidden)\n",
    "            prediction = prediction.reshape(batch_size * seq_len, -1)\n",
    "            target = target.reshape(-1)\n",
    "\n",
    "            loss = criterion(prediction, target)\n",
    "            epoch_loss += loss.detach().item() * seq_len\n",
    "    return epoch_loss / num_batches"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                             \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tepoch: 1\n",
      "\tTrain Perplexity: 1654.533\n",
      "\tValid Perplexity: 1671.347\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                             \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tepoch: 2\n",
      "\tTrain Perplexity: 1426.690\n",
      "\tValid Perplexity: 642.907\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                             \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tepoch: 3\n",
      "\tTrain Perplexity: 134.567\n",
      "\tValid Perplexity: 154.949\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                             \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tepoch: 4\n",
      "\tTrain Perplexity: 72.070\n",
      "\tValid Perplexity: 120.079\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                             \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tepoch: 5\n",
      "\tTrain Perplexity: 52.214\n",
      "\tValid Perplexity: 103.758\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                             \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tepoch: 6\n",
      "\tTrain Perplexity: 41.170\n",
      "\tValid Perplexity: 94.365\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                             \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tepoch: 7\n",
      "\tTrain Perplexity: 34.144\n",
      "\tValid Perplexity: 87.731\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                             \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tepoch: 8\n",
      "\tTrain Perplexity: 29.301\n",
      "\tValid Perplexity: 85.357\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                             \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tepoch: 9\n",
      "\tTrain Perplexity: 25.687\n",
      "\tValid Perplexity: 86.338\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                             \r"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[20], line 10\u001b[0m\n\u001b[1;32m      7\u001b[0m best_valid_loss \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mfloat\u001b[39m(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124minf\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m      9\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m epoch \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(n_epochs):\n\u001b[0;32m---> 10\u001b[0m     train_loss \u001b[38;5;241m=\u001b[39m \u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtrain_data\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moptimizer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcriterion\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\n\u001b[1;32m     11\u001b[0m \u001b[43m                \u001b[49m\u001b[43mbatch_size\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mseq_len\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mclip\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     12\u001b[0m     valid_loss \u001b[38;5;241m=\u001b[39m evaluate(model, valid_data, criterion, batch_size, \n\u001b[1;32m     13\u001b[0m                 seq_len, device)\n\u001b[1;32m     15\u001b[0m     lr_scheduler\u001b[38;5;241m.\u001b[39mstep(valid_loss)\n",
      "Cell \u001b[0;32mIn[18], line 28\u001b[0m, in \u001b[0;36mtrain\u001b[0;34m(model, data, optimizer, criterion, batch_size, seq_len, clip, device)\u001b[0m\n\u001b[1;32m     26\u001b[0m     torch\u001b[38;5;241m.\u001b[39mnn\u001b[38;5;241m.\u001b[39mutils\u001b[38;5;241m.\u001b[39mclip_grad_norm_(model\u001b[38;5;241m.\u001b[39mparameters(), clip) \u001b[38;5;66;03m#prevent gradient explosion - clip is basically \u001b[39;00m\n\u001b[1;32m     27\u001b[0m     optimizer\u001b[38;5;241m.\u001b[39mstep()\n\u001b[0;32m---> 28\u001b[0m     epoch_loss \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[43mloss\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdetach\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mitem\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m \u001b[38;5;241m*\u001b[39m seq_len\n\u001b[1;32m     29\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m epoch_loss \u001b[38;5;241m/\u001b[39m num_batches\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "n_epochs = 10\n",
    "seq_len  = 50\n",
    "clip    = 0.25\n",
    "\n",
    "lr_scheduler = optim.lr_scheduler.ReduceLROnPlateau(optimizer, factor=0.5, patience=0)\n",
    "\n",
    "best_valid_loss = float('inf')\n",
    "\n",
    "for epoch in range(n_epochs):\n",
    "    train_loss = train(model, train_data, optimizer, criterion, \n",
    "                batch_size, seq_len, clip, device)\n",
    "    valid_loss = evaluate(model, valid_data, criterion, batch_size, \n",
    "                seq_len, device)\n",
    "\n",
    "    lr_scheduler.step(valid_loss)\n",
    "\n",
    "    if valid_loss < best_valid_loss:\n",
    "        best_valid_loss = valid_loss\n",
    "        torch.save(model.state_dict(), './models/best-val-auto.pt')\n",
    "        \n",
    "    print(f'\\tepoch: {epoch+1}')\n",
    "    print(f'\\tTrain Perplexity: {math.exp(train_loss):.3f}')\n",
    "    print(f'\\tValid Perplexity: {math.exp(valid_loss):.3f}')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.load_state_dict(torch.load('./models/best-val-auto.pt',  map_location=device))\n",
    "# test_loss = evaluate(model, test_data, criterion, batch_size, seq_len, device)\n",
    "# print(f'Test Perplexity: {math.exp(test_loss):.3f}')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Real-world inference\n",
    "\n",
    "Here we take the prompt, tokenize, encode and feed it into the model to get the predictions.  We then apply softmax while specifying that we want the output due to the last word in the sequence which represents the prediction for the next word.  We divide the logits by a temperature value to alter the model’s confidence by adjusting the softmax probability distribution.\n",
    "\n",
    "Once we have the Softmax distribution, we randomly sample it to make our prediction on the next word. If we get <unk> then we give that another try.  Once we get <eos> we stop predicting.\n",
    "    \n",
    "We decode the prediction back to strings last lines."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate(prompt, max_seq_len, temperature, model, tokenizer, vocab, device, seed=None):\n",
    "    if seed is not None:\n",
    "        torch.manual_seed(seed)\n",
    "    model.eval()\n",
    "    tokens = tokenizer(prompt)\n",
    "    indices = [vocab[t] for t in tokens]\n",
    "    batch_size = 1\n",
    "    hidden = model.init_hidden(batch_size, device)\n",
    "    with torch.no_grad():\n",
    "        for i in range(max_seq_len):\n",
    "            src = torch.LongTensor([indices]).to(device)\n",
    "            prediction, hidden = model(src, hidden)\n",
    "            \n",
    "            #prediction: [batch size, seq len, vocab size]\n",
    "            #prediction[:, -1]: [batch size, vocab size] #probability of last vocab\n",
    "            \n",
    "            probs = torch.softmax(prediction[:, -1] / temperature, dim=-1)  \n",
    "            prediction = torch.multinomial(probs, num_samples=1).item()    \n",
    "            \n",
    "            while prediction == vocab['<unk>']: #if it is unk, we sample again\n",
    "                prediction = torch.multinomial(probs, num_samples=1).item()\n",
    "\n",
    "            if prediction == vocab['<eos>']:    #if it is eos, we stop\n",
    "                break\n",
    "\n",
    "            indices.append(prediction) #autoregressive, thus output becomes input\n",
    "\n",
    "    itos = vocab.get_itos()\n",
    "    tokens = [itos[i] for i in indices]\n",
    "    return tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.5\n",
      "import matplotlib . pyplot as plt %matplotlib inline explanation load the data end of explanation # create a function to use the following data to visualize the data . # get a set of the data and the output column , which is to be able to be able to do this . #\n",
      "\n",
      "0.7\n",
      "import matplotlib . pyplot as plt plt . rcparams[ ' figure . figsize ' ] = ' nearest ' plt . rcparams[ ' figure . figsize ' ] = ' gray ' # from mne . stats import gridsearchcv from sklearn . grid_search import randomforestclassifier from sklearn . model_selection import train_test_split , query from\n",
      "\n",
      "0.75\n",
      "import matplotlib . pyplot as plt plt . rcparams[ ' figure . figsize ' ] = ' nearest ' plt . rcparams[ ' figure . figsize ' ] = ' gray ' # from mne . naive_bayes import aligninfo from mne . mplot3d import randomforestclassifier from mne . api import decisiontreeregressor from sklearn .\n",
      "\n",
      "0.8\n",
      "import matplotlib . pyplot as plt plt . rcparams[ ' figure . figsize ' ] = 5 # plt . xlabel ( ' head ( mm ) ' ) plt . title ( ' histogram of ground truth ' ) plt . xlabel ( ' epoch ' ) plt . ylabel ( ' error\n",
      "\n",
      "1.0\n",
      "import matplotlib . pyplot as plt plt . rcparams[ ' figure . figsize ' ] = 5 # set up the latent output print max ( x_train , y_train ) explanation we can compute the experiment of each output in the test-set ( examples limit in seconds ) by the values ? query the\n",
      "\n"
     ]
    }
   ],
   "source": [
    "prompt = 'import matplotlib.pyplot'\n",
    "max_seq_len = 50\n",
    "seed = 0\n",
    "            #superdiverse   more diverse\n",
    "temperatures = [0.5, 0.7, 0.75, 0.8, 1.0] \n",
    "#sample from this distribution higher probability will get more change\n",
    "for temperature in temperatures:\n",
    "    generation = generate(prompt, max_seq_len, temperature, model, tokenizer, \n",
    "                          vocab, device, seed)\n",
    "    print(str(temperature)+'\\n'+' '.join(generation)+'\\n')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  },
  "vscode": {
   "interpreter": {
    "hash": "c81d839d3c4227cd770621df97fe8191838af02e7eef185a922d8250cb33d344"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
