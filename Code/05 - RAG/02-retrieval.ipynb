{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Natural Language Processing\n",
    "\n",
    "# Retrieval\n",
    "Many LLM applications require user-specific data that is not part of the model's training set. \n",
    "The primary way of accomplishing this is through `Retrieval Augmented Generation (RAG). `\n",
    "In this process, external data is retrieved and then passed to the LLM when doing the generation step.\n",
    "\n",
    "LangChain provides all the building blocks for RAG applications - from simple to complex.\n",
    "\n",
    "This section of the documentation covers everything related to the retrieval step \n",
    "\n",
    "<img src=\"./figures/retrieval.jpeg\" >\n",
    "\n",
    "1. `Document loaders` : Load documents from many different sources (HTML, PDF, code). \n",
    "2. `Document transformers` : One of the essential steps in document retrieval is breaking down a large document into smaller, relevant chunks to enhance the retrieval process.\n",
    "3. `Text embedding models` : Embeddings capture the semantic meaning of the text, allowing you to quickly and efficiently find other pieces of text that are similar.\n",
    "4. `Vector stores`: there has emerged a need for databases to support efficient storage and searching of these embeddings.\n",
    "5. `Retrievers` : Once the data is in the database, you still need to retrieve it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "# Set GPU device\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"1\"\n",
    "\n",
    "os.environ['http_proxy']  = 'http://192.41.170.23:3128'\n",
    "os.environ['https_proxy'] = 'http://192.41.170.23:3128'"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Document loaders\n",
    "- built-in document loader integrations with 3rd-party tools.\n",
    "- Use document loaders to load data from a source as Document's. \n",
    "- A Document is a piece of text and associated metadata. (.txt .html .md .json)\n",
    "- Document loaders provide a \"load\" method for loading data as documents from a configured source. "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### CSV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(5013, 5)"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "csv_path = './docs/csv/OpenThaiGPT_SelfInstruct_Generated.csv'\n",
    "df = pd.read_csv(csv_path)\n",
    "df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "5013"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Load CSV data with a single row per document.\n",
    "from langchain.document_loaders.csv_loader import CSVLoader\n",
    "\n",
    "loader = CSVLoader(file_path=csv_path)\n",
    "data = loader.load()\n",
    "len(data)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### PDF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "22"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# !pip install pypdf\n",
    "from langchain.document_loaders import PyPDFLoader\n",
    "\n",
    "loader = PyPDFLoader(\"./docs/pdf/MachineLearning-Lecture01.pdf\")\n",
    "pages = loader.load_and_split()\n",
    "len(pages)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "22"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# !pip install pymupdf\n",
    "from langchain.document_loaders import PyMuPDFLoader\n",
    "loader = PyMuPDFLoader(\"./docs/pdf/MachineLearning-Lecture01.pdf\")\n",
    "pages = loader.load()\n",
    "len(pages)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# !pip3 install pdfminer\n",
    "# !pip3 install pdfminer-six\n",
    "from langchain.document_loaders import OnlinePDFLoader\n",
    "loader = OnlinePDFLoader(\"https://arxiv.org/pdf/1706.03762.pdf\")\n",
    "pages = loader.load()\n",
    "len(pages)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Suggest exploring a variety of loading techniques for diverse data sources. \n",
    "[Link](https://python.langchain.com/docs/modules/data_connection/document_loaders/pdf)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Document transformers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip3 install pdfminer\n",
    "# !pip3 install pdfminer-six\n",
    "from langchain.document_loaders import OnlinePDFLoader\n",
    "loader = OnlinePDFLoader(\"https://arxiv.org/pdf/1706.03762.pdf\")\n",
    "pages = loader.load()\n",
    "len(pages)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "79"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "text_splitter = RecursiveCharacterTextSplitter(\n",
    "    chunk_size = 700,\n",
    "    chunk_overlap = 100\n",
    ")\n",
    "\n",
    "docs = text_splitter.split_documents(pages)\n",
    "len(docs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "assert len(docs) >= len(pages)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Suggest exploring a variety of spliting techniques.\n",
    "[Link](https://python.langchain.com/docs/modules/data_connection/document_transformers/)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Text embedding models\n",
    "\n",
    "The Embeddings class is a class designed for interfacing with text embedding models. \n",
    "\n",
    "There are lots of embedding model providers (OpenAI, Cohere, Hugging Face, etc) - this class is designed to provide a standard interface for all of them.\n",
    "\n",
    "Embeddings create a vector representation of a piece of text. This is useful because it means we can think about text in the vector space, and do things like semantic search where we look for pieces of text that are most similar in the vector space."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/guntsv/Library/Python/3.9/lib/python/site-packages/urllib3/__init__.py:34: NotOpenSSLWarning: urllib3 v2.0 only supports OpenSSL 1.1.1+, currently the 'ssl' module is compiled with 'LibreSSL 2.8.3'. See: https://github.com/urllib3/urllib3/issues/3020\n",
      "  warnings.warn(\n",
      "/Users/guntsv/Library/Python/3.9/lib/python/site-packages/InstructorEmbedding/instructor.py:7: TqdmExperimentalWarning: Using `tqdm.autonotebook.tqdm` in notebook mode. Use `tqdm.tqdm` instead to force console mode (e.g. in jupyter console)\n",
      "  from tqdm.autonotebook import trange\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "load INSTRUCTOR_Transformer\n",
      "'NoneType' object has no attribute 'cadam32bit_grad_fp32'\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/guntsv/Library/Python/3.9/lib/python/site-packages/bitsandbytes/cextension.py:34: UserWarning: The installed version of bitsandbytes was compiled without GPU support. 8-bit optimizers, 8-bit multiplication, and GPU quantization are unavailable.\n",
      "  warn(\"The installed version of bitsandbytes was compiled without GPU support. \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "max_seq_length  512\n"
     ]
    }
   ],
   "source": [
    "from langchain.embeddings import HuggingFaceInstructEmbeddings\n",
    "import torch\n",
    "\n",
    "embedding_model = HuggingFaceInstructEmbeddings(\n",
    "        model_name = 'hkunlp/instructor-base',              \n",
    "        model_kwargs = {\n",
    "            'device': torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "        },\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "query_A = embedding_model.embed_query('Chacky love to eat sushi.')\n",
    "query_B = embedding_model.embed_query('Chacky don\\'t love to eat Durian.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/yg/7zr68xws2x5d3vc4nhz2l7wc0000gn/T/ipykernel_20868/3324528492.py:3: UserWarning: The use of `x.T` on tensors of dimension other than 2 to reverse their shape is deprecated and it will throw an error in a future release. Consider `x.mT` to transpose batches of matrices or `x.permute(*torch.arange(x.ndim - 1, -1, -1))` to reverse the dimensions of a tensor. (Triggered internally at /Users/runner/work/pytorch/pytorch/pytorch/aten/src/ATen/native/TensorShape.cpp:3575.)\n",
      "  cos_sim = lambda a,b: (a @ b.T) / (norm(a)*norm(b))\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "tensor(0.9132)"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from numpy.linalg import norm\n",
    "\n",
    "cos_sim = lambda a,b: (a @ b.T) / (norm(a)*norm(b))\n",
    "cos_sim(torch.Tensor(query_A), torch.Tensor(query_B))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "5"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "embeddings = embedding_model.embed_documents(\n",
    "    [\n",
    "        \"Hi there!\",\n",
    "        \"Oh, hello!\",\n",
    "        \"What's your name?\",\n",
    "        \"My friends call me World\",\n",
    "        \"Hello World!\"\n",
    "    ]\n",
    ")\n",
    "len(embeddings)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Vector stores\n",
    "\n",
    "One of the most common ways to store and search over unstructured data is to embed it and store the resulting embedding vectors, and then at query time to embed the unstructured query and retrieve the embedding vectors that are 'most similar' to the embedded query. A vector store takes care of storing embedded data and performing vector search for you.\n",
    "\n",
    "<img src=\"./figures/vectorstores.jpeg\" >"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install faiss-cpu\n",
    "# !pip install faiss-gpu"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "from langchain.document_loaders import PyMuPDFLoader\n",
    "from langchain.vectorstores import FAISS\n",
    "#STEP 3\n",
    "loader = PyMuPDFLoader(\"./docs/pdf/MachineLearning-Lecture01.pdf\")\n",
    "docs = loader.load()\n",
    "\n",
    "text_splitter = RecursiveCharacterTextSplitter(\n",
    "    chunk_size=700, \n",
    "    chunk_overlap=100\n",
    ")\n",
    "\n",
    "documents = text_splitter.split_documents(docs)\n",
    "#STEP 4\n",
    "vectordb = FAISS.from_documents(\n",
    "    documents, \n",
    "    embedding_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "vectordb_path = 'vectordb_path'\n",
    "db_file_name = 'ml-andrew-ng'\n",
    "vectordb.save_local(\n",
    "    os.path.join(vectordb_path, db_file_name)\n",
    ")\n",
    "\n",
    "vectordb = FAISS.load_local(\n",
    "        folder_path = db_file_name,\n",
    "        embeddings  = embedding_model\n",
    "    ) "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Similarity search"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Document(page_content=\"And one of the most interesting things we'll talk about later this quarter is what if your \\ndata doesn't lie in a two-dimensional or three-dimensional or sort of even a finite \\ndimensional space, but is it possible — what if your data actually lies in an infinite \\ndimensional space? Our plots here are two-dimensional space. I can't plot you an infinite \\ndimensional space, right? And so it turns out that one of the most successful classes of \\nmachine learning algorithms — some may call support vector machines — actually takes \\ndata and maps data to an infinite dimensional space and then does classification using not \\ntwo features like I've done here, but an infinite number of features.\", metadata={'source': './docs/pdf/MachineLearning-Lecture01.pdf', 'file_path': './docs/pdf/MachineLearning-Lecture01.pdf', 'page': 13, 'total_pages': 22, 'format': 'PDF 1.4', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'creator': 'PScript5.dll Version 5.2.2', 'producer': 'Acrobat Distiller 8.1.0 (Windows)', 'creationDate': \"D:20080711112523-07'00'\", 'modDate': \"D:20080711112523-07'00'\", 'trapped': ''}),\n",
       " Document(page_content='So my friend was very excited. He said, \"Wow. That\\'s great. I\\'m glad to hear this \\nmachine learning stuff was actually useful. So what was it that you learned? Was it \\nlogistic regression? Was it the PCA? Was it the data networks? What was it that you \\nlearned that was so helpful?\" And the student said, \"Oh, it was the MATLAB.\"  \\nSo for those of you that don\\'t know MATLAB yet, I hope you do learn it. It\\'s not hard, \\nand we\\'ll actually have a short MATLAB tutorial in one of the discussion sections for \\nthose of you that don\\'t know it.  \\nOkay. The very last piece of logistical thing is the discussion sections. So discussion', metadata={'source': './docs/pdf/MachineLearning-Lecture01.pdf', 'file_path': './docs/pdf/MachineLearning-Lecture01.pdf', 'page': 8, 'total_pages': 22, 'format': 'PDF 1.4', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'creator': 'PScript5.dll Version 5.2.2', 'producer': 'Acrobat Distiller 8.1.0 (Windows)', 'creationDate': \"D:20080711112523-07'00'\", 'modDate': \"D:20080711112523-07'00'\", 'trapped': ''}),\n",
       " Document(page_content=\"then we want the algorithm to learn the association between the inputs and the outputs \\nand to sort of give us more of the right answers, okay?  \\nIt turns out this specific example that I drew here is an example of something called a \\nregression problem. And the term regression sort of refers to the fact that the variable \\nyou're trying to predict is a continuous value and price.  \\nThere's another class of supervised learning problems which we'll talk about, which are \\nclassification problems. And so, in a classification problem, the variable you're trying to \\npredict is discreet rather than continuous. So as one specific example — so actually a\", metadata={'source': './docs/pdf/MachineLearning-Lecture01.pdf', 'file_path': './docs/pdf/MachineLearning-Lecture01.pdf', 'page': 12, 'total_pages': 22, 'format': 'PDF 1.4', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'creator': 'PScript5.dll Version 5.2.2', 'producer': 'Acrobat Distiller 8.1.0 (Windows)', 'creationDate': \"D:20080711112523-07'00'\", 'modDate': \"D:20080711112523-07'00'\", 'trapped': ''}),\n",
       " Document(page_content=\"why for this class I'm going to ask you to do most of your programming in MATLAB and \\nOctave because if you try to implement the same algorithm in C or Java or something, I \\ncan tell you from personal, painful experience, you end up writing pages and pages of \\ncode rather than relatively few lines of code. I'll also mention that it did take researchers \\nmany, many years to come up with that one line of code, so this is not easy.  \\nSo that was unsupervised learning, and then the last of the four major topics I wanna tell \\nyou about is reinforcement learning. And this refers to problems where you don't do one-\", metadata={'source': './docs/pdf/MachineLearning-Lecture01.pdf', 'file_path': './docs/pdf/MachineLearning-Lecture01.pdf', 'page': 19, 'total_pages': 22, 'format': 'PDF 1.4', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'creator': 'PScript5.dll Version 5.2.2', 'producer': 'Acrobat Distiller 8.1.0 (Windows)', 'creationDate': \"D:20080711112523-07'00'\", 'modDate': \"D:20080711112523-07'00'\", 'trapped': ''})]"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "query = \"What is Liner Regression\"\n",
    "docs = vectordb.similarity_search(query)\n",
    "docs"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Similarity search by vector\n",
    "It is also possible to do a search for documents similar to a given embedding vector using similarity_search_by_vector which accepts an embedding vector as a parameter instead of a string."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Document(page_content=\"And one of the most interesting things we'll talk about later this quarter is what if your \\ndata doesn't lie in a two-dimensional or three-dimensional or sort of even a finite \\ndimensional space, but is it possible — what if your data actually lies in an infinite \\ndimensional space? Our plots here are two-dimensional space. I can't plot you an infinite \\ndimensional space, right? And so it turns out that one of the most successful classes of \\nmachine learning algorithms — some may call support vector machines — actually takes \\ndata and maps data to an infinite dimensional space and then does classification using not \\ntwo features like I've done here, but an infinite number of features.\", metadata={'source': './docs/pdf/MachineLearning-Lecture01.pdf', 'file_path': './docs/pdf/MachineLearning-Lecture01.pdf', 'page': 13, 'total_pages': 22, 'format': 'PDF 1.4', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'creator': 'PScript5.dll Version 5.2.2', 'producer': 'Acrobat Distiller 8.1.0 (Windows)', 'creationDate': \"D:20080711112523-07'00'\", 'modDate': \"D:20080711112523-07'00'\", 'trapped': ''}),\n",
       " Document(page_content='So my friend was very excited. He said, \"Wow. That\\'s great. I\\'m glad to hear this \\nmachine learning stuff was actually useful. So what was it that you learned? Was it \\nlogistic regression? Was it the PCA? Was it the data networks? What was it that you \\nlearned that was so helpful?\" And the student said, \"Oh, it was the MATLAB.\"  \\nSo for those of you that don\\'t know MATLAB yet, I hope you do learn it. It\\'s not hard, \\nand we\\'ll actually have a short MATLAB tutorial in one of the discussion sections for \\nthose of you that don\\'t know it.  \\nOkay. The very last piece of logistical thing is the discussion sections. So discussion', metadata={'source': './docs/pdf/MachineLearning-Lecture01.pdf', 'file_path': './docs/pdf/MachineLearning-Lecture01.pdf', 'page': 8, 'total_pages': 22, 'format': 'PDF 1.4', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'creator': 'PScript5.dll Version 5.2.2', 'producer': 'Acrobat Distiller 8.1.0 (Windows)', 'creationDate': \"D:20080711112523-07'00'\", 'modDate': \"D:20080711112523-07'00'\", 'trapped': ''}),\n",
       " Document(page_content=\"then we want the algorithm to learn the association between the inputs and the outputs \\nand to sort of give us more of the right answers, okay?  \\nIt turns out this specific example that I drew here is an example of something called a \\nregression problem. And the term regression sort of refers to the fact that the variable \\nyou're trying to predict is a continuous value and price.  \\nThere's another class of supervised learning problems which we'll talk about, which are \\nclassification problems. And so, in a classification problem, the variable you're trying to \\npredict is discreet rather than continuous. So as one specific example — so actually a\", metadata={'source': './docs/pdf/MachineLearning-Lecture01.pdf', 'file_path': './docs/pdf/MachineLearning-Lecture01.pdf', 'page': 12, 'total_pages': 22, 'format': 'PDF 1.4', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'creator': 'PScript5.dll Version 5.2.2', 'producer': 'Acrobat Distiller 8.1.0 (Windows)', 'creationDate': \"D:20080711112523-07'00'\", 'modDate': \"D:20080711112523-07'00'\", 'trapped': ''}),\n",
       " Document(page_content=\"why for this class I'm going to ask you to do most of your programming in MATLAB and \\nOctave because if you try to implement the same algorithm in C or Java or something, I \\ncan tell you from personal, painful experience, you end up writing pages and pages of \\ncode rather than relatively few lines of code. I'll also mention that it did take researchers \\nmany, many years to come up with that one line of code, so this is not easy.  \\nSo that was unsupervised learning, and then the last of the four major topics I wanna tell \\nyou about is reinforcement learning. And this refers to problems where you don't do one-\", metadata={'source': './docs/pdf/MachineLearning-Lecture01.pdf', 'file_path': './docs/pdf/MachineLearning-Lecture01.pdf', 'page': 19, 'total_pages': 22, 'format': 'PDF 1.4', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'creator': 'PScript5.dll Version 5.2.2', 'producer': 'Acrobat Distiller 8.1.0 (Windows)', 'creationDate': \"D:20080711112523-07'00'\", 'modDate': \"D:20080711112523-07'00'\", 'trapped': ''})]"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "query = \"What is Liner Regression\"\n",
    "embedding_vector = embedding_model.embed_query(query)\n",
    "docs = vectordb.similarity_search_by_vector(embedding_vector)\n",
    "docs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install pymupdf\n",
    "from langchain.document_loaders import PyMuPDFLoader\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "\n",
    "loader = PyMuPDFLoader(\"./docs/pdf/MachineLearning-Lecture01.pdf\")\n",
    "documents = loader.load()\n",
    "\n",
    "text_splitter = RecursiveCharacterTextSplitter(\n",
    "    chunk_size=700, \n",
    "    chunk_overlap=100\n",
    ")\n",
    "\n",
    "documents = text_splitter.split_documents(documents)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Retrievers\n",
    "\n",
    "A retriever is an interface that returns documents given an unstructured query. It is more general than a vector store. A retriever does not need to be able to store documents, only to return (or retrieve) them. Vector stores can be used as the backbone of a retriever, but there are other types of retrievers as well."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Document(page_content=\"And one of the most interesting things we'll talk about later this quarter is what if your \\ndata doesn't lie in a two-dimensional or three-dimensional or sort of even a finite \\ndimensional space, but is it possible — what if your data actually lies in an infinite \\ndimensional space? Our plots here are two-dimensional space. I can't plot you an infinite \\ndimensional space, right? And so it turns out that one of the most successful classes of \\nmachine learning algorithms — some may call support vector machines — actually takes \\ndata and maps data to an infinite dimensional space and then does classification using not \\ntwo features like I've done here, but an infinite number of features.\", metadata={'source': './docs/pdf/MachineLearning-Lecture01.pdf', 'file_path': './docs/pdf/MachineLearning-Lecture01.pdf', 'page': 13, 'total_pages': 22, 'format': 'PDF 1.4', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'creator': 'PScript5.dll Version 5.2.2', 'producer': 'Acrobat Distiller 8.1.0 (Windows)', 'creationDate': \"D:20080711112523-07'00'\", 'modDate': \"D:20080711112523-07'00'\", 'trapped': ''}),\n",
       " Document(page_content=\"you about is reinforcement learning. And this refers to problems where you don't do one-\\nshot decision-making. So, for example, in the supervised learning cancer prediction \\nproblem, you have a patient come in, you predict that the cancer is malignant or benign. \\nAnd then based on your prediction, maybe the patient lives or dies, and then that's it, \\nright? So you make a decision and then there's a consequence. You either got it right or \\nwrong. In reinforcement learning problems, you are usually asked to make a sequence of \\ndecisions over time.  \\nSo, for example, this is something that my students and I work on. If I give you the keys\", metadata={'source': './docs/pdf/MachineLearning-Lecture01.pdf', 'file_path': './docs/pdf/MachineLearning-Lecture01.pdf', 'page': 19, 'total_pages': 22, 'format': 'PDF 1.4', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'creator': 'PScript5.dll Version 5.2.2', 'producer': 'Acrobat Distiller 8.1.0 (Windows)', 'creationDate': \"D:20080711112523-07'00'\", 'modDate': \"D:20080711112523-07'00'\", 'trapped': ''}),\n",
       " Document(page_content='So my friend was very excited. He said, \"Wow. That\\'s great. I\\'m glad to hear this \\nmachine learning stuff was actually useful. So what was it that you learned? Was it \\nlogistic regression? Was it the PCA? Was it the data networks? What was it that you \\nlearned that was so helpful?\" And the student said, \"Oh, it was the MATLAB.\"  \\nSo for those of you that don\\'t know MATLAB yet, I hope you do learn it. It\\'s not hard, \\nand we\\'ll actually have a short MATLAB tutorial in one of the discussion sections for \\nthose of you that don\\'t know it.  \\nOkay. The very last piece of logistical thing is the discussion sections. So discussion', metadata={'source': './docs/pdf/MachineLearning-Lecture01.pdf', 'file_path': './docs/pdf/MachineLearning-Lecture01.pdf', 'page': 8, 'total_pages': 22, 'format': 'PDF 1.4', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'creator': 'PScript5.dll Version 5.2.2', 'producer': 'Acrobat Distiller 8.1.0 (Windows)', 'creationDate': \"D:20080711112523-07'00'\", 'modDate': \"D:20080711112523-07'00'\", 'trapped': ''}),\n",
       " Document(page_content=\"Microphone 1:  \\nOne, two, three, four, five, six, seven, eight, nine, ten.  \\nMicrophone 2:  \\n[Music playing.]  \\nInstructor (Andrew Ng) : Right. And we get this data. It's the same unsupervised \\nlearning algorithm. The algorithm is actually called independent component analysis, and \\nlater in this quarter, you'll see why. And then output's the following:  \\nMicrophone 1:  \\nOne, two, three, four, five, six, seven, eight, nine, ten.  \\nInstructor (Andrew Ng): And that's the second one:  \\nMicrophone 2:  \\n[Music playing.]  \\nInstructor (Andrew Ng): Okay. So it turns out that beyond solving the cocktail party \\nalgorithm, this specific class of unsupervised learning algorithms are also applied to a\", metadata={'source': './docs/pdf/MachineLearning-Lecture01.pdf', 'file_path': './docs/pdf/MachineLearning-Lecture01.pdf', 'page': 18, 'total_pages': 22, 'format': 'PDF 1.4', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'creator': 'PScript5.dll Version 5.2.2', 'producer': 'Acrobat Distiller 8.1.0 (Windows)', 'creationDate': \"D:20080711112523-07'00'\", 'modDate': \"D:20080711112523-07'00'\", 'trapped': ''})]"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "retriever = vectordb.as_retriever(search_type=\"mmr\")\n",
    "docs = retriever.get_relevant_documents(\"What is Linear Regression\")\n",
    "docs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Document(page_content=\"And one of the most interesting things we'll talk about later this quarter is what if your \\ndata doesn't lie in a two-dimensional or three-dimensional or sort of even a finite \\ndimensional space, but is it possible — what if your data actually lies in an infinite \\ndimensional space? Our plots here are two-dimensional space. I can't plot you an infinite \\ndimensional space, right? And so it turns out that one of the most successful classes of \\nmachine learning algorithms — some may call support vector machines — actually takes \\ndata and maps data to an infinite dimensional space and then does classification using not \\ntwo features like I've done here, but an infinite number of features.\", metadata={'source': './docs/pdf/MachineLearning-Lecture01.pdf', 'file_path': './docs/pdf/MachineLearning-Lecture01.pdf', 'page': 13, 'total_pages': 22, 'format': 'PDF 1.4', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'creator': 'PScript5.dll Version 5.2.2', 'producer': 'Acrobat Distiller 8.1.0 (Windows)', 'creationDate': \"D:20080711112523-07'00'\", 'modDate': \"D:20080711112523-07'00'\", 'trapped': ''}),\n",
       " Document(page_content='So my friend was very excited. He said, \"Wow. That\\'s great. I\\'m glad to hear this \\nmachine learning stuff was actually useful. So what was it that you learned? Was it \\nlogistic regression? Was it the PCA? Was it the data networks? What was it that you \\nlearned that was so helpful?\" And the student said, \"Oh, it was the MATLAB.\"  \\nSo for those of you that don\\'t know MATLAB yet, I hope you do learn it. It\\'s not hard, \\nand we\\'ll actually have a short MATLAB tutorial in one of the discussion sections for \\nthose of you that don\\'t know it.  \\nOkay. The very last piece of logistical thing is the discussion sections. So discussion', metadata={'source': './docs/pdf/MachineLearning-Lecture01.pdf', 'file_path': './docs/pdf/MachineLearning-Lecture01.pdf', 'page': 8, 'total_pages': 22, 'format': 'PDF 1.4', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'creator': 'PScript5.dll Version 5.2.2', 'producer': 'Acrobat Distiller 8.1.0 (Windows)', 'creationDate': \"D:20080711112523-07'00'\", 'modDate': \"D:20080711112523-07'00'\", 'trapped': ''}),\n",
       " Document(page_content=\"then we want the algorithm to learn the association between the inputs and the outputs \\nand to sort of give us more of the right answers, okay?  \\nIt turns out this specific example that I drew here is an example of something called a \\nregression problem. And the term regression sort of refers to the fact that the variable \\nyou're trying to predict is a continuous value and price.  \\nThere's another class of supervised learning problems which we'll talk about, which are \\nclassification problems. And so, in a classification problem, the variable you're trying to \\npredict is discreet rather than continuous. So as one specific example — so actually a\", metadata={'source': './docs/pdf/MachineLearning-Lecture01.pdf', 'file_path': './docs/pdf/MachineLearning-Lecture01.pdf', 'page': 12, 'total_pages': 22, 'format': 'PDF 1.4', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'creator': 'PScript5.dll Version 5.2.2', 'producer': 'Acrobat Distiller 8.1.0 (Windows)', 'creationDate': \"D:20080711112523-07'00'\", 'modDate': \"D:20080711112523-07'00'\", 'trapped': ''}),\n",
       " Document(page_content=\"And this sort of learning problem of learning to predict housing prices is an example of \\nwhat's called a supervised learning problem. And the reason that it's called supervised \\nlearning is because we're providing the algorithm a data set of a bunch of square \\nfootages, a bunch of housing sizes, and as well as sort of the right answer of what the \\nactual prices of a number of houses were, right?  \\nSo we call this supervised learning because we're supervising the algorithm or, in other \\nwords, we're giving the algorithm the, quote, right answer for a number of houses. And \\nthen we want the algorithm to learn the association between the inputs and the outputs\", metadata={'source': './docs/pdf/MachineLearning-Lecture01.pdf', 'file_path': './docs/pdf/MachineLearning-Lecture01.pdf', 'page': 12, 'total_pages': 22, 'format': 'PDF 1.4', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'creator': 'PScript5.dll Version 5.2.2', 'producer': 'Acrobat Distiller 8.1.0 (Windows)', 'creationDate': \"D:20080711112523-07'00'\", 'modDate': \"D:20080711112523-07'00'\", 'trapped': ''})]"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "retriever = vectordb.as_retriever(search_type=\"similarity_score_threshold\", search_kwargs={\"score_threshold\": .5})\n",
    "docs = retriever.get_relevant_documents(\"What is Linear Regression\")\n",
    "docs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Document(page_content=\"And one of the most interesting things we'll talk about later this quarter is what if your \\ndata doesn't lie in a two-dimensional or three-dimensional or sort of even a finite \\ndimensional space, but is it possible — what if your data actually lies in an infinite \\ndimensional space? Our plots here are two-dimensional space. I can't plot you an infinite \\ndimensional space, right? And so it turns out that one of the most successful classes of \\nmachine learning algorithms — some may call support vector machines — actually takes \\ndata and maps data to an infinite dimensional space and then does classification using not \\ntwo features like I've done here, but an infinite number of features.\", metadata={'source': './docs/pdf/MachineLearning-Lecture01.pdf', 'file_path': './docs/pdf/MachineLearning-Lecture01.pdf', 'page': 13, 'total_pages': 22, 'format': 'PDF 1.4', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'creator': 'PScript5.dll Version 5.2.2', 'producer': 'Acrobat Distiller 8.1.0 (Windows)', 'creationDate': \"D:20080711112523-07'00'\", 'modDate': \"D:20080711112523-07'00'\", 'trapped': ''}),\n",
       " Document(page_content='So my friend was very excited. He said, \"Wow. That\\'s great. I\\'m glad to hear this \\nmachine learning stuff was actually useful. So what was it that you learned? Was it \\nlogistic regression? Was it the PCA? Was it the data networks? What was it that you \\nlearned that was so helpful?\" And the student said, \"Oh, it was the MATLAB.\"  \\nSo for those of you that don\\'t know MATLAB yet, I hope you do learn it. It\\'s not hard, \\nand we\\'ll actually have a short MATLAB tutorial in one of the discussion sections for \\nthose of you that don\\'t know it.  \\nOkay. The very last piece of logistical thing is the discussion sections. So discussion', metadata={'source': './docs/pdf/MachineLearning-Lecture01.pdf', 'file_path': './docs/pdf/MachineLearning-Lecture01.pdf', 'page': 8, 'total_pages': 22, 'format': 'PDF 1.4', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'creator': 'PScript5.dll Version 5.2.2', 'producer': 'Acrobat Distiller 8.1.0 (Windows)', 'creationDate': \"D:20080711112523-07'00'\", 'modDate': \"D:20080711112523-07'00'\", 'trapped': ''}),\n",
       " Document(page_content=\"then we want the algorithm to learn the association between the inputs and the outputs \\nand to sort of give us more of the right answers, okay?  \\nIt turns out this specific example that I drew here is an example of something called a \\nregression problem. And the term regression sort of refers to the fact that the variable \\nyou're trying to predict is a continuous value and price.  \\nThere's another class of supervised learning problems which we'll talk about, which are \\nclassification problems. And so, in a classification problem, the variable you're trying to \\npredict is discreet rather than continuous. So as one specific example — so actually a\", metadata={'source': './docs/pdf/MachineLearning-Lecture01.pdf', 'file_path': './docs/pdf/MachineLearning-Lecture01.pdf', 'page': 12, 'total_pages': 22, 'format': 'PDF 1.4', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'creator': 'PScript5.dll Version 5.2.2', 'producer': 'Acrobat Distiller 8.1.0 (Windows)', 'creationDate': \"D:20080711112523-07'00'\", 'modDate': \"D:20080711112523-07'00'\", 'trapped': ''})]"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "retriever = vectordb.as_retriever(search_kwargs={\"k\": 3})\n",
    "docs = retriever.get_relevant_documents(\"What is Linear Regression\")\n",
    "docs"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summary Step"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.document_loaders import PyMuPDFLoader\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "import torch\n",
    "from langchain.vectorstores import FAISS\n",
    "from langchain.embeddings import HuggingFaceInstructEmbeddings\n",
    "\n",
    "##STEP1 Document loaders\n",
    "folder_path = './docs/pdf/'\n",
    "pdf_list = []\n",
    "for filename in os.listdir(folder_path):\n",
    "    if filename.endswith('.pdf'):\n",
    "        pdf_list.append(folder_path + filename)\n",
    "\n",
    "documents = []\n",
    "pdf_loaders = [PyPDFLoader(pdf) for pdf in pdf_list]\n",
    "for loader in pdf_loaders:\n",
    "    documents.extend(loader.load())\n",
    "\n",
    "##STEP2 Document transformers\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "text_splitter = RecursiveCharacterTextSplitter(\n",
    "    chunk_size = 700,\n",
    "    chunk_overlap = 100\n",
    ")\n",
    "\n",
    "docs = text_splitter.split_documents(documents) \n",
    "\n",
    "##STEP3 Text embedding models\n",
    "from langchain.embeddings import HuggingFaceInstructEmbeddings\n",
    "embedding_model = HuggingFaceInstructEmbeddings(\n",
    "        model_name = 'hkunlp/instructor-base',              \n",
    "        model_kwargs = {\n",
    "            'device': torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "        },\n",
    "    )\n",
    "\n",
    "##STEP4 Vector stores\n",
    "vector_path = 'vectordb_path'\n",
    "db_file_name = 'ml_andrew_full_course'\n",
    "\n",
    "vectordb = FAISS.from_documents(\n",
    "        documents = docs, \n",
    "        embedding = embedding_model)\n",
    "\n",
    "vectordb.save_local(\n",
    "    os.path.join(vector_path, db_file_name)\n",
    ")\n",
    "\n",
    "##STEP5 Retrievers\n",
    "vectordb = FAISS.load_local(\n",
    "        folder_path = os.path.join(vector_path, db_file_name),\n",
    "        embeddings  = embedding_model\n",
    "    ) "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Appendix\n",
    "\n",
    "- [Multi-Vector Retriever for RAG on tables, text, and images](https://blog.langchain.dev/semi-structured-multi-modal-rag/)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Caching\n",
    "Embeddings can be stored or temporarily cached to avoid needing to recompute them.\n",
    "\n",
    "Caching embeddings can be done using a CacheBackedEmbeddings. \n",
    "\n",
    "The cache backed embedder is a wrapper around an embedder that caches embeddings in a key-value store. \n",
    "\n",
    "The text is hashed and the hash is used as the key in the cache.\n",
    "\n",
    "The main supported way to initialized a CacheBackedEmbeddings is from_bytes_store. This takes in the following parameters:\n",
    "- `underlying_embedder`: The embedder to use for embedding.\n",
    "- `document_embedding_cache`: The cache to use for storing document embeddings.\n",
    "- `namespace`: The namespace to use for document cache. This namespace is used to avoid collisions with other caches.\n",
    "- `Attention`: Be sure to set the namespace parameter to avoid collisions of the same text embedded using different embeddings models.\n",
    "\n",
    "#### With Caching vs Without caching"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/guntsv/Library/Python/3.9/lib/python/site-packages/urllib3/__init__.py:34: NotOpenSSLWarning: urllib3 v2.0 only supports OpenSSL 1.1.1+, currently the 'ssl' module is compiled with 'LibreSSL 2.8.3'. See: https://github.com/urllib3/urllib3/issues/3020\n",
      "  warnings.warn(\n",
      "/Users/guntsv/Library/Python/3.9/lib/python/site-packages/InstructorEmbedding/instructor.py:7: TqdmExperimentalWarning: Using `tqdm.autonotebook.tqdm` in notebook mode. Use `tqdm.tqdm` instead to force console mode (e.g. in jupyter console)\n",
      "  from tqdm.autonotebook import trange\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "load INSTRUCTOR_Transformer\n",
      "'NoneType' object has no attribute 'cadam32bit_grad_fp32'\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/guntsv/Library/Python/3.9/lib/python/site-packages/bitsandbytes/cextension.py:34: UserWarning: The installed version of bitsandbytes was compiled without GPU support. 8-bit optimizers, 8-bit multiplication, and GPU quantization are unavailable.\n",
      "  warn(\"The installed version of bitsandbytes was compiled without GPU support. \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "max_seq_length  512\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[]"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain.storage import (\n",
    "    InMemoryStore,\n",
    "    LocalFileStore,\n",
    "    RedisStore,\n",
    "    UpstashRedisStore,\n",
    ")\n",
    "from langchain.embeddings import CacheBackedEmbeddings\n",
    "from langchain.vectorstores import FAISS\n",
    "\n",
    "fs = LocalFileStore(\"./cache/\")\n",
    "\n",
    "from langchain.embeddings import HuggingFaceInstructEmbeddings\n",
    "import torch\n",
    "\n",
    "underlying_embeddings = HuggingFaceInstructEmbeddings(\n",
    "        model_name = 'hkunlp/instructor-base',              \n",
    "        model_kwargs = {\n",
    "            'device': torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "        },\n",
    "    )\n",
    "\n",
    "cached_embedder = CacheBackedEmbeddings.from_bytes_store(\n",
    "    underlying_embeddings, fs, namespace=underlying_embeddings.model_name\n",
    ")\n",
    "\n",
    "list(fs.yield_keys())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install pymupdf\n",
    "from langchain.document_loaders import PyMuPDFLoader\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "\n",
    "loader = PyMuPDFLoader(\"./docs/pdf/MachineLearning-Lecture01.pdf\")\n",
    "documents = loader.load()\n",
    "\n",
    "text_splitter = RecursiveCharacterTextSplitter(\n",
    "    chunk_size=700, \n",
    "    chunk_overlap=100\n",
    ")\n",
    "\n",
    "documents = text_splitter.split_documents(documents)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def epoch_time(start_time, end_time):\n",
    "    elapsed_time = end_time - start_time\n",
    "    elapsed_mins = int(elapsed_time / 60)\n",
    "    elapsed_secs = int(elapsed_time - (elapsed_mins * 60))\n",
    "    return elapsed_mins, elapsed_secs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time : 0 m 7 s\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "start_time = time.time()\n",
    "vectordb = FAISS.from_documents(documents, cached_embedder)\n",
    "vectordb2 = FAISS.from_documents(documents, cached_embedder)\n",
    "end_time = time.time()\n",
    "epoch_mins, epoch_secs = epoch_time(start_time, end_time)\n",
    "print(f'Time : {epoch_mins} m {epoch_secs} s')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "load INSTRUCTOR_Transformer\n",
      "max_seq_length  512\n"
     ]
    }
   ],
   "source": [
    "from langchain.embeddings import HuggingFaceInstructEmbeddings\n",
    "embedding_model = HuggingFaceInstructEmbeddings(\n",
    "        model_name = 'hkunlp/instructor-base',              \n",
    "        model_kwargs = {\n",
    "            'device': torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "        },\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time : 0 m 15 s\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "start_time = time.time()\n",
    "vectordb = FAISS.from_documents(documents, embedding_model)\n",
    "vectordb2 = FAISS.from_documents(documents, embedding_model)\n",
    "end_time = time.time()\n",
    "epoch_mins, epoch_secs = epoch_time(start_time, end_time)\n",
    "print(f'Time : {epoch_mins} m {epoch_secs} s')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### MultiQueryRetreiver\n",
    "\n",
    "Distance-based vector database retrieval relies on high-dimensional space representations to find similar documents, but query wording changes and inadequate embeddings can lead to varying results. The MultiQueryRetriever automates prompt tuning by generating diverse queries from a user input, collecting relevant documents for each query, and combining the results to potentially overcome the limitations of distance-based retrieval and provide a more comprehensive set of results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "# #STEP4\n",
    "db_file_name = './vectordb_path/ml-andrew-ng/'\n",
    "vectordb = FAISS.load_local(\n",
    "        folder_path = db_file_name,\n",
    "        embeddings  = embedding_model\n",
    "    ) \n",
    "# #STEP5\n",
    "retreiver = vectordb.as_retriever()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.chat_models import ChatOpenAI\n",
    "from langchain.retrievers.multi_query import MultiQueryRetriever\n",
    "\n",
    "llm = ChatOpenAI(temperature=0)\n",
    "retriever_from_llm = MultiQueryRetriever.from_llm(\n",
    "    retriever=vectordb.as_retriever(), llm=llm\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set logging for the queries\n",
    "import logging\n",
    "\n",
    "logging.basicConfig()\n",
    "logging.getLogger(\"langchain.retrievers.multi_query\").setLevel(logging.INFO)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:langchain.retrievers.multi_query:Generated queries: ['1. How do Linear Regression and Logistic Regression differ from each other?', '2. In what ways do Linear Regression and Logistic Regression vary?', '3. Can you explain the distinctions between Linear Regression and Logistic Regression?']\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "7"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "question = \"What are the difference between Linear Regression and Logistic Regression?\"\n",
    "unique_docs = retriever_from_llm.get_relevant_documents(query=question)\n",
    "len(unique_docs)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### MultiVector Retriever\n",
    "It can often be beneficial to store multiple vectors per document. There are multiple use cases where this is beneficial. LangChain has a base MultiVectorRetriever which makes querying this type of setup easy. A lot of the complexity lies in how to create the multiple vectors per document. This notebook covers some of the common ways to create those vectors and use the MultiVectorRetriever.\n",
    "\n",
    "The methods to create multiple vectors per document include:\n",
    "\n",
    "Smaller chunks: split a document into smaller chunks, and embed those (this is ParentDocumentRetriever).\n",
    "Summary: create a summary for each document, embed that along with (or instead of) the document.\n",
    "Hypothetical questions: create hypothetical questions that each document would be appropriate to answer, embed those along with (or instead of) the document.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.vectorstores import FAISS\n",
    "from langchain.embeddings import HuggingFaceInstructEmbeddings\n",
    "from langchain.document_loaders import PyPDFLoader\n",
    "\n",
    "folder_path = './docs/pdf'\n",
    "documents = [] \n",
    "pdf_list = []\n",
    "for filename in os.listdir(folder_path):\n",
    "    if filename.endswith('.pdf'):\n",
    "        pdf_list.append(folder_path + filename)\n",
    "\n",
    "pdf_loaders = [PyPDFLoader(pdf) for pdf in pdf_list]\n",
    "for loader in pdf_loaders:\n",
    "    documents.extend(loader.load())\n",
    "\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "text_splitter = RecursiveCharacterTextSplitter(\n",
    "    chunk_size = 700,\n",
    "    chunk_overlap = 100\n",
    ")\n",
    "docs = text_splitter.split_documents(documents) \n",
    "\n",
    "embedding_model = HuggingFaceInstructEmbeddings(\n",
    "        model_name = 'hkunlp/instructor-base',              \n",
    "        model_kwargs = {\n",
    "            'device': torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "        },\n",
    "    )\n",
    "\n",
    "db_file_name = 'ml-andrew-ng/'\n",
    "\n",
    "vectordb = FAISS.load_local(\n",
    "        folder_path = db_file_name,\n",
    "        embeddings  = embedding_model\n",
    "    ) \n",
    "\n",
    "retreiver = vectordb.as_retriever()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.retrievers.multi_vector import MultiVectorRetriever\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "from langchain.storage import InMemoryStore\n",
    "# from langchain.document_loaders import TextLoader\n",
    "\n",
    "# The storage layer for the parent documents\n",
    "store = InMemoryStore()\n",
    "id_key = \"doc_id\"\n",
    "# The retriever (empty to start)\n",
    "retriever = MultiVectorRetriever(\n",
    "    vectorstore=vectordb,\n",
    "    docstore=store,\n",
    "    id_key=id_key,\n",
    ")\n",
    "import uuid\n",
    "\n",
    "doc_ids = [str(uuid.uuid4()) for _ in docs]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The splitter to use to create smaller chunks\n",
    "child_text_splitter = RecursiveCharacterTextSplitter(chunk_size=400)\n",
    "\n",
    "sub_docs = []\n",
    "for i, doc in enumerate(docs):\n",
    "    _id = doc_ids[i]\n",
    "    _sub_docs = child_text_splitter.split_documents([doc])\n",
    "    for _doc in _sub_docs:\n",
    "        _doc.metadata[id_key] = _id\n",
    "    sub_docs.extend(_sub_docs)\n",
    "\n",
    "retriever.vectorstore.add_documents(sub_docs)\n",
    "retriever.docstore.mset(list(zip(doc_ids, docs)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sub_docs = vectorstore.similarity_search(\"What is Linear Regression\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
