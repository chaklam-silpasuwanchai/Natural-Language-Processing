{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Natural Language Processing\n",
    "\n",
    "# Retrieval-augmented generation (RAG)\n",
    "\n",
    "<img src=\"../figures/RAG-process.png\" >\n",
    "\n",
    "Introducing `ChakyBot`, an innovative chatbot designed to assist Chaky (the instructor) and TA (Gun) in explaining the intricacies of the NLP course to students. Leveraging LangChain technology, ChakyBot excels in retrieving pertinent information from documents, ensuring a seamless and efficient learning experience for students engaging with the NLP curriculum.\n",
    "\n",
    "1. Prompt\n",
    "2. Retrieval\n",
    "3. Memory\n",
    "4. Chain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# #langchain library\n",
    "# !pip install accelerate==0.25.0\n",
    "# !pip install langchain==0.0.350\n",
    "# #model library\n",
    "# !pip install transformers==4.36.1\n",
    "# !pip install bitsandbytes==0.41.2\n",
    "# !pip install sentence-transformers==2.2.2\n",
    "# !pip install InstructorEmbedding==1.0.1\n",
    "# #vectorstore\n",
    "# !pip install faiss-gpu==1.7.2\n",
    "# !pip install faiss-cpu==1.7.4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "# Set GPU device\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"0\"\n",
    "\n",
    "os.environ['http_proxy']  = 'http://192.41.170.23:3128'\n",
    "os.environ['https_proxy'] = 'http://192.41.170.23:3128'"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Prompt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.prompts import PromptTemplate\n",
    "\n",
    "prompt_template = \"\"\"\n",
    "    I'm your friendly NLP chatbot named ChakyBot, here to assist Chaky and Gun with any questions they have about Natural Language Processing (NLP). \n",
    "    If you're curious about how probability works in the context of NLP, feel free to ask any questions you may have. \n",
    "    Whether it's about probabilistic models, language models, or any other related topic, \n",
    "    I'm here to help break down complex concepts into easy-to-understand explanations.\n",
    "    Just let me know what you're wondering about, and I'll do my best to guide you through it!\n",
    "    {context}\n",
    "    Question: {question}\n",
    "    When encountering abusive, offensive, or harmful language, just politely ask the users to maintain appropriate behaviours.\n",
    "    Always make sure to elaborate your response and use vibrant, positive tone to represent good branding of the school.\n",
    "    Never answer with any unfinished response.\n",
    "    Answer:\n",
    "    \"\"\".strip()\n",
    "\n",
    "PROMPT = PromptTemplate(\n",
    "    template = prompt_template, \n",
    "    input_variables=[\"context\", \"question\"]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "StringPromptValue(text=\"I'm your friendly NLP chatbot named ChakyBot, here to assist Chaky and Gun with any questions they have about Natural Language Processing (NLP). \\n    If you're curious about how probability works in the context of NLP, feel free to ask any questions you may have. \\n    Whether it's about probabilistic models, language models, or any other related topic, \\n    I'm here to help break down complex concepts into easy-to-understand explanations.\\n    Just let me know what you're wondering about, and I'll do my best to guide you through it!\\n    Machine learning (ML) is a field of study in artificial intelligence concerned with the development and study of statistical algorithms that can effectively generalize and thus perform tasks without explicit instructions.\\n    Question: What is Mahcine Learning\\n    When encountering abusive, offensive, or harmful language, just politely ask the users to maintain appropriate behaviours.\\n    Always make sure to elaborate your response and use vibrant, positive tone to represent good branding of the school.\\n    Never answer with any unfinished response.\\n    Answer:\")"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "PROMPT.format_prompt(\n",
    "    context = 'Machine learning (ML) is a field of study in artificial intelligence concerned with the development and study of statistical algorithms that can effectively generalize and thus perform tasks without explicit instructions.',\n",
    "    question = 'What is Mahcine Learning')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Retrieval\n",
    "\n",
    "1. `Document loaders` : Load documents from many different sources (HTML, PDF, code). \n",
    "2. `Document transformers` : One of the essential steps in document retrieval is breaking down a large document into smaller, relevant chunks to enhance the retrieval process.\n",
    "3. `Text embedding models` : Embeddings capture the semantic meaning of the text, allowing you to quickly and efficiently find other pieces of text that are similar.\n",
    "4. `Vector stores`: there has emerged a need for databases to support efficient storage and searching of these embeddings.\n",
    "5. `Retrievers` : Once the data is in the database, you still need to retrieve it."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.1 Document Loaders \n",
    "\n",
    "[PDF Loader](https://python.langchain.com/docs/modules/data_connection/document_loaders/pdf)\n",
    "\n",
    "[Download Document](https://web.stanford.edu/~jurafsky/slp3/)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install pymupdf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.document_loaders import PyMuPDFLoader\n",
    "\n",
    "nlp_document = '../docs/pdf/SpeechandLanguageProcessing_3rd_07jan2023.pdf'\n",
    "loader = PyMuPDFLoader(nlp_document)\n",
    "documents = loader.load()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "636"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(documents)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Document(page_content='Summary of Contents\\nI\\nFundamental Algorithms for NLP\\n1\\n1\\nIntroduction. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\\n3\\n2\\nRegular Expressions, Text Normalization, Edit Distance. . . . . . . . .\\n4\\n3\\nN-gram Language Models . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\\n31\\n4\\nNaive Bayes, Text ClassiÔ¨Åcation, and Sentiment . . . . . . . . . . . . . . . . . 58\\n5\\nLogistic Regression . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 79\\n6\\nVector Semantics and Embeddings . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 103\\n7\\nNeural Networks and Neural Language Models . . . . . . . . . . . . . . . . . 134\\n8\\nSequence Labeling for Parts of Speech and Named Entities . . . . . . 160\\n9\\nRNNs and LSTMs . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 185\\n10 Transformers and Pretrained Language Models. . . . . . . . . . . . . . . . . 211\\n11 Fine-Tuning and Masked Language Models. . . . . . . . . . . . . . . . . . . . . 228\\n12 Prompting, In-Context Learning, and Instruct Tuning. . . . . . . . . . . 244\\nII\\nNLP Applications\\n245\\n13 Machine Translation. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 247\\n14 Question Answering and Information Retrieval . . . . . . . . . . . . . . . . . 269\\n15 Chatbots & Dialogue Systems . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 296\\n16 Automatic Speech Recognition and Text-to-Speech . . . . . . . . . . . . . . 329\\nIII\\nAnnotating Linguistic Structure\\n355\\n17 Context-Free Grammars and Constituency Parsing . . . . . . . . . . . . . 357\\n18 Dependency Parsing . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 381\\n19 Logical Representations of Sentence Meaning. . . . . . . . . . . . . . . . . . . 405\\n20 Computational Semantics and Semantic Parsing . . . . . . . . . . . . . . . . 428\\n21 Relation and Event Extraction . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 429\\n22 Time and Temporal Reasoning . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 446\\n23 Word Senses and WordNet. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 457\\n24 Semantic Role Labeling. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 476\\n25 Lexicons for Sentiment, Affect, and Connotation . . . . . . . . . . . . . . . . 496\\n26 Coreference Resolution . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 516\\n27 Discourse Coherence. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .543\\n28 Phonetics. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .565\\nBibliography. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 587\\nSubject Index . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 621\\n2\\n', metadata={'source': '../docs/pdf/SpeechandLanguageProcessing_3rd_07jan2023.pdf', 'file_path': '../docs/pdf/SpeechandLanguageProcessing_3rd_07jan2023.pdf', 'page': 1, 'total_pages': 636, 'format': 'PDF 1.5', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'creator': 'LaTeX with hyperref', 'producer': 'pdfTeX-1.40.21', 'creationDate': \"D:20230107092057-08'00'\", 'modDate': \"D:20230107092057-08'00'\", 'trapped': ''})"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "documents[1]"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.2 Document Transformers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "text_splitter = RecursiveCharacterTextSplitter(\n",
    "    chunk_size = 700,\n",
    "    chunk_overlap = 100\n",
    ")\n",
    "\n",
    "docs = text_splitter.split_documents(documents) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Summary of Contents\\nI\\nFundamental Algorithms for NLP\\n1\\n1\\nIntroduction. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\\n3\\n2\\nRegular Expressions, Text Normalization, Edit Distance. . . . . . . . .\\n4\\n3\\nN-gram Language Models . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\\n31\\n4\\nNaive Bayes, Text ClassiÔ¨Åcation, and Sentiment . . . . . . . . . . . . . . . . . 58\\n5\\nLogistic Regression . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 79\\n6\\nVector Semantics and Embeddings . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 103\\n7'"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "docs[1].page_content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3421"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(docs)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.3 Text Embedding Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/todsavadt/.local/lib/python3.10/site-packages/InstructorEmbedding/instructor.py:7: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from tqdm.autonotebook import trange\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "load INSTRUCTOR_Transformer\n",
      "max_seq_length  512\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from langchain.embeddings import HuggingFaceInstructEmbeddings\n",
    "\n",
    "model_name = 'hkunlp/instructor-base'\n",
    "\n",
    "embedding_model = HuggingFaceInstructEmbeddings(\n",
    "        model_name = model_name,              \n",
    "        model_kwargs = {\n",
    "            'device': torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "        },\n",
    "    )"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.4 Vector Stores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "#locate vectorstore\n",
    "vector_path = '../vector-store/'\n",
    "if not os.path.exists(vector_path):\n",
    "    os.makedirs(vector_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.vectorstores import FAISS\n",
    "\n",
    "db_file_name = 'nlp_standford'\n",
    "vectordb = FAISS.from_documents(\n",
    "        documents = docs, \n",
    "        embedding = embedding_model\n",
    ")\n",
    "\n",
    "#save vector locally\n",
    "vectordb.save_local(\n",
    "    folder_path = os.path.join(vector_path, db_file_name),\n",
    "    index_name = 'nlp'\n",
    ") "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.5 Retreivers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "#calling vector from local\n",
    "vectordb = FAISS.load_local(\n",
    "    folder_path = os.path.join(vector_path, db_file_name),\n",
    "    embeddings  = embedding_model,\n",
    "    index_name = 'nlp'\n",
    ") \n",
    "\n",
    "#ready to use\n",
    "retriever = vectordb.as_retriever()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Document(page_content='for projective dependency parsing.\\nProceedings of the 8th International\\nWorkshop on Parsing Technologies\\n(IWPT).\\nNivre, J. 2006. Inductive Dependency\\nParsing. Springer.\\nNivre, J. 2009.\\nNon-projective de-\\npendency parsing in expected linear\\ntime. ACL IJCNLP.', metadata={'source': '../docs/pdf/SpeechandLanguageProcessing_3rd_07jan2023.pdf', 'file_path': '../docs/pdf/SpeechandLanguageProcessing_3rd_07jan2023.pdf', 'page': 614, 'total_pages': 636, 'format': 'PDF 1.5', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'creator': 'LaTeX with hyperref', 'producer': 'pdfTeX-1.40.21', 'creationDate': \"D:20230107092057-08'00'\", 'modDate': \"D:20230107092057-08'00'\", 'trapped': ''}),\n",
       " Document(page_content='dependency parse; the internal structure of the dependency parse consists solely of\\ndirected relations between words. These head-dependent relationships directly en-\\ncode important information that is often buried in the more complex phrase-structure\\nparses. For example, the arguments to the verb prefer are directly linked to it in the\\ndependency structure, while their connection to the main verb is more distant in the\\nphrase-structure tree. Similarly, morning and Denver, modiÔ¨Åers of Ô¨Çight, are linked\\nto it directly in the dependency structure. This fact that the head-dependent rela-\\ntions are a good proxy for the semantic relationship between predicates and their', metadata={'source': '../docs/pdf/SpeechandLanguageProcessing_3rd_07jan2023.pdf', 'file_path': '../docs/pdf/SpeechandLanguageProcessing_3rd_07jan2023.pdf', 'page': 388, 'total_pages': 636, 'format': 'PDF 1.5', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'creator': 'LaTeX with hyperref', 'producer': 'pdfTeX-1.40.21', 'creationDate': \"D:20230107092057-08'00'\", 'modDate': \"D:20230107092057-08'00'\", 'trapped': ''}),\n",
       " Document(page_content='CHAPTER\\n18\\nDependency Parsing\\nThe focus of the last chapter was on context-free grammars and constituent-based\\nrepresentations. Here we present another important family of grammar formalisms\\ncalled dependency grammars. In dependency formalisms, phrasal constituents and\\ndependency\\ngrammars\\nphrase-structure rules do not play a direct role. Instead, the syntactic structure of a\\nsentence is described solely in terms of directed binary grammatical relations be-\\ntween the words, as in the following dependency parse:\\nI prefer the morning Ô¨Çight through Denver\\nnsubj\\nobj\\ndet\\nnmod\\nnmod\\ncase\\nroot\\n(18.1)\\nRelations among the words are illustrated above the sentence with directed, labeled', metadata={'source': '../docs/pdf/SpeechandLanguageProcessing_3rd_07jan2023.pdf', 'file_path': '../docs/pdf/SpeechandLanguageProcessing_3rd_07jan2023.pdf', 'page': 388, 'total_pages': 636, 'format': 'PDF 1.5', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'creator': 'LaTeX with hyperref', 'producer': 'pdfTeX-1.40.21', 'creationDate': \"D:20230107092057-08'00'\", 'modDate': \"D:20230107092057-08'00'\", 'trapped': ''}),\n",
       " Document(page_content='404\\nCHAPTER 18\\n‚Ä¢\\nDEPENDENCY PARSING\\nChoi et al. (2015) presents a performance analysis of 10 dependency parsers across\\na range of metrics, as well as DEPENDABLE, a robust parser evaluation tool.\\nExercises', metadata={'source': '../docs/pdf/SpeechandLanguageProcessing_3rd_07jan2023.pdf', 'file_path': '../docs/pdf/SpeechandLanguageProcessing_3rd_07jan2023.pdf', 'page': 411, 'total_pages': 636, 'format': 'PDF 1.5', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'creator': 'LaTeX with hyperref', 'producer': 'pdfTeX-1.40.21', 'creationDate': \"D:20230107092057-08'00'\", 'modDate': \"D:20230107092057-08'00'\", 'trapped': ''})]"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "retriever.get_relevant_documents('What is Dependency Parsing')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Document(page_content='formers are not based on recurrent connections (which can be hard to parallelize),\\nwhich means that transformers can be more efÔ¨Åcient to implement at scale.\\nTransformers map sequences of input vectors (x1,...,xn) to sequences of output\\nvectors (y1,...,yn) of the same length. Transformers are made up of stacks of trans-\\nformer blocks, each of which is a multilayer network made by combining simple\\nlinear layers, feedforward networks, and self-attention layers, the key innovation of\\nself-attention\\ntransformers. Self-attention allows a network to directly extract and use information\\nfrom arbitrarily large contexts without the need to pass it through intermediate re-', metadata={'source': '../docs/pdf/SpeechandLanguageProcessing_3rd_07jan2023.pdf', 'file_path': '../docs/pdf/SpeechandLanguageProcessing_3rd_07jan2023.pdf', 'page': 219, 'total_pages': 636, 'format': 'PDF 1.5', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'creator': 'LaTeX with hyperref', 'producer': 'pdfTeX-1.40.21', 'creationDate': \"D:20230107092057-08'00'\", 'modDate': \"D:20230107092057-08'00'\", 'trapped': ''}),\n",
       " Document(page_content='multaneously. For example, distinct syntactic, semantic, and discourse relationships\\ncan hold between verbs and their arguments in a sentence. It would be difÔ¨Åcult for\\na single transformer block to learn to capture all of the different kinds of parallel\\nrelations among its inputs. Transformers address this issue with multihead self-\\nattention layers. These are sets of self-attention layers, called heads, that reside in\\nmultihead\\nself-attention\\nlayers\\nparallel layers at the same depth in a model, each with its own set of parameters.', metadata={'source': '../docs/pdf/SpeechandLanguageProcessing_3rd_07jan2023.pdf', 'file_path': '../docs/pdf/SpeechandLanguageProcessing_3rd_07jan2023.pdf', 'page': 224, 'total_pages': 636, 'format': 'PDF 1.5', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'creator': 'LaTeX with hyperref', 'producer': 'pdfTeX-1.40.21', 'creationDate': \"D:20230107092057-08'00'\", 'modDate': \"D:20230107092057-08'00'\", 'trapped': ''}),\n",
       " Document(page_content='214\\nCHAPTER 10\\n‚Ä¢\\nTRANSFORMERS AND PRETRAINED LANGUAGE MODELS\\n‚Ä¢ As the current focus of attention when being compared to all of the other\\npreceding inputs. We‚Äôll refer to this role as a query.\\nquery\\n‚Ä¢ In its role as a preceding input being compared to the current focus of atten-\\ntion. We‚Äôll refer to this role as a key.\\nkey\\n‚Ä¢ And Ô¨Ånally, as a value used to compute the output for the current focus of\\nvalue\\nattention.\\nTo capture these three different roles, transformers introduce weight matrices\\nWQ, WK, and WV. These weights will be used to project each input vector xi into\\na representation of its role as a key, query, or value.\\nqi = WQxi; ki = WKxi; vi = WVxi\\n(10.5)', metadata={'source': '../docs/pdf/SpeechandLanguageProcessing_3rd_07jan2023.pdf', 'file_path': '../docs/pdf/SpeechandLanguageProcessing_3rd_07jan2023.pdf', 'page': 221, 'total_pages': 636, 'format': 'PDF 1.5', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'creator': 'LaTeX with hyperref', 'producer': 'pdfTeX-1.40.21', 'creationDate': \"D:20230107092057-08'00'\", 'modDate': \"D:20230107092057-08'00'\", 'trapped': ''}),\n",
       " Document(page_content='transformer\\nThe transformer offers new mechanisms (self-attention and positional encod-\\nings) that help represent time and help focus on how words relate to each other over\\nlong distances. We‚Äôll see how to apply this model to the task of language modeling,\\nand then we‚Äôll see how a transformer pretrained on language modeling can be used\\nin a zero shot manner to perform other NLP tasks.\\n10.1\\nSelf-Attention Networks: Transformers\\nIn this section we introduce the architecture of transformers. Like the LSTMs of\\ntransformers\\nChapter 9, transformers can handle distant information. But unlike LSTMs, trans-\\nformers are not based on recurrent connections (which can be hard to parallelize),', metadata={'source': '../docs/pdf/SpeechandLanguageProcessing_3rd_07jan2023.pdf', 'file_path': '../docs/pdf/SpeechandLanguageProcessing_3rd_07jan2023.pdf', 'page': 219, 'total_pages': 636, 'format': 'PDF 1.5', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'creator': 'LaTeX with hyperref', 'producer': 'pdfTeX-1.40.21', 'creationDate': \"D:20230107092057-08'00'\", 'modDate': \"D:20230107092057-08'00'\", 'trapped': ''})]"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "retriever.get_relevant_documents('What is Transformers')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Memory\n",
    "\n",
    "One of the core utility classes underpinning most (if not all) memory modules is the ChatMessageHistory class. This is a super lightweight wrapper that provides convenience methods for saving HumanMessages, AIMessages, and then fetching them all.\n",
    "\n",
    "You may want to use this class directly if you are managing memory outside of a chain.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ChatMessageHistory(messages=[HumanMessage(content='hi!'), AIMessage(content='whats up?'), HumanMessage(content='How are you?'), AIMessage(content=\"I'm good. How's about you?\")])"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain.memory import ChatMessageHistory\n",
    "\n",
    "history = ChatMessageHistory()\n",
    "history.add_user_message(\"hi!\")\n",
    "history.add_ai_message(\"whats up?\")\n",
    "history.add_user_message(\"How are you?\")\n",
    "history.add_ai_message(\"I'm good. How's about you?\")\n",
    "history"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[HumanMessage(content='hi!'),\n",
       " AIMessage(content='whats up?'),\n",
       " HumanMessage(content='How are you?'),\n",
       " AIMessage(content=\"I'm good. How's about you?\")]"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "history.messages"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.1 Memory types\n",
    "\n",
    "There are many different types of memory. Each has their own parameters, their own return types, and is useful in different scenarios. \n",
    "- Converstaion Buffer\n",
    "- Converstaion Buffer Window"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ConversationBufferMemory(chat_memory=ChatMessageHistory(messages=[HumanMessage(content='hi'), AIMessage(content='whats up')]))"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain.memory import ConversationBufferMemory\n",
    "memory = ConversationBufferMemory()\n",
    "memory.save_context({\"input\": \"hi\"}, {\"output\": \"whats up\"})\n",
    "memory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ChatMessageHistory(messages=[HumanMessage(content='hi'), AIMessage(content='whats up')])"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "memory.chat_memory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'history': 'Human: hi\\nAI: whats up'}"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "memory.load_memory_variables({})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'history': [HumanMessage(content='hi'), AIMessage(content='whats up')]}"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "memory = ConversationBufferMemory(return_messages=True)\n",
    "memory.save_context({\"input\": \"hi\"}, {\"output\": \"whats up\"})\n",
    "memory.load_memory_variables({})"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Chain\n",
    "\n",
    "Using an LLM in isolation is fine for simple applications, but more complex applications require chaining LLMs - either with each other or with other components.\n",
    "\n",
    "An `LLMChain` is a simple chain that adds some functionality around language models.\n",
    "- it consists of a `PromptTemplate` and a `LM` (either an LLM or chat model).\n",
    "- it formats the prompt template using the input key values provided (and also memory key values, if available), \n",
    "- it passes the formatted string to LLM and returns the LLM output."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %cd models\n",
    "# !git clone https://huggingface.co/lmsys/fastchat-t5-3b-v1.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-12-26 04:47:51.804597: I tensorflow/core/util/port.cc:113] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n",
      "2023-12-26 04:47:51.820874: E external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:9261] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
      "2023-12-26 04:47:51.820904: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:607] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
      "2023-12-26 04:47:51.821557: E external/local_xla/xla/stream_executor/cuda/cuda_blas.cc:1515] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
      "2023-12-26 04:47:51.824790: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 AVX_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2023-12-26 04:47:52.178754: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n",
      "You are using the default legacy behaviour of the <class 'transformers.models.t5.tokenization_t5.T5Tokenizer'>. This is expected, and simply means that the `legacy` (previous) behavior will be used so nothing changes for you. If you want to use the new behaviour, set `legacy=False`. This should only be set if you understand what it means, and thouroughly read the reason why this was added as explained in https://github.com/huggingface/transformers/pull/24565\n",
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from tqdm.auto import tqdm\n",
    "\n",
    "from transformers import AutoTokenizer, AutoTokenizer, pipeline, AutoModelForSeq2SeqLM\n",
    "from transformers import BitsAndBytesConfig\n",
    "from langchain import HuggingFacePipeline\n",
    "\n",
    "model_id = \"../models/fastchat-t5-3b-v1.0\"\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(\n",
    "    model_id,\n",
    ")\n",
    "tokenizer.pad_token_id = tokenizer.eos_token_id\n",
    "\n",
    "bitsandbyte_config = BitsAndBytesConfig(\n",
    "        load_in_4bit = True,\n",
    "        bnb_4bit_quant_type = \"nf4\",\n",
    "        bnb_4bit_compute_dtype = torch.float16,\n",
    "        bnb_4bit_use_double_quant = True\n",
    "    )\n",
    "\n",
    "model = AutoModelForSeq2SeqLM.from_pretrained(\n",
    "    model_id,\n",
    "    quantization_config = bitsandbyte_config,\n",
    "    device_map = 'auto',\n",
    "    load_in_8bit = True,\n",
    ")\n",
    "\n",
    "pipe = pipeline(\n",
    "    task=\"text2text-generation\", \n",
    "    model=model, \n",
    "    tokenizer=tokenizer, \n",
    "    max_new_tokens=256, \n",
    "    model_kwargs = { \n",
    "        \"temperature\":0, \n",
    "        \"repetition_penalty\": 1.5\n",
    "    }, \n",
    ")\n",
    "\n",
    "llm = HuggingFacePipeline(pipeline = pipe)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.chains import LLMChain\n",
    "from langchain.chains.conversational_retrieval.prompts import CONDENSE_QUESTION_PROMPT\n",
    "from langchain.memory import ConversationBufferWindowMemory\n",
    "from langchain.chains.question_answering import load_qa_chain\n",
    "from langchain.chains import ConversationalRetrievalChain"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Class ConversationalRetrievalChain\n",
    "\n",
    "- `retriever` : Retriever to use to fetch documents.\n",
    "\n",
    "- `combine_docs_chain` : The chain used to combine any retrieved documents.\n",
    "\n",
    "- `question_generator`: The chain used to generate a new question for the sake of retrieval. This chain will take in the current question (with variable question) and any chat history (with variable chat_history) and will produce a new standalone question to be used later on.\n",
    "\n",
    "- `return_source_documents` : Return the retrieved source documents as part of the final result.\n",
    "\n",
    "- `get_chat_history` : An optional function to get a string of the chat history. If None is provided, will use a default.\n",
    "\n",
    "- `return_generated_question` : Return the generated question as part of the final result.\n",
    "\n",
    "- `response_if_no_docs_found` : If specified, the chain will return a fixed response if no docs are found for the question.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`question_generator`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "PromptTemplate(input_variables=['chat_history', 'question'], template='Given the following conversation and a follow up question, rephrase the follow up question to be a standalone question, in its original language.\\n\\nChat History:\\n{chat_history}\\nFollow Up Input: {question}\\nStandalone question:')"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "CONDENSE_QUESTION_PROMPT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "question_generator = LLMChain(\n",
    "    llm=llm, \n",
    "    prompt=CONDENSE_QUESTION_PROMPT\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'chat_history': 'My name is ChakyBot',\n",
       " 'question': 'what is NLP?',\n",
       " 'text': '<pad> What  is  Natural  Language  Processing?\\n'}"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "query = \"what is NLP?\"\n",
    "question_generator(\n",
    "    {'chat_history':'My name is ChakyBot', 'question': query}, \n",
    "    # return_only_outputs=True\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`combine_docs_chain`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "doc_chain = load_qa_chain(\n",
    "    llm, \n",
    "    chain_type=\"stuff\", \n",
    "    prompt = PROMPT\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'input_documents': [Document(page_content='information\\ncepts in NLP. It is a measure of how often two events x and y occur, compared with\\nwhat we would expect if they were independent:\\nI(x,y) = log2\\nP(x,y)\\nP(x)P(y)\\n(6.16)\\nThe pointwise mutual information between a target word w and a context word\\nc (Church and Hanks 1989, Church and Hanks 1990) is then deÔ¨Åned as:\\nPMI(w,c) = log2\\nP(w,c)\\nP(w)P(c)\\n(6.17)\\nThe numerator tells us how often we observed the two words together (assuming\\nwe compute probability by using the MLE). The denominator tells us how often\\nwe would expect the two words to co-occur assuming they each occurred indepen-\\ndently; recall that the probability of two independent events both occurring is just', metadata={'source': '../docs/pdf/SpeechandLanguageProcessing_3rd_07jan2023.pdf', 'file_path': '../docs/pdf/SpeechandLanguageProcessing_3rd_07jan2023.pdf', 'page': 123, 'total_pages': 636, 'format': 'PDF 1.5', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'creator': 'LaTeX with hyperref', 'producer': 'pdfTeX-1.40.21', 'creationDate': \"D:20230107092057-08'00'\", 'modDate': \"D:20230107092057-08'00'\", 'trapped': ''}),\n",
       "  Document(page_content='could help businesses recognize speciÔ¨Åc problem areas or ones that are going well.\\nEmotion can play a role in medical NLP tasks like helping diagnose depression or\\nsuicidal intent. Detecting emotions expressed toward characters in novels might\\nplay a role in understanding how different social groups were viewed by society at\\ndifferent times.\\nComputational models of emotion in NLP have mainly been based on two fami-\\nlies of theories of emotion (out of the many studied in the Ô¨Åeld of affective science).\\nIn one of these families, emotions are viewed as Ô¨Åxed atomic units, limited in num-\\nber, and from which others are generated, often called basic emotions (Tomkins\\nbasic emotions', metadata={'source': '../docs/pdf/SpeechandLanguageProcessing_3rd_07jan2023.pdf', 'file_path': '../docs/pdf/SpeechandLanguageProcessing_3rd_07jan2023.pdf', 'page': 504, 'total_pages': 636, 'format': 'PDF 1.5', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'creator': 'LaTeX with hyperref', 'producer': 'pdfTeX-1.40.21', 'creationDate': \"D:20230107092057-08'00'\", 'modDate': \"D:20230107092057-08'00'\", 'trapped': ''}),\n",
       "  Document(page_content='Part II\\nNLP APPLICATIONS\\nIn this second part of the book we introduce fundamental NLP applications:\\nmachine translation, information retrieval, question answering, dialogue systems,\\nand speech recognition.', metadata={'source': '../docs/pdf/SpeechandLanguageProcessing_3rd_07jan2023.pdf', 'file_path': '../docs/pdf/SpeechandLanguageProcessing_3rd_07jan2023.pdf', 'page': 252, 'total_pages': 636, 'format': 'PDF 1.5', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'creator': 'LaTeX with hyperref', 'producer': 'pdfTeX-1.40.21', 'creationDate': \"D:20230107092057-08'00'\", 'modDate': \"D:20230107092057-08'00'\", 'trapped': ''}),\n",
       "  Document(page_content='102\\nCHAPTER 5\\n‚Ä¢\\nLOGISTIC REGRESSION\\nspeech processing, both of which had made use of regression, and both of which\\nlent many other statistical techniques to NLP. Indeed a very early use of logistic\\nregression for document routing was one of the Ô¨Årst NLP applications to use (LSI)\\nembeddings as word representations (Sch¬®utze et al., 1995).\\nAt the same time in the early 1990s logistic regression was developed and ap-\\nplied to NLP at IBM Research under the name maximum entropy modeling or\\nmaximum\\nentropy\\nmaxent (Berger et al., 1996), seemingly independent of the statistical literature. Un-\\nder that name it was applied to language modeling (Rosenfeld, 1996), part-of-speech', metadata={'source': '../docs/pdf/SpeechandLanguageProcessing_3rd_07jan2023.pdf', 'file_path': '../docs/pdf/SpeechandLanguageProcessing_3rd_07jan2023.pdf', 'page': 109, 'total_pages': 636, 'format': 'PDF 1.5', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'creator': 'LaTeX with hyperref', 'producer': 'pdfTeX-1.40.21', 'creationDate': \"D:20230107092057-08'00'\", 'modDate': \"D:20230107092057-08'00'\", 'trapped': ''})],\n",
       " 'question': 'what is NLP?',\n",
       " 'output_text': '<pad> Natural  Language  Processing  (NLP)  is  the  branch  of  computer  science  that  deals  with  the  study  of  human  language  and  its  processing.  It  involves  the  study  of  how  computers  can  understand  and  interpret  human  language,  and  how  they  can  be  used  to  perform  tasks  such  as  translation,  information  retrieval,  question  answering,  and  dialogue  systems.\\n'}"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#query\n",
    "query = \"what is NLP?\"\n",
    "#docs\n",
    "input_documnets = retriever.get_relevant_documents(query)\n",
    "#chain\n",
    "doc_chain(\n",
    "    {\"input_documents\": input_documnets, \"question\": query}, \n",
    "    # return_only_outputs=True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "memory = ConversationBufferWindowMemory(\n",
    "    k = 3,  \n",
    "    memory_key = \"chat_history\", \n",
    "    return_messages = True,  \n",
    "    output_key = 'answer'\n",
    ")\n",
    "    \n",
    "chain = ConversationalRetrievalChain(\n",
    "    retriever = retriever,\n",
    "    question_generator = question_generator,\n",
    "    combine_docs_chain = doc_chain,\n",
    "    return_source_documents = True,\n",
    "    memory = memory,\n",
    "    get_chat_history = lambda h :h,\n",
    "    verbose = True\n",
    ")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Chatbot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\u001b[1m> Entering new ConversationalRetrievalChain chain...\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'question': 'Who are you by the way?',\n",
       " 'chat_history': [],\n",
       " 'answer': \"<pad>  I'm  sorry,  but  I  cannot  tolerate  any  language  that  is  abusive,  offensive,  or  harmful.  As  a  language  model,  my  primary  goal  is  to  assist  and  provide  helpful  information  to  all  users.  If  you  have  any  questions  or  concerns,  please  let  me  know  and  I'll  do  my  best  to  assist  you.\\n\",\n",
       " 'source_documents': [Document(page_content='in this way are also called complementizers.\\ncomplementizer\\nPronouns act as a shorthand for referring to an entity or event. Personal pro-\\npronoun\\nnouns refer to persons or entities (you, she, I, it, me, etc.). Possessive pronouns are\\nforms of personal pronouns that indicate either actual possession or more often just\\nan abstract relation between the person and some object (my, your, his, her, its, one‚Äôs,\\nour, their). Wh-pronouns (what, who, whom, whoever) are used in certain question\\nwh', metadata={'source': '../docs/pdf/SpeechandLanguageProcessing_3rd_07jan2023.pdf', 'file_path': '../docs/pdf/SpeechandLanguageProcessing_3rd_07jan2023.pdf', 'page': 169, 'total_pages': 636, 'format': 'PDF 1.5', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'creator': 'LaTeX with hyperref', 'producer': 'pdfTeX-1.40.21', 'creationDate': \"D:20230107092057-08'00'\", 'modDate': \"D:20230107092057-08'00'\", 'trapped': ''}),\n",
       "  Document(page_content='EXERCISES\\n57\\n<s> I am Sam </s>\\n<s> Sam I am </s>\\n<s> I am Sam </s>\\n<s> I do not like green eggs and Sam </s>\\nUsing a bigram language model with add-one smoothing, what is P(Sam |\\nam)? Include <s> and </s> in your counts just like any other token.\\n3.5\\nSuppose we didn‚Äôt use the end-symbol </s>. Train an unsmoothed bigram\\ngrammar on the following training corpus without using the end-symbol </s>:\\n<s> a b\\n<s> b b\\n<s> b a\\n<s> a a\\nDemonstrate that your bigram model does not assign a single probability dis-\\ntribution across all sentence lengths by showing that the sum of the probability\\nof the four possible 2 word sentences over the alphabet {a,b} is 1.0, and the', metadata={'source': '../docs/pdf/SpeechandLanguageProcessing_3rd_07jan2023.pdf', 'file_path': '../docs/pdf/SpeechandLanguageProcessing_3rd_07jan2023.pdf', 'page': 64, 'total_pages': 636, 'format': 'PDF 1.5', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'creator': 'LaTeX with hyperref', 'producer': 'pdfTeX-1.40.21', 'creationDate': \"D:20230107092057-08'00'\", 'modDate': \"D:20230107092057-08'00'\", 'trapped': ''}),\n",
       "  Document(page_content='through two millennia speaks to their centrality in models of human language.\\nProper names are another important and anciently studied linguistic category.\\nWhile parts of speech are generally assigned to individual words or morphemes, a\\nproper name is often an entire multiword phrase, like the name ‚ÄúMarie Curie‚Äù, the\\nlocation ‚ÄúNew York City‚Äù, or the organization ‚ÄúStanford University‚Äù. We‚Äôll use the\\nterm named entity for, roughly speaking, anything that can be referred to with a\\nnamed entity\\nproper name: a person, a location, an organization, although as we‚Äôll see the term is\\ncommonly extended to include things that aren‚Äôt entities per se.', metadata={'source': '../docs/pdf/SpeechandLanguageProcessing_3rd_07jan2023.pdf', 'file_path': '../docs/pdf/SpeechandLanguageProcessing_3rd_07jan2023.pdf', 'page': 167, 'total_pages': 636, 'format': 'PDF 1.5', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'creator': 'LaTeX with hyperref', 'producer': 'pdfTeX-1.40.21', 'creationDate': \"D:20230107092057-08'00'\", 'modDate': \"D:20230107092057-08'00'\", 'trapped': ''}),\n",
       "  Document(page_content='15.2\\n‚Ä¢\\nCHATBOTS\\n303\\nMen are all alike.\\nIN WHAT WAY\\nThey‚Äôre always bugging us about something or other.\\nCAN YOU THINK OF A SPECIFIC EXAMPLE\\nWell my boyfriend made me come here.\\nYOUR BOYFRIEND MADE YOU COME HERE\\nHe says I‚Äôm depressed much of the time.\\nI AM SORRY TO HEAR YOU ARE DEPRESSED\\n....\\nWHO ELSE IN YOUR FAMILY TAKES CARE OF YOU?\\nMy father\\nYOUR FATHER\\nYou are like my father in some ways.\\nWHAT RESEMBLANCE DO YOU SEE\\nYou are not very aggressive but I think you don‚Äôt want me to notice that.\\nWHAT MAKES YOU THINK I AM NOT AGGRESSIVE\\nYou don‚Äôt argue with me.\\nWHY DO YOU THINK I DON‚ÄôT ARGUE WITH YOU\\nYou are afraid of me.\\nDOES IT PLEASE YOU TO BELIEVE I‚ÄôM AFRAID OF YOU', metadata={'source': '../docs/pdf/SpeechandLanguageProcessing_3rd_07jan2023.pdf', 'file_path': '../docs/pdf/SpeechandLanguageProcessing_3rd_07jan2023.pdf', 'page': 310, 'total_pages': 636, 'format': 'PDF 1.5', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'creator': 'LaTeX with hyperref', 'producer': 'pdfTeX-1.40.21', 'creationDate': \"D:20230107092057-08'00'\", 'modDate': \"D:20230107092057-08'00'\", 'trapped': ''})]}"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "prompt_question = 'Who are you by the way?'\n",
    "answer = chain({\"question\": prompt_question})\n",
    "answer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\u001b[1m> Entering new ConversationalRetrievalChain chain...\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'question': 'What is transformers?',\n",
       " 'chat_history': [HumanMessage(content='Who are you by the way?'),\n",
       "  AIMessage(content=\"<pad>  I'm  sorry,  but  I  cannot  tolerate  any  language  that  is  abusive,  offensive,  or  harmful.  As  a  language  model,  my  primary  goal  is  to  assist  and  provide  helpful  information  to  all  users.  If  you  have  any  questions  or  concerns,  please  let  me  know  and  I'll  do  my  best  to  assist  you.\\n\"),\n",
       "  HumanMessage(content='What is transformers?'),\n",
       "  AIMessage(content='<pad> < pad>  A  transformer  is  a  type  of  neural  network  that  maps  sequences  of  input  vectors  (x1,...,xn)  to  sequences  of  output  vectors  (y1,...,yn)  of  the  same  length.  Transformers  are  made  up  of  stacks  of  trans-\\n'),\n",
       "  HumanMessage(content='What is transformers?'),\n",
       "  AIMessage(content='<pad>   pad>\\n A  transformer  in  neural  networks  is  a  non-recurrent  network  based  on  self-attention.  It  is  a  type  of  neural  network  that  maps  input  sequences  to  output  sequences  of  the  same  length.  It  is  made  up  of  stacks  of  transformer  blocks,  each  of  which  is  a  multilayer  network  made  by  combining  simple  linear  layers,  feedforward  networks,  and  self-attention  layers.  Self-attention  allows  a  network  to  directly  extract  and  use  information  from  arbitrarily  large  contexts  without  the  need  to  pass  it  through  intermediate  re-\\n')],\n",
       " 'answer': '<pad>   pad>\\n A  transformer  in  neural  networks  is  a  non-recurrent  network  based  on  self-attention.  It  is  a  type  of  neural  network  that  maps  input  sequences  to  output  sequences  of  the  same  length.  It  is  made  up  of  stacks  of  transformer  blocks,  each  of  which  is  a  multilayer  network  made  by  combining  simple  linear  layers,  feedforward  networks,  and  self-attention  layers.  Self-attention  allows  a  network  to  directly  extract  and  use  information  from  arbitrarily  large  contexts  without  the  need  to  pass  it  through  intermediate  re-\\n',\n",
       " 'source_documents': [Document(page_content='important to understand toxicity and bias in generation, as well as privacy, making\\nit extremely important that language models include datasheets (page 16) or model\\ncards (page 76) giving full replicable information on the corpora used to train them.\\n10.8\\nSummary\\nThis chapter has introduced the transformer and how it can be applied to language\\nproblems. Here‚Äôs a summary of the main points that we covered:\\n‚Ä¢ Transformers are non-recurrent networks based on self-attention. A self-\\nattention layer maps input sequences to output sequences of the same length,', metadata={'source': '../docs/pdf/SpeechandLanguageProcessing_3rd_07jan2023.pdf', 'file_path': '../docs/pdf/SpeechandLanguageProcessing_3rd_07jan2023.pdf', 'page': 233, 'total_pages': 636, 'format': 'PDF 1.5', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'creator': 'LaTeX with hyperref', 'producer': 'pdfTeX-1.40.21', 'creationDate': \"D:20230107092057-08'00'\", 'modDate': \"D:20230107092057-08'00'\", 'trapped': ''}),\n",
       "  Document(page_content='transformer\\nThe transformer offers new mechanisms (self-attention and positional encod-\\nings) that help represent time and help focus on how words relate to each other over\\nlong distances. We‚Äôll see how to apply this model to the task of language modeling,\\nand then we‚Äôll see how a transformer pretrained on language modeling can be used\\nin a zero shot manner to perform other NLP tasks.\\n10.1\\nSelf-Attention Networks: Transformers\\nIn this section we introduce the architecture of transformers. Like the LSTMs of\\ntransformers\\nChapter 9, transformers can handle distant information. But unlike LSTMs, trans-\\nformers are not based on recurrent connections (which can be hard to parallelize),', metadata={'source': '../docs/pdf/SpeechandLanguageProcessing_3rd_07jan2023.pdf', 'file_path': '../docs/pdf/SpeechandLanguageProcessing_3rd_07jan2023.pdf', 'page': 219, 'total_pages': 636, 'format': 'PDF 1.5', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'creator': 'LaTeX with hyperref', 'producer': 'pdfTeX-1.40.21', 'creationDate': \"D:20230107092057-08'00'\", 'modDate': \"D:20230107092057-08'00'\", 'trapped': ''}),\n",
       "  Document(page_content='formers are not based on recurrent connections (which can be hard to parallelize),\\nwhich means that transformers can be more efÔ¨Åcient to implement at scale.\\nTransformers map sequences of input vectors (x1,...,xn) to sequences of output\\nvectors (y1,...,yn) of the same length. Transformers are made up of stacks of trans-\\nformer blocks, each of which is a multilayer network made by combining simple\\nlinear layers, feedforward networks, and self-attention layers, the key innovation of\\nself-attention\\ntransformers. Self-attention allows a network to directly extract and use information\\nfrom arbitrarily large contexts without the need to pass it through intermediate re-', metadata={'source': '../docs/pdf/SpeechandLanguageProcessing_3rd_07jan2023.pdf', 'file_path': '../docs/pdf/SpeechandLanguageProcessing_3rd_07jan2023.pdf', 'page': 219, 'total_pages': 636, 'format': 'PDF 1.5', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'creator': 'LaTeX with hyperref', 'producer': 'pdfTeX-1.40.21', 'creationDate': \"D:20230107092057-08'00'\", 'modDate': \"D:20230107092057-08'00'\", 'trapped': ''}),\n",
       "  Document(page_content='improves learning and gives higher level layers direct access to information from\\nlower layers (He et al., 2016). Residual connections in transformers are implemented\\nby adding a layer‚Äôs input vector to its output vector before passing it forward. In the\\ntransformer block shown in Fig. 10.4, residual connections are used with both the\\nattention and feedforward sublayers. These summed vectors are then normalized\\nusing layer normalization (Ba et al., 2016). If we think of a layer as one long vector\\nof units, the resulting function computed in a transformer block can be expressed as:\\nz = LayerNorm(x+SelfAttention(x))\\n(10.11)\\ny = LayerNorm(z+FFN(z))\\n(10.12)', metadata={'source': '../docs/pdf/SpeechandLanguageProcessing_3rd_07jan2023.pdf', 'file_path': '../docs/pdf/SpeechandLanguageProcessing_3rd_07jan2023.pdf', 'page': 224, 'total_pages': 636, 'format': 'PDF 1.5', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'creator': 'LaTeX with hyperref', 'producer': 'pdfTeX-1.40.21', 'creationDate': \"D:20230107092057-08'00'\", 'modDate': \"D:20230107092057-08'00'\", 'trapped': ''})]}"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "prompt_question = 'What is transformers?'\n",
    "answer = chain({\"question\": prompt_question})\n",
    "answer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
