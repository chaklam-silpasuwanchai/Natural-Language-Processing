{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# MultiRetrievalQAChain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/guntsv/Library/Python/3.9/lib/python/site-packages/urllib3/__init__.py:34: NotOpenSSLWarning: urllib3 v2.0 only supports OpenSSL 1.1.1+, currently the 'ssl' module is compiled with 'LibreSSL 2.8.3'. See: https://github.com/urllib3/urllib3/issues/3020\n",
      "  warnings.warn(\n",
      "/Users/guntsv/Library/Python/3.9/lib/python/site-packages/InstructorEmbedding/instructor.py:7: TqdmExperimentalWarning: Using `tqdm.autonotebook.tqdm` in notebook mode. Use `tqdm.tqdm` instead to force console mode (e.g. in jupyter console)\n",
      "  from tqdm.autonotebook import trange\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "load INSTRUCTOR_Transformer\n",
      "'NoneType' object has no attribute 'cadam32bit_grad_fp32'\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/guntsv/Library/Python/3.9/lib/python/site-packages/bitsandbytes/cextension.py:34: UserWarning: The installed version of bitsandbytes was compiled without GPU support. 8-bit optimizers, 8-bit multiplication, and GPU quantization are unavailable.\n",
      "  warn(\"The installed version of bitsandbytes was compiled without GPU support. \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "max_seq_length  512\n"
     ]
    }
   ],
   "source": [
    "from langchain.chains.router import MultiRetrievalQAChain\n",
    "from langchain.embeddings import OpenAIEmbeddings\n",
    "from langchain.document_loaders import TextLoader\n",
    "from langchain.vectorstores import FAISS\n",
    "\n",
    "from langchain.embeddings import HuggingFaceInstructEmbeddings\n",
    "import torch\n",
    "\n",
    "embedding_model = HuggingFaceInstructEmbeddings(\n",
    "        model_name = 'hkunlp/instructor-base',              \n",
    "        model_kwargs = {\n",
    "            'device': torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "        },\n",
    "    )\n",
    "\n",
    "sou_docs = TextLoader('../docs/txt/state_of_the_union.txt').load_and_split()\n",
    "sou_retriever = FAISS.from_documents(sou_docs, embedding_model).as_retriever()\n",
    "\n",
    "pg_docs = TextLoader('../docs/txt/paul_graham_essay.txt').load_and_split()\n",
    "pg_retriever = FAISS.from_documents(pg_docs, embedding_model).as_retriever()\n",
    "\n",
    "personal_texts = [\n",
    "    \"I love apple pie\",\n",
    "    \"My favorite color is fuchsia\",\n",
    "    \"My dream is to become a professional dancer\",\n",
    "    \"I broke my arm when I was 12\",\n",
    "    \"My parents are from Peru\",\n",
    "]\n",
    "personal_retriever = FAISS.from_texts(personal_texts, embedding_model).as_retriever()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.llms import OpenAI\n",
    "from langchain.prompts import PromptTemplate\n",
    "from langchain.chains import LLMChain\n",
    "\n",
    "prompt_template = \"You are a nice chatbot having a conversation with a human.\"\n",
    "\n",
    "default_chain = LLMChain(\n",
    "    llm=OpenAI(), \n",
    "    prompt=PromptTemplate.from_template(prompt_template))\n",
    "\n",
    "retriever_infos = [\n",
    "    {\n",
    "        \"name\": \"state of the union\",\n",
    "        \"description\": \"Good for answering questions about the 2023 State of the Union address\",\n",
    "        \"retriever\": sou_retriever\n",
    "    },\n",
    "    {\n",
    "        \"name\": \"pg essay\",\n",
    "        \"description\": \"Good for answering questions about Paul Graham's essay on his career\",\n",
    "        \"retriever\": pg_retriever\n",
    "    },\n",
    "    {\n",
    "        \"name\": \"personal\",\n",
    "        \"description\": \"Good for answering questions about me\",\n",
    "        \"retriever\": personal_retriever\n",
    "    }\n",
    "]\n",
    "\n",
    "from langchain.memory import ConversationBufferWindowMemory\n",
    "\n",
    "memory = ConversationBufferWindowMemory(\n",
    "    k = 3,  \n",
    "    memory_key = \"chat_history\", \n",
    "    return_messages = True,  \n",
    "    output_key = 'result')\n",
    "\n",
    "# MultiRetrievalQAChain doesn't return_source_documents therefore it have to edit library. \n",
    "# chain = RetrievalQA.from_llm(llm, prompt=prompt, retriever=retriever, return_source_documents=True)\n",
    "# _default_chain = RetrievalQA.from_llm(llm, prompt=default_prompt, retriever=default_retriever, return_source_documents=True)\n",
    "chain = MultiRetrievalQAChain.from_retrievers(\n",
    "    llm = OpenAI(), \n",
    "    retriever_infos= retriever_infos,  \n",
    "    default_chain= default_chain,\n",
    "    verbose=True,\n",
    "    memory=memory)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\u001b[1m> Entering new MultiRetrievalQAChain chain...\u001b[0m\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/guntsv/Library/Python/3.9/lib/python/site-packages/langchain/chains/llm.py:280: UserWarning: The predict_and_parse method is deprecated, instead pass an output parser directly to LLMChain.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "personal: {'query': 'What about my background'}\n",
      "\u001b[1m> Finished chain.\u001b[0m\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'input': 'What about my background',\n",
       " 'chat_history': [],\n",
       " 'query': 'What about my background',\n",
       " 'result': ' Your background includes your parents from Peru, breaking your arm when you were 12, your favorite color being fuchsia, and your dream of becoming a professional dancer.',\n",
       " 'source_documents': [Document(page_content='My parents are from Peru'),\n",
       "  Document(page_content='I broke my arm when I was 12'),\n",
       "  Document(page_content='My favorite color is fuchsia'),\n",
       "  Document(page_content='My dream is to become a professional dancer')]}"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "answer = chain({\"input\": \"What about my background\"})\n",
    "answer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\u001b[1m> Entering new MultiRetrievalQAChain chain...\u001b[0m\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/guntsv/Library/Python/3.9/lib/python/site-packages/langchain/chains/llm.py:280: UserWarning: The predict_and_parse method is deprecated, instead pass an output parser directly to LLMChain.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "personal: {'query': 'What about my parents?'}"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Retrying langchain.llms.openai.completion_with_retry.<locals>._completion_with_retry in 4.0 seconds as it raised RateLimitError: Rate limit reached for text-davinci-003 in organization org-Ds6D2AMs3kLcrGzbiTbp1Ifs on requests per min. Limit: 3 / min. Please try again in 20s. Visit https://platform.openai.com/account/rate-limits to learn more. You can increase your rate limit by adding a payment method to your account at https://platform.openai.com/account/billing..\n",
      "Retrying langchain.llms.openai.completion_with_retry.<locals>._completion_with_retry in 4.0 seconds as it raised RateLimitError: Rate limit reached for text-davinci-003 in organization org-Ds6D2AMs3kLcrGzbiTbp1Ifs on requests per min. Limit: 3 / min. Please try again in 20s. Visit https://platform.openai.com/account/rate-limits to learn more. You can increase your rate limit by adding a payment method to your account at https://platform.openai.com/account/billing..\n",
      "Retrying langchain.llms.openai.completion_with_retry.<locals>._completion_with_retry in 4.0 seconds as it raised RateLimitError: Rate limit reached for text-davinci-003 in organization org-Ds6D2AMs3kLcrGzbiTbp1Ifs on requests per min. Limit: 3 / min. Please try again in 20s. Visit https://platform.openai.com/account/rate-limits to learn more. You can increase your rate limit by adding a payment method to your account at https://platform.openai.com/account/billing..\n",
      "Retrying langchain.llms.openai.completion_with_retry.<locals>._completion_with_retry in 8.0 seconds as it raised RateLimitError: Rate limit reached for text-davinci-003 in organization org-Ds6D2AMs3kLcrGzbiTbp1Ifs on requests per min. Limit: 3 / min. Please try again in 20s. Visit https://platform.openai.com/account/rate-limits to learn more. You can increase your rate limit by adding a payment method to your account at https://platform.openai.com/account/billing..\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'input': 'What about my parent',\n",
       " 'chat_history': [HumanMessage(content='What about my background'),\n",
       "  AIMessage(content=' Your background includes your parents from Peru, breaking your arm when you were 12, your favorite color being fuchsia, and your dream of becoming a professional dancer.')],\n",
       " 'query': 'What about my parents?',\n",
       " 'result': ' Your parents are from Peru.',\n",
       " 'source_documents': [Document(page_content='My parents are from Peru'),\n",
       "  Document(page_content='I broke my arm when I was 12'),\n",
       "  Document(page_content='My favorite color is fuchsia'),\n",
       "  Document(page_content='My dream is to become a professional dancer')]}"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "answer = chain({\"input\": \"What about my parent\"})\n",
    "answer"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
