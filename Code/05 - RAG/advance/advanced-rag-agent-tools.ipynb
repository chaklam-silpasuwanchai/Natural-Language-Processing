{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Natural Language Processing\n",
    "\n",
    "# Agents\n",
    "- The core idea of agents is to use a language model to choose `a sequence of actions` to take. \n",
    "- In chains, a sequence of actions is hardcoded (in code). \n",
    "- In agents, a language model is used as a reasoning engine to determine `which actions to take and in which order.`\n",
    "\n",
    "There are several key components here: Agent, Tools, AgentExecutor, etc."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'0.0.350'"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import langchain\n",
    "langchain.__version__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "device(type='cuda')"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os\n",
    "import torch\n",
    "# Set GPU device\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"1\"\n",
    "\n",
    "os.environ['http_proxy']  = 'http://192.41.170.23:3128'\n",
    "os.environ['https_proxy'] = 'http://192.41.170.23:3128'\n",
    "\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from tqdm.auto import tqdm\n",
    "\n",
    "from transformers import AutoTokenizer, AutoTokenizer, pipeline, AutoModelForSeq2SeqLM\n",
    "from transformers import BitsAndBytesConfig\n",
    "from langchain import HuggingFacePipeline\n",
    "\n",
    "model_id = \"models/fastchat-t5-3b-v1.0\"\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(\n",
    "    model_id,\n",
    ")\n",
    "tokenizer.pad_token_id = tokenizer.eos_token_id\n",
    "\n",
    "bitsandbyte_config = BitsAndBytesConfig(\n",
    "        load_in_4bit = True,\n",
    "        bnb_4bit_quant_type = \"nf4\",\n",
    "        bnb_4bit_compute_dtype = torch.float16,\n",
    "        bnb_4bit_use_double_quant = True\n",
    "    )\n",
    "\n",
    "model = AutoModelForSeq2SeqLM.from_pretrained(\n",
    "    model_id,\n",
    "    quantization_config = bitsandbyte_config,\n",
    "    device_map = 'auto',\n",
    "    load_in_8bit = True,\n",
    ")\n",
    "\n",
    "pipe = pipeline(\n",
    "    task=\"text2text-generation\", \n",
    "    model=model, \n",
    "    tokenizer=tokenizer, \n",
    "    max_new_tokens=256, \n",
    "    model_kwargs = { \n",
    "        \"temperature\":0, \n",
    "        \"repetition_penalty\": 1.5\n",
    "    }, \n",
    ")\n",
    "\n",
    "llm = HuggingFacePipeline(pipeline = pipe)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Agent\n",
    "This is the chain responsible for deciding what step to take next, which powered by a language model and a prompt. The inputs to this chain are:\n",
    "\n",
    "- `Tools`: Descriptions of available tools\n",
    "- `User input`: The high level objective\n",
    "- `Intermediate steps`: Any (action, tool output) pairs previously executed in order to achieve the user input\n",
    "\n",
    "The output is the next action(s) to take or the final response to send to the user (AgentActions or AgentFinish). An action specifies a tool and the input to that tool.\n",
    "\n",
    "Different agents have different prompting styles for reasoning, different ways of encoding inputs, and different ways of parsing the output. [Explore Agent Types here](https://python.langchain.com/docs/modules/agents/agent_types/)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Tools\n",
    "Tools are functions that an agent can invoke. There are two important design considerations around tools:\n",
    "1. Giving the agent access to the right tools\n",
    "2. Describing the tools in a way that is most helpful to the agent\n",
    "   \n",
    "Without thinking through both, you won't be able to build a working agent. If you don't give the agent access to a correct set of tools, it will never be able to accomplish the objectives you give it. If you don't describe the tools well, the agent won't know how to use them properly.\n",
    "\n",
    "LangChain provides a wide set of built-in tools, but also makes it easy to define your own (including custom descriptions). [Explore the tools integrations section](https://python.langchain.com/docs/integrations/tools/)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "636"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain.document_loaders import PyMuPDFLoader\n",
    "\n",
    "nlp_document = './docs/pdf/SpeechandLanguageProcessing_3rd_07jan2023.pdf'\n",
    "loader = PyMuPDFLoader(nlp_document)\n",
    "documents = loader.load()\n",
    "len(documents)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3421"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "import torch\n",
    "from langchain_community.embeddings import HuggingFaceInstructEmbeddings\n",
    "from langchain.vectorstores import FAISS\n",
    "\n",
    "text_splitter = RecursiveCharacterTextSplitter(\n",
    "    chunk_size = 700,\n",
    "    chunk_overlap = 100\n",
    ")\n",
    "docs = text_splitter.split_documents(documents) \n",
    "len(docs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      ".gitattributes: 100%|██████████| 1.48k/1.48k [00:00<00:00, 10.9MB/s]\n",
      "1_Pooling/config.json: 100%|██████████| 270/270 [00:00<00:00, 2.43MB/s]\n",
      "2_Dense/config.json: 100%|██████████| 115/115 [00:00<00:00, 769kB/s]\n",
      "pytorch_model.bin: 100%|██████████| 2.36M/2.36M [00:00<00:00, 11.7MB/s]\n",
      "README.md: 100%|██████████| 66.2k/66.2k [00:00<00:00, 242kB/s]\n",
      "config.json: 100%|██████████| 1.55k/1.55k [00:00<00:00, 10.0MB/s]\n",
      "config_sentence_transformers.json: 100%|██████████| 122/122 [00:00<00:00, 912kB/s]\n",
      "pytorch_model.bin: 100%|██████████| 439M/439M [00:37<00:00, 11.6MB/s] \n",
      "sentence_bert_config.json: 100%|██████████| 53.0/53.0 [00:00<00:00, 346kB/s]\n",
      "special_tokens_map.json: 100%|██████████| 2.20k/2.20k [00:00<00:00, 15.7MB/s]\n",
      "spiece.model: 100%|██████████| 792k/792k [00:04<00:00, 175kB/s]\n",
      "tokenizer.json: 100%|██████████| 2.42M/2.42M [00:01<00:00, 1.47MB/s]\n",
      "tokenizer_config.json: 100%|██████████| 2.43k/2.43k [00:00<00:00, 16.5MB/s]\n",
      "modules.json: 100%|██████████| 461/461 [00:00<00:00, 3.51MB/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "load INSTRUCTOR_Transformer\n",
      "max_seq_length  512\n"
     ]
    }
   ],
   "source": [
    "model_name = 'hkunlp/instructor-base'\n",
    "\n",
    "embedding_model = HuggingFaceInstructEmbeddings(\n",
    "    model_name = model_name,              \n",
    "    model_kwargs = {\n",
    "        'device': torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "    },\n",
    ")\n",
    "\n",
    "vectordb = FAISS.from_documents(docs, embedding_model)\n",
    "retriever = vectordb.as_retriever()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Retriever Tool\n",
    "Now we need to create a tool for our retriever. The main things we need to pass in are a name for the retriever as well as a description. These will both be used by the language model, so they should be informative."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Tool(name='search_nlp_stanford', description='Searches and returns documents regarding the nlp-stanford.', args_schema=<class 'langchain.tools.retriever.RetrieverInput'>, func=<bound method BaseRetriever.get_relevant_documents of VectorStoreRetriever(tags=['FAISS', 'HuggingFaceInstructEmbeddings'], vectorstore=<langchain_community.vectorstores.faiss.FAISS object at 0x7f8484c29db0>)>, coroutine=<bound method BaseRetriever.aget_relevant_documents of VectorStoreRetriever(tags=['FAISS', 'HuggingFaceInstructEmbeddings'], vectorstore=<langchain_community.vectorstores.faiss.FAISS object at 0x7f8484c29db0>)>)"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain.agents.agent_toolkits import create_retriever_tool\n",
    "retriever_tool = create_retriever_tool(\n",
    "    retriever,\n",
    "    \"search_nlp_stanford\",\n",
    "    \"Searches and returns documents regarding the nlp-stanford.\",\n",
    ")\n",
    "retriever_tool"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "tools = [retriever_tool]"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. AgentExecutor\n",
    "The agent executor is the runtime for an agent. This is what actually calls the agent, executes the actions it chooses, passes the action outputs back to the agent, and repeats."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Agent Constructor\n",
    "Here, we will use the high level create_conversational_retrieval_agent API to construct the agent.\n",
    "\n",
    "Notice that beside the list of tools, the only thing we need to pass in is a language model to use."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.agents.agent_toolkits import create_conversational_retrieval_agent\n",
    "\n",
    "agent_executor = create_conversational_retrieval_agent(\n",
    "    llm, \n",
    "    tools, \n",
    "    verbose=True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\u001b[1m> Entering new AgentExecutor chain...\u001b[0m\n",
      "\u001b[32;1m\u001b[1;3m<pad> Chaky, how can I help you today?\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'input': \"hi, I'm Chaky\",\n",
       " 'chat_history': [HumanMessage(content=\"hi, I'm Chaky\"),\n",
       "  AIMessage(content='<pad> Chaky, how can I help you today?')],\n",
       " 'output': '<pad> Chaky, how can I help you today?',\n",
       " 'intermediate_steps': []}"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# We can now try it out!\n",
    "result = agent_executor(\n",
    "    {\n",
    "        \"input\": \"hi, I'm Chaky\"\n",
    "    }\n",
    ")\n",
    "result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\u001b[1m> Entering new AgentExecutor chain...\u001b[0m\n",
      "\u001b[32;1m\u001b[1;3m<pad> AI:<< pad> Chaky, I apologize for the confusion. I didn't understand your question. Could you please provide more context or clarify it for me?\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'input': 'How is it different between constituency parser and dependency parser?',\n",
       " 'chat_history': [HumanMessage(content=\"hi, I'm Chaky\"),\n",
       "  AIMessage(content='<pad> Chaky, how can I help you today?'),\n",
       "  HumanMessage(content='How is it different between constituency parser and dependency parser?'),\n",
       "  AIMessage(content=\"<pad> AI:<< pad> Chaky, I apologize for the confusion. I didn't understand your question. Could you please provide more context or clarify it for me?\")],\n",
       " 'output': \"<pad> AI:<< pad> Chaky, I apologize for the confusion. I didn't understand your question. Could you please provide more context or clarify it for me?\",\n",
       " 'intermediate_steps': []}"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "result = agent_executor(\n",
    "    {\n",
    "        \"input\": \"How is it different between constituency parser and dependency parser?\"\n",
    "    }\n",
    ")\n",
    "result"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Creating from components\n",
    "\n",
    "What actually is going on underneath the hood? Let’s take a look so we can understand how to modify going forward.\n",
    "\n",
    "There are a few components:\n",
    "\n",
    "- The memory\n",
    "- The prompt template\n",
    "- The agent\n",
    "- The agent executor"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The Memory\n",
    "In this example, we want the agent to remember not only previous conversations, but also previous intermediate steps. For that, we can use AgentTokenBufferMemory. \n",
    "\n",
    "Note that if you want to change whether the agent remembers intermediate steps, or how the long the buffer is, or anything like that you should change this part."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This is needed for both the memory and the prompt\n",
    "memory_key = \"history\"\n",
    "\n",
    "from langchain.agents.openai_functions_agent.agent_token_buffer_memory import (\n",
    "    AgentTokenBufferMemory,\n",
    ")\n",
    "\n",
    "memory = AgentTokenBufferMemory(memory_key=memory_key, llm=llm)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The Prompt Template\n",
    "For the prompt template, we will use the OpenAIFunctionsAgent default way of creating one, but pass in a system prompt and a placeholder for memory."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ChatPromptTemplate(input_variables=['agent_scratchpad', 'history', 'input'], input_types={'history': typing.List[typing.Union[langchain_core.messages.ai.AIMessage, langchain_core.messages.human.HumanMessage, langchain_core.messages.chat.ChatMessage, langchain_core.messages.system.SystemMessage, langchain_core.messages.function.FunctionMessage, langchain_core.messages.tool.ToolMessage]], 'agent_scratchpad': typing.List[typing.Union[langchain_core.messages.ai.AIMessage, langchain_core.messages.human.HumanMessage, langchain_core.messages.chat.ChatMessage, langchain_core.messages.system.SystemMessage, langchain_core.messages.function.FunctionMessage, langchain_core.messages.tool.ToolMessage]]}, messages=[SystemMessage(content=\"I'm your friendly NLP chatbot named ChakyBot, here to assist Chaky and Gun with any questions they have about Natural Language Processing (NLP). \\n    If you're curious about how probability works in the context of NLP, feel free to ask any questions you may have. \\n    Whether it's about probabilistic models, language models, or any other related topic, \\n    I'm here to help break down complex concepts into easy-to-understand explanations.\\n    Just let me know what you're wondering about, and I'll do my best to guide you through it!\"), MessagesPlaceholder(variable_name='history'), HumanMessagePromptTemplate(prompt=PromptTemplate(input_variables=['input'], template='{input}')), MessagesPlaceholder(variable_name='agent_scratchpad')])"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain.prompts import MessagesPlaceholder\n",
    "from langchain_core.messages import SystemMessage\n",
    "from langchain.agents.openai_functions_agent.base import OpenAIFunctionsAgent\n",
    "\n",
    "system_message = SystemMessage(\n",
    "    content=(\n",
    "    \"\"\"I'm your friendly NLP chatbot named ChakyBot, here to assist Chaky and Gun with any questions they have about Natural Language Processing (NLP). \n",
    "    If you're curious about how probability works in the context of NLP, feel free to ask any questions you may have. \n",
    "    Whether it's about probabilistic models, language models, or any other related topic, \n",
    "    I'm here to help break down complex concepts into easy-to-understand explanations.\n",
    "    Just let me know what you're wondering about, and I'll do my best to guide you through it!\"\"\"\n",
    "    )\n",
    ")\n",
    "\n",
    "prompt = OpenAIFunctionsAgent.create_prompt(\n",
    "    system_message=system_message,\n",
    "    extra_prompt_messages=[MessagesPlaceholder(variable_name=memory_key)],\n",
    ")\n",
    "\n",
    "prompt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.agents.openai_functions_agent.base import OpenAIFunctionsAgent\n",
    "from langchain.agents import AgentExecutor\n",
    "\n",
    "# We will use the OpenAIFunctionsAgent\n",
    "agent = OpenAIFunctionsAgent(\n",
    "    llm=llm, \n",
    "    tools=tools, \n",
    "    prompt=prompt,\n",
    "    verbose=True,\n",
    ")\n",
    "\n",
    "agent_executor = AgentExecutor(\n",
    "    agent=agent,\n",
    "    tools=tools,\n",
    "    memory=memory,\n",
    "    verbose=True,\n",
    "    return_intermediate_steps=True,\n",
    ")\n",
    "\n",
    "# Importantly, we pass in return_intermediate_steps=True since we are recording that with our memory object"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\u001b[1m> Entering new AgentExecutor chain...\u001b[0m\n",
      "\u001b[32;1m\u001b[1;3m<pad> System:  I'm  ChakyBot,  a  chatbot  designed  to  assist  Chaky  and  Gun  with  their  Natural  Language  Processing  (NLP)  questions.  I'm  here  to  help  break  down  complex  concepts  into  easy-to-understand  explanations,  and  I'm  always  happy  to  answer  any  questions  you  may  have.  Just  let  me  know  what  you're  curious  about,  and  I'll  do  my  best  to  guide  you  through  it!\n",
      "\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'input': 'Who are you by the way?',\n",
       " 'history': [HumanMessage(content='Who are you by the way?'),\n",
       "  AIMessage(content=\"<pad> System:  I'm  ChakyBot,  a  chatbot  designed  to  assist  Chaky  and  Gun  with  their  Natural  Language  Processing  (NLP)  questions.  I'm  here  to  help  break  down  complex  concepts  into  easy-to-understand  explanations,  and  I'm  always  happy  to  answer  any  questions  you  may  have.  Just  let  me  know  what  you're  curious  about,  and  I'll  do  my  best  to  guide  you  through  it!\\n\")],\n",
       " 'output': \"<pad> System:  I'm  ChakyBot,  a  chatbot  designed  to  assist  Chaky  and  Gun  with  their  Natural  Language  Processing  (NLP)  questions.  I'm  here  to  help  break  down  complex  concepts  into  easy-to-understand  explanations,  and  I'm  always  happy  to  answer  any  questions  you  may  have.  Just  let  me  know  what  you're  curious  about,  and  I'll  do  my  best  to  guide  you  through  it!\\n\",\n",
       " 'intermediate_steps': []}"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "result = agent_executor({\"input\": \"Who are you by the way?\"})\n",
    "result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\u001b[1m> Entering new AgentExecutor chain...\u001b[0m\n",
      "\u001b[32;1m\u001b[1;3m<pad> AI:<< pad>\n",
      " System:  Transformers  are  a  type  of  neural  network  architecture  that  are  used  to  process  and  generate  natural  language  text.  They  are  a  type  of  language  model  that  can  be  trained  to  understand  and  generate  text  in  a  given  context.  Transformers  are  often  used  in  NLP  tasks  such  as  sentiment  analysis,  text  classification,  and  language  modeling.  They  are  designed  to  learn  the  structure  and  meaning  of  text  data,  and  they  can  be  used  to  build  more  complex  models  that  can  handle  more  complex  tasks  such  as  text  generation  and  generation.\n",
      "\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'input': 'What is transformers?',\n",
       " 'history': [HumanMessage(content='Who are you by the way?'),\n",
       "  AIMessage(content=\"<pad> System:  I'm  ChakyBot,  a  chatbot  designed  to  assist  Chaky  and  Gun  with  their  Natural  Language  Processing  (NLP)  questions.  I'm  here  to  help  break  down  complex  concepts  into  easy-to-understand  explanations,  and  I'm  always  happy  to  answer  any  questions  you  may  have.  Just  let  me  know  what  you're  curious  about,  and  I'll  do  my  best  to  guide  you  through  it!\\n\"),\n",
       "  HumanMessage(content='What is transformers?'),\n",
       "  AIMessage(content='<pad> AI:<< pad>\\n System:  Transformers  are  a  type  of  neural  network  architecture  that  are  used  to  process  and  generate  natural  language  text.  They  are  a  type  of  language  model  that  can  be  trained  to  understand  and  generate  text  in  a  given  context.  Transformers  are  often  used  in  NLP  tasks  such  as  sentiment  analysis,  text  classification,  and  language  modeling.  They  are  designed  to  learn  the  structure  and  meaning  of  text  data,  and  they  can  be  used  to  build  more  complex  models  that  can  handle  more  complex  tasks  such  as  text  generation  and  generation.\\n')],\n",
       " 'output': '<pad> AI:<< pad>\\n System:  Transformers  are  a  type  of  neural  network  architecture  that  are  used  to  process  and  generate  natural  language  text.  They  are  a  type  of  language  model  that  can  be  trained  to  understand  and  generate  text  in  a  given  context.  Transformers  are  often  used  in  NLP  tasks  such  as  sentiment  analysis,  text  classification,  and  language  modeling.  They  are  designed  to  learn  the  structure  and  meaning  of  text  data,  and  they  can  be  used  to  build  more  complex  models  that  can  handle  more  complex  tasks  such  as  text  generation  and  generation.\\n',\n",
       " 'intermediate_steps': []}"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "result = agent_executor({\"input\": \"What is transformers?\"})\n",
    "result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[HumanMessage(content='Who are you by the way?'),\n",
       " AIMessage(content=\"<pad> System:  I'm  ChakyBot,  a  chatbot  designed  to  assist  Chaky  and  Gun  with  their  Natural  Language  Processing  (NLP)  questions.  I'm  here  to  help  break  down  complex  concepts  into  easy-to-understand  explanations,  and  I'm  always  happy  to  answer  any  questions  you  may  have.  Just  let  me  know  what  you're  curious  about,  and  I'll  do  my  best  to  guide  you  through  it!\\n\"),\n",
       " HumanMessage(content='What is transformers?'),\n",
       " AIMessage(content='<pad> AI:<< pad>\\n System:  Transformers  are  a  type  of  neural  network  architecture  that  are  used  to  process  and  generate  natural  language  text.  They  are  a  type  of  language  model  that  can  be  trained  to  understand  and  generate  text  in  a  given  context.  Transformers  are  often  used  in  NLP  tasks  such  as  sentiment  analysis,  text  classification,  and  language  modeling.  They  are  designed  to  learn  the  structure  and  meaning  of  text  data,  and  they  can  be  used  to  build  more  complex  models  that  can  handle  more  complex  tasks  such  as  text  generation  and  generation.\\n')]"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "result['history']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
