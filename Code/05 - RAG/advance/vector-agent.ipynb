{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "device(type='cpu')"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os\n",
    "import torch\n",
    "# Set GPU device\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"1\"\n",
    "\n",
    "os.environ['http_proxy']  = 'http://192.41.170.23:3128'\n",
    "os.environ['https_proxy'] = 'http://192.41.170.23:3128'\n",
    "\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loaders\n",
    "from langchain.schema.document import Document\n",
    "from langchain_community.document_loaders import PyMuPDFLoader\n",
    "# Model\n",
    "from langchain_openai import OpenAIEmbeddings\n",
    "from langchain_openai import ChatOpenAI\n",
    "# Embedding Support\n",
    "from langchain_community.vectorstores import FAISS\n",
    "from langchain_community.vectorstores import Chroma\n",
    "# Agent & Tool\n",
    "from langchain.agents.agent_toolkits.vectorstore.base import create_vectorstore_agent\n",
    "from langchain.agents.agent_toolkits.vectorstore.toolkit import VectorStoreToolkit, VectorStoreInfo\n",
    "from langchain.agents.agent_toolkits.vectorstore.prompt import PREFIX, ROUTER_PREFIX\n",
    "\n",
    "from dotenv import load_dotenv\n",
    "load_dotenv()\n",
    "\n",
    "from dotenv import load_dotenv\n",
    "import os\n",
    "\n",
    "load_dotenv()\n",
    "openai_api_key = os.getenv(\"OPENAI_API_KEY\")\n",
    "openai_organization = os.getenv(\"OPENAI_API_ORG\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "llm = ChatOpenAI(\n",
    "    model_name  = \"gpt-3.5-turbo-16k\",\n",
    "    temperature = 0, \n",
    "    max_tokens  = 4096,\n",
    "    openai_api_key=openai_api_key, \n",
    "    openai_organization=openai_organization\n",
    ")\n",
    "embedding_model = OpenAIEmbeddings(\n",
    "    openai_api_key=openai_api_key, \n",
    "    openai_organization=openai_organization\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "pdf_path = './docs/LLaVA.pdf' \n",
    "pdf_loader = PyMuPDFLoader(pdf_path)\n",
    "pages = pdf_loader.load_and_split()\n",
    "\n",
    "vectorstore = FAISS.from_documents(\n",
    "    pages, \n",
    "    embedding=embedding_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create vectorstore info object\n",
    "vectorstore_info = VectorStoreInfo(\n",
    "    name        = 'vectorStoreTool',\n",
    "    description = 'a sample vectorstore',\n",
    "    vectorstore = vectorstore,)\n",
    "\n",
    "# Convert vectorstore info object to agent toolkit\n",
    "toolkit = VectorStoreToolkit(\n",
    "    vectorstore_info = vectorstore_info, \n",
    "    llm = llm, )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/homebrew/lib/python3.11/site-packages/langchain_core/_api/deprecation.py:189: LangChainDeprecationWarning: The class `OpenAI` was deprecated in LangChain 0.1.0 and will be removed in 0.2.0. Use langchain_openai.OpenAI instead.\n",
      "  warn_deprecated(\n",
      "/opt/homebrew/lib/python3.11/site-packages/langchain_core/_api/deprecation.py:189: LangChainDeprecationWarning: The class `OpenAI` was deprecated in LangChain 0.1.0 and will be removed in 0.2.0. Use langchain_openai.OpenAI instead.\n",
      "  warn_deprecated(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\u001b[1m> Entering new AgentExecutor chain...\u001b[0m\n",
      "\u001b[32;1m\u001b[1;3mTo summarize all documents, I should use the vectorStoreTool.\n",
      "Action: vectorStoreTool\n",
      "Action Input: \"Summary all documents\"\u001b[0m\n",
      "Observation: \u001b[36;1m\u001b[1;3mThe documents provided contain information about evaluating the performance of GPT-4 (a language model) on image-related tasks. Table 3 shows the relative scores of GPT-4 compared to other settings on different types of questions. The instructions for brief and detailed image descriptions are listed in Tables 8 and 9 respectively. The CC3M dataset is described, which involves extracting noun-phrases from captions. A travel blog post prompt is given, asking for an engaging post about a trip. Figure 3 demonstrates LLaVA's ability to recognize visual content and provide detailed responses. The limitations of a simplified version of GPT-4 are discussed, and the use of language-only GPT-4 or ChatGPT to create instruction-following data involving visual content is explained.\u001b[0m\n",
      "Thought:\u001b[32;1m\u001b[1;3mI now know the summary of all the documents.\n",
      "Final Answer: The documents contain information about evaluating the performance of GPT-4 on image-related tasks, instructions for image descriptions, details about the CC3M dataset, a travel blog post prompt, demonstrations of LLaVA's ability to recognize visual content, and discussions about the limitations of a simplified version of GPT-4 and creating instruction-following data involving visual content.\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "# Create the agent executor\n",
    "PREFIX = \"\"\"You are an agent designed to answer questions about sets of documents.\n",
    "You have access to tools for interacting with the documents, and the inputs to the tools are questions.\n",
    "\"\"\"\n",
    "user_prompt = 'Summary all documents'\n",
    "agent_executor = create_vectorstore_agent(\n",
    "    llm=llm, toolkit=toolkit, prefix=PREFIX, verbose=True)\n",
    "\n",
    "result = agent_executor.run(user_prompt)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"The documents contain information about evaluating the performance of GPT-4 on image-related tasks, instructions for image descriptions, details about the CC3M dataset, a travel blog post prompt, demonstrations of LLaVA's ability to recognize visual content, and discussions about the limitations of a simplified version of GPT-4 and creating instruction-following data involving visual content.\""
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
