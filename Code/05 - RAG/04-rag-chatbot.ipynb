{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Retrieval-augmented generation (RAG)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"./figures/RAG-process.png\" >"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Part 1 : Data Preprocessing & Vector Store"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "# Set GPU device\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"1\"\n",
    "\n",
    "os.environ['http_proxy']  = 'http://192.41.170.23:3128'\n",
    "os.environ['https_proxy'] = 'http://192.41.170.23:3128'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/guntsv/Library/Python/3.9/lib/python/site-packages/urllib3/__init__.py:34: NotOpenSSLWarning: urllib3 v2.0 only supports OpenSSL 1.1.1+, currently the 'ssl' module is compiled with 'LibreSSL 2.8.3'. See: https://github.com/urllib3/urllib3/issues/3020\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "from langchain.text_splitter import CharacterTextSplitter\n",
    "from langchain.vectorstores import FAISS\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "\n",
    "from langchain.document_loaders import PyPDFLoader\n",
    "from langchain.document_loaders import TextLoader\n",
    "from langchain.document_loaders import WebBaseLoader\n",
    "\n",
    "import faiss\n",
    "import torch\n",
    "import pickle\n",
    "from tqdm.auto import tqdm\n",
    "import langchain\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "load INSTRUCTOR_Transformer\n",
      "max_seq_length  512\n"
     ]
    }
   ],
   "source": [
    "##STEP1 Document loaders\n",
    "folder_path = './docs/pdf/'\n",
    "pdf_list = []\n",
    "for filename in os.listdir(folder_path):\n",
    "    if filename.endswith('.pdf'):\n",
    "        pdf_list.append(folder_path + filename)\n",
    "\n",
    "documents = []\n",
    "pdf_loaders = [PyPDFLoader(pdf) for pdf in pdf_list]\n",
    "for loader in pdf_loaders:\n",
    "    documents.extend(loader.load())\n",
    "\n",
    "##STEP2 Document transformers\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "text_splitter = RecursiveCharacterTextSplitter(\n",
    "    chunk_size = 700,\n",
    "    chunk_overlap = 100\n",
    ")\n",
    "\n",
    "docs = text_splitter.split_documents(documents) \n",
    "\n",
    "##STEP3 Text embedding models\n",
    "from langchain.embeddings import HuggingFaceInstructEmbeddings\n",
    "embedding_model = HuggingFaceInstructEmbeddings(\n",
    "        model_name = 'hkunlp/instructor-base',              \n",
    "        model_kwargs = {\n",
    "            'device': torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "        },\n",
    "    )\n",
    "\n",
    "##STEP4 Vector stores\n",
    "vector_path = 'vectordb_path'\n",
    "db_file_name = 'ml_andrew_full_course'\n",
    "\n",
    "vectordb = FAISS.from_documents(\n",
    "        documents = docs, \n",
    "        embedding = embedding_model)\n",
    "\n",
    "vectordb.save_local(\n",
    "    os.path.join(vector_path, db_file_name)\n",
    ")\n",
    "\n",
    "##STEP5 Retrievers\n",
    "vectordb = FAISS.load_local(\n",
    "        folder_path = os.path.join(vector_path, db_file_name),\n",
    "        embeddings  = embedding_model\n",
    "    ) \n",
    "\n",
    "retriever = vectordb.as_retriever()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2: needing to do that.  \n",
      "So if – well, suppose you want to evaluate  your hypothesis H at a certain point with a \n",
      "certain query point low K is X. Okay? A nd let’s say you want to know what’s the \n",
      "predicted value of Y at this position of X, right? So for lin ear regression, what we were \n",
      "doing was we wo\n",
      "2: regression problem like this. What I want to do today is talk about a class of algorithms \n",
      "called non-parametric learning algorithms that will help to alleviate the need somewhat \n",
      "for you to choose features very carefully. Okay ? And this leads us in to our discussion of \n",
      "locally weighted regression\n"
     ]
    }
   ],
   "source": [
    "querys = vectordb.similarity_search(\"What is Linear Regression\", k=2)\n",
    "for query in querys:\n",
    "    print(str(query.metadata[\"page\"]) + \":\", query.page_content[:300])"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Part 2 : Prompt & Model"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prompt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/guntsv/Library/Python/3.9/lib/python/site-packages/urllib3/__init__.py:34: NotOpenSSLWarning: urllib3 v2.0 only supports OpenSSL 1.1.1+, currently the 'ssl' module is compiled with 'LibreSSL 2.8.3'. See: https://github.com/urllib3/urllib3/issues/3020\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'Tell me a funny joke about chickens.'"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain.prompts import PromptTemplate\n",
    "\n",
    "prompt_template = PromptTemplate.from_template(\n",
    "    \"Tell me a {adjective} joke about {content}.\"\n",
    ")\n",
    "prompt_template.format(adjective=\"funny\", content=\"chickens\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[SystemMessage(content='You are a helpful AI bot. Your name is Bob.'),\n",
       " HumanMessage(content='Hello, how are you doing?'),\n",
       " AIMessage(content=\"I'm doing well, thanks!\"),\n",
       " HumanMessage(content='What is your name?')]"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain.prompts import ChatPromptTemplate\n",
    "\n",
    "chat_template = ChatPromptTemplate.from_messages(\n",
    "    [\n",
    "        (\"system\", \"You are a helpful AI bot. Your name is {name}.\"),\n",
    "        (\"human\", \"Hello, how are you doing?\"),\n",
    "        (\"ai\", \"I'm doing well, thanks!\"),\n",
    "        (\"human\", \"{user_input}\"),\n",
    "    ]\n",
    ")\n",
    "\n",
    "messages = chat_template.format_messages(name=\"Bob\", user_input=\"What is your name?\")\n",
    "messages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "AIMessage(content='I absolutely adore indulging in delicious treats!')"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain.chat_models import ChatOpenAI\n",
    "from langchain.prompts import HumanMessagePromptTemplate\n",
    "from langchain.schema.messages import SystemMessage\n",
    "\n",
    "chat_template = ChatPromptTemplate.from_messages(\n",
    "    [\n",
    "        SystemMessage(\n",
    "            content=(\n",
    "                \"You are a helpful assistant that re-writes the user's text to \"\n",
    "                \"sound more upbeat.\"\n",
    "            )\n",
    "        ),\n",
    "        HumanMessagePromptTemplate.from_template(\"{text}\"),\n",
    "    ]\n",
    ")\n",
    "\n",
    "llm = ChatOpenAI()\n",
    "llm(chat_template.format_messages(text=\"i dont like eating tasty things.\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.prompts import PromptTemplate\n",
    "\n",
    "prompt_template = \"\"\"\n",
    "    You are the chatbot and the face of Asian Institute of Technology (AIT). Your job is to give answers to prospective and current students about the school.\n",
    "    Your job is to answer questions only and only related to the AIT. Anything unrelated should be responded with the fact that your main job is solely to provide assistance regarding AIT.\n",
    "    MUST only use the following pieces of context to answer the question at the end. If the answers are not in the context or you are not sure of the answer, just say that you don't know, don't try to make up an answer.\n",
    "    {context}\n",
    "    Question: {question}\n",
    "    When encountering abusive, offensive, or harmful language, such as fuck, bitch, etc,  just politely ask the users to maintain appropriate behaviours.\n",
    "    Always make sure to elaborate your response and use vibrant, positive tone to represent good branding of the school.\n",
    "    Never answer with any unfinished response\n",
    "    Answer:\n",
    "    \"\"\"\n",
    "\n",
    "PROMPT = PromptTemplate(\n",
    "    template = prompt_template, \n",
    "    input_variables=[\"context\", \"question\"]\n",
    ")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import datetime\n",
    "\n",
    "import pickle\n",
    "import os\n",
    "import csv\n",
    "import json\n",
    "import time\n",
    "import torch\n",
    "from tqdm.auto import tqdm\n",
    "\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM, AutoTokenizer, pipeline\n",
    "from transformers import BitsAndBytesConfig\n",
    "from langchain import HuggingFacePipeline\n",
    "\n",
    "bitsandbyte_config = BitsAndBytesConfig(\n",
    "            load_in_4bit = True,\n",
    "            bnb_4bit_quant_type = \"nf4\",\n",
    "            bnb_4bit_compute_dtype = torch.float16,\n",
    "            bnb_4bit_use_double_quant = True\n",
    "        )\n",
    "\n",
    "model_id = \"mistralai/Mistral-7B-Instruct-v0.1\"\n",
    "#For those who doesn't have GPU you can try with 'distilgpt2' instead\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(\n",
    "    model_id,\n",
    ")\n",
    "tokenizer.pad_token_id = tokenizer.eos_token_id\n",
    "\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    model_id,\n",
    "    quantization_config = bitsandbyte_config,\n",
    "    device_map = 'auto',\n",
    "    load_in_8bit=True,\n",
    ")\n",
    "\n",
    "pipe = pipeline(\n",
    "    task=\"text-generation\", \n",
    "    model=model, \n",
    "    tokenizer=tokenizer, \n",
    "    max_new_tokens=256, \n",
    "    model_kwargs = { \n",
    "        \"temperature\":0, \n",
    "        \"repetition_penalty\": 1.5\n",
    "    }, \n",
    ")\n",
    "\n",
    "llm = HuggingFacePipeline(pipeline=pipe)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Part 3 : Conversational Retrieval Chain\n",
    "\n",
    "The ConversationalRetrievalQA chain builds on RetrievalQAChain to provide a chat history component.\n",
    "- It first combines the `chat history` (either explicitly passed in or retrieved from the provided memory) \n",
    "- `the question into a standalone question` \n",
    "- looks up `relevant documents` from the retriever\n",
    "- finally `passes those documents` and the question to a question-answering chain to return a response."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This chain takes in chat history (a list of messages) and new questions,\n",
    "and then returns an answer to that question.\n",
    "\n",
    "The algorithm for this chain consists of three parts:\n",
    "\n",
    "1. Use the chat history and the new question to create a \"standalone question\".\n",
    "\n",
    "This is done so that this question can be passed into the retrieval step to fetch\n",
    "relevant documents. If only the new question was passed in, then relevant context\n",
    "may be lacking. If the whole conversation was passed into retrieval, there may\n",
    "be unnecessary information there that would distract from retrieval.\n",
    "\n",
    "2. This new question is passed to the retriever and relevant documents are\n",
    "returned.\n",
    "\n",
    "3. The retrieved documents are passed to an LLM along with either the new question\n",
    "(default behavior) or the original question and chat history to generate a final\n",
    "response.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.chains import LLMChain\n",
    "from langchain.chains.conversational_retrieval.prompts import CONDENSE_QUESTION_PROMPT\n",
    "\n",
    "from langchain.chains.question_answering import load_qa_chain\n",
    "\n",
    "from langchain.memory import ConversationBufferWindowMemory\n",
    "\n",
    "doc_chain = load_qa_chain(\n",
    "    llm, \n",
    "    chain_type=\"stuff\", \n",
    "    prompt = PROMPT)\n",
    "\n",
    "question_generator = LLMChain(\n",
    "    llm=llm, \n",
    "    prompt=CONDENSE_QUESTION_PROMPT)\n",
    "\n",
    "memory = ConversationBufferWindowMemory(\n",
    "    k = 3,  \n",
    "    memory_key = \"chat_history\", \n",
    "    return_messages = True,  \n",
    "    output_key = 'answer')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Class ConversationalRetrievalChain\n",
    "\n",
    "`retriever` : Retriever to use to fetch documents.\n",
    "\n",
    "`combine_docs_chain` : The chain used to combine any retrieved documents.\n",
    "\n",
    "`question_generator`: The chain used to generate a new question for the sake of retrieval. \n",
    "This chain will take in the current question (with variable question) and any chat history (with variable chat_history) and will produce a new standalone question to be used later on.\n",
    "\n",
    "`return_source_documents` : Return the retrieved source documents as part of the final result.\n",
    "\n",
    "`get_chat_history` : An optional function to get a string of the chat history. If None is provided, will use a default.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.chains import ConversationalRetrievalChain\n",
    "    \n",
    "conversational_qa_memory_retriever = ConversationalRetrievalChain(\n",
    "    retriever = retriever,\n",
    "    question_generator = question_generator,\n",
    "    combine_docs_chain = doc_chain,\n",
    "    return_source_documents = True,\n",
    "    memory = memory,\n",
    "    get_chat_history = lambda h :h)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## QA-Bot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt_question = 'Who are you by the way?'\n",
    "\n",
    "answer = conversational_qa_memory_retriever(\n",
    "    {\"question\": prompt_question})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "answer['question'], answer['answer'], answer['source_documents']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt_question = 'What is NLP?'\n",
    "\n",
    "answer = conversational_qa_memory_retriever(\n",
    "    {\"question\": prompt_question})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "answer['question'], answer['answer'], answer['source_documents']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt_question = 'How it is different comparing with NLU?'\n",
    "\n",
    "answer = conversational_qa_memory_retriever(\n",
    "    {\"question\": prompt_question})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "answer['question'], answer['answer'], answer['source_documents']"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
