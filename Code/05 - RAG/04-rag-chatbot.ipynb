{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Retrieval-augmented generation (RAG)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"./figures/RAG-process.png\" >"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Part 1 : Data Preprocessing & Vector Store"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "# Set GPU device\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"1\"\n",
    "\n",
    "os.environ['http_proxy']  = 'http://192.41.170.23:3128'\n",
    "os.environ['https_proxy'] = 'http://192.41.170.23:3128'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.text_splitter import CharacterTextSplitter\n",
    "from langchain.vectorstores import FAISS\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "\n",
    "from langchain.document_loaders import PyPDFLoader\n",
    "from langchain.document_loaders import TextLoader\n",
    "from langchain.document_loaders import WebBaseLoader\n",
    "\n",
    "import torch\n",
    "from tqdm.auto import tqdm\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "load INSTRUCTOR_Transformer\n",
      "'NoneType' object has no attribute 'cadam32bit_grad_fp32'\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/guntsv/Library/Python/3.9/lib/python/site-packages/bitsandbytes/cextension.py:34: UserWarning: The installed version of bitsandbytes was compiled without GPU support. 8-bit optimizers, 8-bit multiplication, and GPU quantization are unavailable.\n",
      "  warn(\"The installed version of bitsandbytes was compiled without GPU support. \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "max_seq_length  512\n"
     ]
    }
   ],
   "source": [
    "##STEP1 Document loaders\n",
    "folder_path = './docs/pdf/'\n",
    "pdf_list = []\n",
    "for filename in os.listdir(folder_path):\n",
    "    if filename.endswith('.pdf'):\n",
    "        pdf_list.append(folder_path + filename)\n",
    "\n",
    "documents = []\n",
    "pdf_loaders = [PyPDFLoader(pdf) for pdf in pdf_list]\n",
    "for loader in pdf_loaders:\n",
    "    documents.extend(loader.load())\n",
    "\n",
    "##STEP2 Document transformers\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "text_splitter = RecursiveCharacterTextSplitter(\n",
    "    chunk_size = 700,\n",
    "    chunk_overlap = 100\n",
    ")\n",
    "\n",
    "docs = text_splitter.split_documents(documents) \n",
    "\n",
    "##STEP3 Text embedding models\n",
    "from langchain.embeddings import HuggingFaceInstructEmbeddings\n",
    "embedding_model = HuggingFaceInstructEmbeddings(\n",
    "        model_name = 'hkunlp/instructor-base',              \n",
    "        model_kwargs = {\n",
    "            'device': torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "        },\n",
    "    )\n",
    "\n",
    "##STEP4 Vector stores\n",
    "vector_path = 'vectordb_path'\n",
    "db_file_name = 'ml_andrew_full_course'\n",
    "\n",
    "vectordb = FAISS.from_documents(\n",
    "        documents = docs, \n",
    "        embedding = embedding_model)\n",
    "\n",
    "vectordb.save_local(\n",
    "    os.path.join(vector_path, db_file_name)\n",
    ")\n",
    "\n",
    "##STEP5 Retrievers\n",
    "vectordb = FAISS.load_local(\n",
    "        folder_path = os.path.join(vector_path, db_file_name),\n",
    "        embeddings  = embedding_model\n",
    "    ) \n",
    "\n",
    "retriever = vectordb.as_retriever()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2: needing to do that.  \n",
      "So if – well, suppose you want to evaluate  your hypothesis H at a certain point with a \n",
      "certain query point low K is X. Okay? A nd let’s say you want to know what’s the \n",
      "predicted value of Y at this position of X, right? So for lin ear regression, what we were \n",
      "doing was we wo\n",
      "2: regression problem like this. What I want to do today is talk about a class of algorithms \n",
      "called non-parametric learning algorithms that will help to alleviate the need somewhat \n",
      "for you to choose features very carefully. Okay ? And this leads us in to our discussion of \n",
      "locally weighted regression\n"
     ]
    }
   ],
   "source": [
    "querys = vectordb.similarity_search(\"What is Linear Regression\", k=2)\n",
    "for query in querys:\n",
    "    print(str(query.metadata[\"page\"]) + \":\", query.page_content[:300])"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Part 2 : Prompt & Model"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prompt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Tell me a funny joke about chickens.'"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain.prompts import PromptTemplate\n",
    "\n",
    "prompt_template = PromptTemplate.from_template(\n",
    "    \"Tell me a {adjective} joke about {content}.\"\n",
    ")\n",
    "prompt_template.format(adjective=\"funny\", content=\"chickens\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[SystemMessage(content='You are a helpful AI bot. Your name is Bob.'),\n",
       " HumanMessage(content='Hello, how are you doing?'),\n",
       " AIMessage(content=\"I'm doing well, thanks!\"),\n",
       " HumanMessage(content='What is your name?')]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain.prompts import ChatPromptTemplate\n",
    "\n",
    "chat_template = ChatPromptTemplate.from_messages(\n",
    "    [\n",
    "        (\"system\", \"You are a helpful AI bot. Your name is {name}.\"),\n",
    "        (\"human\", \"Hello, how are you doing?\"),\n",
    "        (\"ai\", \"I'm doing well, thanks!\"),\n",
    "        (\"human\", \"{user_input}\"),\n",
    "    ]\n",
    ")\n",
    "\n",
    "messages = chat_template.format_messages(name=\"Bob\", user_input=\"What is your name?\")\n",
    "messages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.chat_models import ChatOpenAI\n",
    "from langchain.prompts import HumanMessagePromptTemplate\n",
    "from langchain.schema.messages import SystemMessage\n",
    "\n",
    "chat_template = ChatPromptTemplate.from_messages(\n",
    "    [\n",
    "        SystemMessage(\n",
    "            content=(\n",
    "                \"You are a helpful assistant that re-writes the user's text to \"\n",
    "                \"sound more upbeat.\"\n",
    "            )\n",
    "        ),\n",
    "        HumanMessagePromptTemplate.from_template(\"{text}\"),\n",
    "    ]\n",
    ")\n",
    "\n",
    "# llm = ChatOpenAI()\n",
    "# llm(chat_template.format_messages(text=\"i dont like eating tasty things.\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.prompts import PromptTemplate\n",
    "\n",
    "prompt_template = \"\"\"\n",
    "    You are the chatbot and the face of Asian Institute of Technology (AIT). Your job is to give answers to prospective and current students about the school.\n",
    "    Your job is to answer questions only and only related to the AIT. Anything unrelated should be responded with the fact that your main job is solely to provide assistance regarding AIT.\n",
    "    MUST only use the following pieces of context to answer the question at the end. If the answers are not in the context or you are not sure of the answer, just say that you don't know, don't try to make up an answer.\n",
    "    {context}\n",
    "    Question: {question}\n",
    "    When encountering abusive, offensive, or harmful language, such as fuck, bitch, etc,  just politely ask the users to maintain appropriate behaviours.\n",
    "    Always make sure to elaborate your response and use vibrant, positive tone to represent good branding of the school.\n",
    "    Never answer with any unfinished response\n",
    "    Answer:\n",
    "    \"\"\"\n",
    "\n",
    "PROMPT = PromptTemplate(\n",
    "    template = prompt_template, \n",
    "    input_variables=[\"context\", \"question\"]\n",
    ")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM, AutoTokenizer, pipeline\n",
    "from transformers import BitsAndBytesConfig\n",
    "# from langchain import HuggingFacePipeline\n",
    "from langchain.llms.huggingface_pipeline import HuggingFacePipeline\n",
    "\n",
    "bitsandbyte_config = BitsAndBytesConfig(\n",
    "            load_in_4bit = True,\n",
    "            bnb_4bit_quant_type = \"nf4\",\n",
    "            bnb_4bit_compute_dtype = torch.float16,\n",
    "            bnb_4bit_use_double_quant = True\n",
    "        )\n",
    "\n",
    "model_id = \"lmsys/fastchat-t5-3b-v1.0\"\n",
    "# model_id = \"distilgpt2\"\n",
    "#For those who doesn't have GPU you can try with 'distilgpt2' instead\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(\n",
    "    model_id,\n",
    ")\n",
    "tokenizer.pad_token_id = tokenizer.eos_token_id\n",
    "\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    model_id,\n",
    "    quantization_config = bitsandbyte_config,\n",
    "    device_map = 'auto',\n",
    "    load_in_8bit=True,\n",
    ")\n",
    "\n",
    "pipe = pipeline(\n",
    "    task=\"text-generation\", \n",
    "    model=model, \n",
    "    tokenizer=tokenizer, \n",
    "    max_new_tokens=256, \n",
    "    model_kwargs = { \n",
    "        \"temperature\":0, \n",
    "        \"repetition_penalty\": 1.5\n",
    "    }, \n",
    ")\n",
    "\n",
    "llm = HuggingFacePipeline(pipeline=pipe)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Part 3 : Conversational Retrieval Chain\n",
    "\n",
    "The ConversationalRetrievalQA chain builds on RetrievalQAChain to provide a chat history component.\n",
    "- It first combines the `chat history` (either explicitly passed in or retrieved from the provided memory) \n",
    "- `the question into a standalone question` \n",
    "- looks up `relevant documents` from the retriever\n",
    "- finally `passes those documents` and the question to a question-answering chain to return a response."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This chain takes in chat history (a list of messages) and new questions,\n",
    "and then returns an answer to that question.\n",
    "\n",
    "The algorithm for this chain consists of three parts:\n",
    "\n",
    "1. Use the chat history and the new question to create a \"standalone question\".\n",
    "\n",
    "This is done so that this question can be passed into the retrieval step to fetch\n",
    "relevant documents. If only the new question was passed in, then relevant context\n",
    "may be lacking. If the whole conversation was passed into retrieval, there may\n",
    "be unnecessary information there that would distract from retrieval.\n",
    "\n",
    "2. This new question is passed to the retriever and relevant documents are\n",
    "returned.\n",
    "\n",
    "3. The retrieved documents are passed to an LLM along with either the new question\n",
    "(default behavior) or the original question and chat history to generate a final\n",
    "response.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.chains import LLMChain\n",
    "from langchain.chains.conversational_retrieval.prompts import CONDENSE_QUESTION_PROMPT\n",
    "\n",
    "from langchain.chains.question_answering import load_qa_chain\n",
    "\n",
    "from langchain.memory import ConversationBufferWindowMemory\n",
    "\n",
    "doc_chain = load_qa_chain(\n",
    "    llm, \n",
    "    chain_type=\"stuff\", \n",
    "    prompt = PROMPT)\n",
    "\n",
    "question_generator = LLMChain(\n",
    "    llm=llm, \n",
    "    prompt=CONDENSE_QUESTION_PROMPT)\n",
    "\n",
    "memory = ConversationBufferWindowMemory(\n",
    "    k = 3,  \n",
    "    memory_key = \"chat_history\", \n",
    "    return_messages = True,  \n",
    "    output_key = 'answer')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Class ConversationalRetrievalChain\n",
    "\n",
    "`retriever` : Retriever to use to fetch documents.\n",
    "\n",
    "`combine_docs_chain` : The chain used to combine any retrieved documents.\n",
    "\n",
    "`question_generator`: The chain used to generate a new question for the sake of retrieval. This chain will take in the current question (with variable question) and any chat history (with variable chat_history) and will produce a new standalone question to be used later on.\n",
    "\n",
    "`return_source_documents` : Return the retrieved source documents as part of the final result.\n",
    "\n",
    "`get_chat_history` : An optional function to get a string of the chat history. If None is provided, will use a default.\n",
    "\n",
    "`return_generated_question` : Return the generated question as part of the final result.\n",
    "\n",
    "`response_if_no_docs_found` : If specified, the chain will return a fixed response if no docs are found for the question.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.chains import ConversationalRetrievalChain\n",
    "    \n",
    "chain = ConversationalRetrievalChain(\n",
    "    retriever = retriever,\n",
    "    question_generator = question_generator,\n",
    "    combine_docs_chain = doc_chain,\n",
    "    return_source_documents = True,\n",
    "    memory = memory,\n",
    "    get_chat_history = lambda h :h)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "langchain.schema.vectorstore.VectorStoreRetriever.Config"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chain.retriever.Config"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## QA-Bot\n",
    "\n",
    "`pip3 install --upgrade --no-deps --force-reinstall --pre torch torchvision torchaudio --index-url https://download.pytorch.org/whl/nightly/cpu`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt_question = 'Who are you by the way?'\n",
    "\n",
    "answer = chain(\n",
    "    {\"question\": prompt_question})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('Who are you by the way?',\n",
       " 'I am the chatbot for the Asian Institute of Technology (AIT). My main job is to provide assistance and answer questions related to AIT. How can I assist you today?',\n",
       " [Document(page_content=\"and if they're other questions, I'll take them after.  \\n[End of Audio]  \\nDuration: 79 minutes\", metadata={'source': './docs/pdf/MachineLearning-Lecture02.pdf', 'page': 17}),\n",
       "  Document(page_content=\"touches many areas of science and industrie s, and I'm just kind of curious. How many \\npeople here are computer science majors, are in the computer science department? Okay. \\nAbout half of you. How many people are from  EE? Oh, okay, maybe about a fifth. How\", metadata={'source': './docs/pdf/MachineLearning-Lecture01.pdf', 'page': 0}),\n",
       "  Document(page_content=\"there. Actually, [inaudible] I think this room just got a new projector that — someone \\nsent you an excited email — was it just on Frid ay? — saying we just got a new projector \\nand they said 4,000-to-1 something or othe r brightness ratio. I don't know. Someone was \\nvery excited about the new projector in this room, but I guess we'll see that in operation \\non Wednesday.  \\nSo start by talking about what machine learni ng is. What is machine learning? Actually, \\ncan you read the text out there? Raise your hand if the text on the small screens is legible. \\nOh, okay, cool, mostly legible. Okay. So I'll just read it out.\", metadata={'source': './docs/pdf/MachineLearning-Lecture01.pdf', 'page': 10}),\n",
       "  Document(page_content='highly interdisciplinary topic in which just the TAs find l earning algorithms to problems \\nin computer vision and biology and robots a nd language. And machine learning is one of \\nthose things that has and is having a large impact on many applications.  \\nSo just in my own daily work, I actually frequently end up talking to people like \\nhelicopter pilots to biologists to people in  computer systems or databases to economists \\nand sort of also an unending stream of  people from industry coming to Stanford \\ninterested in applying machine learni ng methods to their own problems.  \\nSo yeah, this is fun. A couple of weeks ago, a student actually forwar ded to me an article', metadata={'source': './docs/pdf/MachineLearning-Lecture01.pdf', 'page': 0})])"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "answer['question'], answer['answer'], answer['source_documents']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt_question = 'What is NLP?'\n",
    "\n",
    "answer = chain(\n",
    "    {\"question\": prompt_question})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('What is NLP?',\n",
       " 'NLP stands for Natural Language Processing. It is a field of study in artificial intelligence that focuses on the interaction between computers and human language. It involves tasks such as text analysis, machine translation, sentiment analysis, and speech recognition. If you have any specific questions about NLP at AIT, feel free to ask!',\n",
       " [Document(page_content=\"these medical records, which started to be digitized only about maybe 15 years, applying \\nlearning algorithms to them can turn raw medi cal records into what I might loosely call \\nmedical knowledge in which we start to detect trends in medical practice and even start to \\nalter medical practice as a result of me dical knowledge that's derived by applying \\nlearning algorithms to the sorts of medical r ecords that hospitals have just been building \\nover the last 15, 20 years in an electronic format.  \\nTurns out that most of you probably use learning algorithms — I don't know — I think \\nhalf a dozen times a day or maybe a dozen  times a day or more, and often without\", metadata={'source': './docs/pdf/MachineLearning-Lecture01.pdf', 'page': 2}),\n",
       "  Document(page_content='fit to the data. Okay? So in linear regression we  have a fix set of parameters theta, right? \\nThat must fit to the data. In contrast, what  I’m gonna talk about now is our first non-\\nparametric learning algorithm. The formal defi nition, which is not very  intuitive, so I’ve \\nreplaced it with a second, say, more intuitive. The, sort of, formal definition of the non-\\nparametric learning algorithm is that it’s an  algorithm where the number of parameters \\ngoes with M, with the size of the training se t. And usually it’s de fined as a number of \\nparameters grows linearly with the size of the training set. Th is is the formal definition. A', metadata={'source': './docs/pdf/MachineLearning-Lecture03.pdf', 'page': 2}),\n",
       "  Document(page_content='probably favorite machine learning algorithm.  \\nWe’ll then talk about a probabl e second interpretation of linear regression and use that to \\nmove onto our first classification algorithm, which is logistic regr ession; take a brief \\ndigression to tell you about something cal led the perceptron algorithm, which is \\nsomething we’ll come back to, again, later this  quarter; and time allowing I hope to get to \\nNewton’s method, which is an algorithm fo r fitting logistic regression models.  \\nSo this is recap where we’re talking about in the previous lecture, remember the notation \\nI defined was that I used this X superscrip t I, Y superscript I to denote the I training', metadata={'source': './docs/pdf/MachineLearning-Lecture03.pdf', 'page': 0}),\n",
       "  Document(page_content=\"there. Actually, [inaudible] I think this room just got a new projector that — someone \\nsent you an excited email — was it just on Frid ay? — saying we just got a new projector \\nand they said 4,000-to-1 something or othe r brightness ratio. I don't know. Someone was \\nvery excited about the new projector in this room, but I guess we'll see that in operation \\non Wednesday.  \\nSo start by talking about what machine learni ng is. What is machine learning? Actually, \\ncan you read the text out there? Raise your hand if the text on the small screens is legible. \\nOh, okay, cool, mostly legible. Okay. So I'll just read it out.\", metadata={'source': './docs/pdf/MachineLearning-Lecture01.pdf', 'page': 10})])"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "answer['question'], answer['answer'], answer['source_documents']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Retrying langchain.chat_models.openai.ChatOpenAI.completion_with_retry.<locals>._completion_with_retry in 4.0 seconds as it raised RateLimitError: Rate limit reached for gpt-3.5-turbo in organization org-Ds6D2AMs3kLcrGzbiTbp1Ifs on requests per min (RPM): Limit 3, Used 3, Requested 1. Please try again in 20s. Visit https://platform.openai.com/account/rate-limits to learn more. You can increase your rate limit by adding a payment method to your account at https://platform.openai.com/account/billing..\n",
      "Retrying langchain.chat_models.openai.ChatOpenAI.completion_with_retry.<locals>._completion_with_retry in 4.0 seconds as it raised RateLimitError: Rate limit reached for gpt-3.5-turbo in organization org-Ds6D2AMs3kLcrGzbiTbp1Ifs on requests per min (RPM): Limit 3, Used 3, Requested 1. Please try again in 20s. Visit https://platform.openai.com/account/rate-limits to learn more. You can increase your rate limit by adding a payment method to your account at https://platform.openai.com/account/billing..\n",
      "Retrying langchain.chat_models.openai.ChatOpenAI.completion_with_retry.<locals>._completion_with_retry in 4.0 seconds as it raised RateLimitError: Rate limit reached for gpt-3.5-turbo in organization org-Ds6D2AMs3kLcrGzbiTbp1Ifs on requests per min (RPM): Limit 3, Used 3, Requested 1. Please try again in 20s. Visit https://platform.openai.com/account/rate-limits to learn more. You can increase your rate limit by adding a payment method to your account at https://platform.openai.com/account/billing..\n"
     ]
    }
   ],
   "source": [
    "prompt_question = 'How it is different comparing with NLU?'\n",
    "\n",
    "answer = chain(\n",
    "    {\"question\": prompt_question})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('How it is different comparing with NLU?',\n",
       " 'As the chatbot for Asian Institute of Technology (AIT), I am here to provide assistance regarding AIT only. I do not have information about NLP or NLU. If you have any questions specifically related to AIT, I would be happy to help you with those.',\n",
       " [Document(page_content='than least squares regression and logistic regr ession, and, in particular, because it outputs \\nonly values are either zero or one it turns out  it’s very difficult to  endow this algorithm \\nwith probabilistic semantics. And this is , again, even though – oh, excuse me. Right \\nthere. Okay. And even though this learning ru le looks, again, looks  cosmetically very \\nsimilar to what we have in l ogistics regression this is actual ly a very different type of \\nlearning rule than the others that were seen in this class. So because this is such a simple \\nlearning algorithm, right? It ju st computes theta transpose X and then you threshold and', metadata={'source': './docs/pdf/MachineLearning-Lecture03.pdf', 'page': 15}),\n",
       "  Document(page_content='fit to the data. Okay? So in linear regression we  have a fix set of parameters theta, right? \\nThat must fit to the data. In contrast, what  I’m gonna talk about now is our first non-\\nparametric learning algorithm. The formal defi nition, which is not very  intuitive, so I’ve \\nreplaced it with a second, say, more intuitive. The, sort of, formal definition of the non-\\nparametric learning algorithm is that it’s an  algorithm where the number of parameters \\ngoes with M, with the size of the training se t. And usually it’s de fined as a number of \\nparameters grows linearly with the size of the training set. Th is is the formal definition. A', metadata={'source': './docs/pdf/MachineLearning-Lecture03.pdf', 'page': 2}),\n",
       "  Document(page_content='really the same learning algorithm?  \\nStudent: [Inaudible] constants?  \\nInstructor (Andrew Ng) :Say that again.  \\nStudent: [Inaudible]  \\nInstructor (Andrew Ng) :Oh, right. Okay, cool.', metadata={'source': './docs/pdf/MachineLearning-Lecture03.pdf', 'page': 13}),\n",
       "  Document(page_content='narrow Gaussian – excuse me, a fairly narrow be ll shape, so that the weights of the points \\nare far away fall off rapidly. Whereas if tow is large then you’d end up choosing a \\nweighting function that falls of relatively sl owly with distance from your query. Okay?  \\nSo I hope you can, therefore, see that if you a pply locally weighted li near regression to a \\ndata set that looks like this, th en to ask what your hypothesis out put is at a point like this \\nyou end up having a straight line making that pr ediction. To ask what kind of class this \\n[inaudible] at that value you put  a straight line there and you pr edict that value. It turns', metadata={'source': './docs/pdf/MachineLearning-Lecture03.pdf', 'page': 4})])"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "answer['question'], answer['answer'], answer['source_documents']"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Github Recommendation\n",
    "https://github.com/curiousily/Get-Things-Done-with-Prompt-Engineering-and-LangChain\n",
    "https://github.com/ksm26/LangChain-Chat-with-Your-Data"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
