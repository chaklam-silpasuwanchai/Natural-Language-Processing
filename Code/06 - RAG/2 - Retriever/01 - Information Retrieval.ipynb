{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Information Retrieval"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## TF-IDF \n",
    "It is a method of information retrieval that is used to rank the importance of words in a document.\n",
    "It is based on the idea that words that appear in a document more often are more relevant to the document.\n",
    "\n",
    "\n",
    "**Terminology :**\n",
    "- t — term (word)\n",
    "- d — document (set of words)\n",
    "- N — count of corpus\n",
    "- corpus — the total document set\n",
    "\n",
    "**Term Frequency (TF) :** Suppose we have a set of English text documents and wish to rank which document is most relevant to the query , “Data Science is awesome !” A simple way to start out is by eliminating documents that do not contain all three words “Data”,”is”, “Science”, and “awesome”, but this still leaves many documents. To further distinguish them, we might count the number of times each term occurs in each document; the number of times a term occurs in a document is called its term frequency.\n",
    "\n",
    "tf(t, d) = (Number of times term t appears in document d) / (Total number of terms in document d)\n",
    "\n",
    "\n",
    "**Document Frequency :**\n",
    "This measures the importance of document in whole set of corpus, this is very similar to TF. The only difference is that TF is frequency counter for a term t in document d, where as DF is the count of occurrences of term t in the document set N. In other words, DF is the number of documents in which the word is present. We consider one occurrence if the term consists in the document at least once, we do not need to know the number of times the term is present.\n",
    "\n",
    "df(t) = occurrence of t in N documents\n",
    "\n",
    "**Inverse Document Frequency(IDF):**\n",
    "While computing TF, all terms are considered equally important. However it is known that certain terms, such as “is”, “of”, and “that”, may appear a lot of times but have little importance. Thus we need to weigh down the frequent terms while scale up the rare ones, by computing IDF, an inverse document frequency factor is incorporated which diminishes the weight of terms that occur very frequently in the document set and increases the weight of terms that occur rarely. IDF is the inverse of the document frequency which measures the informativeness of term t. When we calculate IDF, it will be very low for the most occurring words such as stop words (because stop words such as “is” is present in almost all of the documents, and N/df will give a very low value to that word). This finally gives what we want, a relative weightage.\n",
    "\n",
    "idf(t) = N/df\n",
    "\n",
    "Now there are few other problems with the IDF , in case of a large corpus,say 100,000,000 , the IDF value explodes , to avoid the effect we take the log of idf . During the query time, when a word which is not in vocab occurs, the df will be 0. As we cannot divide by 0, we smoothen the value by adding 1 to the denominator. that’s the final formula:\n",
    "\n",
    "idf(t) = log(N/(df + 1))\n",
    "\n",
    "**TF-IDF** is a the right measure to evaluate how important a word is to a document in a collection or corpus. here are many different variations of TF-IDF but for now let us concentrate on the this basic version.\n",
    "\n",
    "tf-idf(t, d) = tf(t, d) * log(N/(df + 1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "#importing the libraries\n",
    "import numpy as np\n",
    "from nltk.tokenize import word_tokenize\n",
    "\n",
    "#sample to consider for TF-IDF\n",
    "sample_text = ['Topic sentences are similar to mini thesis statements.'\n",
    "               'Like a thesis statement, a topic sentence has a specific '\n",
    "               'main point. Whereas the thesis is the main point of the essay',\n",
    "               'the topic sentence is the main point of the paragraph.'\n",
    "               'Like the thesis statement, a topic sentence has a unifying function. '\n",
    "               'But a thesis statement or topic sentence alone doesn’t guarantee unity.',\n",
    "               'An essay is unified if all the paragraphs relate to the thesis,'\n",
    "               'whereas a paragraph is unified if all the sentences relate to the topic sentence.']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total documents:  3\n",
      "Total words:  35\n"
     ]
    }
   ],
   "source": [
    "#tokenizing the sample text and creating unique set of words\n",
    "sentences = []\n",
    "word_set = []\n",
    "\n",
    "for sent in sample_text:\n",
    "    words = [word.lower() for word in word_tokenize(sent) if word.isalpha()]\n",
    "    sentences.append(words)\n",
    "    for word in words:\n",
    "        if word not in word_set:\n",
    "            word_set.append(word)\n",
    "            \n",
    "# Set of words\n",
    "word_set = set(word_set)\n",
    "# total documents in our corpus\n",
    "total_docs = len(sample_text)\n",
    "print('Total documents: ', total_docs)\n",
    "print('Total words: ', len(word_set))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'similar': 0, 'guarantee': 1, 'unity': 2, 'or': 3, 'all': 4, 'are': 5, 'sentence': 6, 'has': 7, 'point': 8, 'doesn': 9, 'paragraphs': 10, 'the': 11, 'is': 12, 'an': 13, 'thesis': 14, 'statement': 15, 'specific': 16, 'whereas': 17, 'topic': 18, 'main': 19, 'of': 20, 'essay': 21, 'unifying': 22, 't': 23, 'if': 24, 'a': 25, 'but': 26, 'to': 27, 'sentences': 28, 'relate': 29, 'mini': 30, 'function': 31, 'alone': 32, 'paragraph': 33, 'unified': 34}\n"
     ]
    }
   ],
   "source": [
    "#Creating index for each word from vocabulary\n",
    "word_index = {}\n",
    "for i, word in enumerate(word_set):\n",
    "    word_index[word] = i\n",
    "\n",
    "print(word_index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'similar': 1, 'guarantee': 1, 'unity': 1, 'or': 1, 'all': 2, 'are': 1, 'sentence': 5, 'has': 2, 'point': 3, 'doesn': 1, 'paragraphs': 1, 'the': 11, 'is': 4, 'an': 1, 'thesis': 6, 'statement': 3, 'specific': 1, 'whereas': 2, 'topic': 6, 'main': 3, 'of': 2, 'essay': 2, 'unifying': 1, 't': 1, 'if': 2, 'a': 7, 'but': 1, 'to': 3, 'sentences': 2, 'relate': 2, 'mini': 1, 'function': 1, 'alone': 1, 'paragraph': 1, 'unified': 2}\n"
     ]
    }
   ],
   "source": [
    "#Creating a dictionary for keeping count\n",
    "def count_dict(sentences):\n",
    "    count_dict = {}\n",
    "    for word in word_set:\n",
    "        count_dict[word] = 0\n",
    "    for sent in sentences:\n",
    "        for word in sent:\n",
    "            count_dict[word] += 1\n",
    "    return count_dict\n",
    "    \n",
    "word_count = count_dict(sentences)\n",
    "print(word_count)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "def term_frequency(document, word):\n",
    "    N = len(document)\n",
    "    occurance = len([token for token in document if token == word])\n",
    "    return occurance / N"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "def inverse_document_frequency(word):\n",
    "    try:\n",
    "        word_occurance = word_count[word] + 1\n",
    "    except:\n",
    "        word_occurance = 1\n",
    "    return np.log(total_docs / word_occurance)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tf_idf(sentence):\n",
    "    vec = np.zeros((len(word_set),))\n",
    "    for word in sentence:\n",
    "        tf = term_frequency(sentence, word)\n",
    "        idf = inverse_document_frequency(word)\n",
    "        vec[word_index[word]] = tf * idf\n",
    "    return vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>similar</th>\n",
       "      <th>guarantee</th>\n",
       "      <th>unity</th>\n",
       "      <th>or</th>\n",
       "      <th>all</th>\n",
       "      <th>are</th>\n",
       "      <th>sentence</th>\n",
       "      <th>has</th>\n",
       "      <th>point</th>\n",
       "      <th>doesn</th>\n",
       "      <th>...</th>\n",
       "      <th>a</th>\n",
       "      <th>but</th>\n",
       "      <th>to</th>\n",
       "      <th>sentences</th>\n",
       "      <th>relate</th>\n",
       "      <th>mini</th>\n",
       "      <th>function</th>\n",
       "      <th>alone</th>\n",
       "      <th>paragraph</th>\n",
       "      <th>unified</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.014481</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.014481</td>\n",
       "      <td>-0.024755</td>\n",
       "      <td>0.0</td>\n",
       "      <td>-0.020549</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.105089</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>-0.010274</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.014481</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.01308</td>\n",
       "      <td>0.01308</td>\n",
       "      <td>0.01308</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>-0.067079</td>\n",
       "      <td>0.0</td>\n",
       "      <td>-0.009280</td>\n",
       "      <td>0.01308</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.094919</td>\n",
       "      <td>0.01308</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.01308</td>\n",
       "      <td>0.01308</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>-0.026660</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.037724</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>-0.022129</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.015595</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>3 rows × 35 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "    similar  guarantee    unity       or  all       are  sentence  has  \\\n",
       "0  0.014481    0.00000  0.00000  0.00000  0.0  0.014481 -0.024755  0.0   \n",
       "1  0.000000    0.01308  0.01308  0.01308  0.0  0.000000 -0.067079  0.0   \n",
       "2  0.000000    0.00000  0.00000  0.00000  0.0  0.000000 -0.026660  0.0   \n",
       "\n",
       "      point    doesn  ...         a      but        to  sentences  relate  \\\n",
       "0 -0.020549  0.00000  ... -0.105089  0.00000 -0.010274        0.0     0.0   \n",
       "1 -0.009280  0.01308  ... -0.094919  0.01308  0.000000        0.0     0.0   \n",
       "2  0.000000  0.00000  ... -0.037724  0.00000 -0.022129        0.0     0.0   \n",
       "\n",
       "       mini  function    alone  paragraph  unified  \n",
       "0  0.014481   0.00000  0.00000   0.000000      0.0  \n",
       "1  0.000000   0.01308  0.01308   0.000000      0.0  \n",
       "2  0.000000   0.00000  0.00000   0.015595      0.0  \n",
       "\n",
       "[3 rows x 35 columns]"
      ]
     },
     "execution_count": 138,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vectors = []\n",
    "for sent in sentences:\n",
    "    vectors.append(tf_idf(sent))\n",
    "\n",
    "# Create a DataFrame\n",
    "df = pd.DataFrame(vectors, columns=word_count.keys())\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cosine Similarity A-B: 0.9338\n",
      "Cosine Similarity A-C: 0.8415\n",
      "Cosine Similarity B-C: 0.8845\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "def cosine_similarity(a, b):\n",
    "    dot_product = np.dot(a, b)\n",
    "    norm_a = np.linalg.norm(a)\n",
    "    norm_b = np.linalg.norm(b)\n",
    "    \n",
    "    similarity = dot_product / (norm_a * norm_b)\n",
    "    return round(similarity, 4)\n",
    "\n",
    "# Example usage:\n",
    "similarity_score = cosine_similarity(vectors[0], vectors[1])\n",
    "print(\"Cosine Similarity A-B:\", similarity_score)\n",
    "similarity_score = cosine_similarity(vectors[0], vectors[2])\n",
    "print(\"Cosine Similarity A-C:\", similarity_score)\n",
    "similarity_score = cosine_similarity(vectors[1], vectors[2])\n",
    "print(\"Cosine Similarity B-C:\", similarity_score)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Reference\n",
    "- https://towardsdatascience.com/tf-idf-for-document-ranking-from-scratch-in-python-on-real-world-dataset-796d339a4089\n",
    "- https://www.kaggle.com/code/yassinehamdaoui1/creating-tf-idf-model-from-scratch\n",
    "- https://github.com/ashushekar/TF-IDF-Model/blob/master/tfidf.py\n",
    "- https://medium.com/@ashwinnaidu1991/creating-a-tf-idf-model-from-scratch-in-python-71047f16494e"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## [BM 25 (BM is an abbreviation of best matching)](https://www.cs.otago.ac.nz/homepages/andrew/papers/2014-2.pdf)\n",
    "It is  a term-based ranking model that aims to provide accurate and relevant search results by scoring documents based on their term frequencies and document lengths. This essay explores the fundamental concepts and working principles of the BM25 ranking algorithm.\n",
    "\n",
    "$\\text{BM25}(D, Q) = \\sum_{i=1}^{n} \\text{IDF}(q) \\cdot \\frac{{\\text{TF}(q, D) \\cdot (k_1 + 1)}}{{\\text{TF}(q, D) + k_1 \\cdot \\left(1 - b + b \\cdot \\frac{{\\lvert D \\rvert}}{{\\text{avgdl}}}\\right)}}$\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "a = \"purple is the best city in the forest\".split()\n",
    "b = \"there is an art to getting your way and throwing bananas on to the street is not it\".split()\n",
    "c = \"it is not often you find soggy bananas on the street\".split()\n",
    "d = \"green should have smelled more tranquil but somehow it just tasted rotten\".split()\n",
    "e = \"joyce enjoyed eating pancakes with ketchup\".split()\n",
    "f = \"as the asteroid hurtled toward earth becky was upset her dentist appointment had been canceled\".split()\n",
    "docs = [a, b, c, d, e, f]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "avgdl = sum(len(sentence) for sentence in [a,b,c,d,e,f]) / len(docs)\n",
    "N = len(docs)\n",
    "\n",
    "def bm25(word, sentence, k=1.2, b=0.75):\n",
    "    freq = sentence.count(word)  # or f(q,D) - freq of query in Doc\n",
    "    tf = (freq * (k + 1)) / (freq + k * (1 - b + b * (len(sentence) / avgdl)))\n",
    "    N_q = sum([doc.count(word) for doc in docs])  # number of docs that contain the word\n",
    "    idf = np.log(((N - N_q + 0.5) / (N_q + 0.5)) + 1)\n",
    "    return round(tf*idf, 4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1.7677, 0.0, 0.8425, 1.0543)"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bm25('purple', a), bm25('purple', b), bm25('bananas', b), bm25('bananas', c)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Using Okapi BM25"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ! pip3 install rank_bm25"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from rank_bm25 import BM25Okapi\n",
    "\n",
    "corpus = [\n",
    "    \"Hello there good man!\",\n",
    "    \"It is quite windy in London\",\n",
    "    \"How is the weather today?\"\n",
    "]\n",
    "\n",
    "tokenized_corpus = [doc.split(\" \") for doc in corpus]\n",
    "\n",
    "bm25 = BM25Okapi(tokenized_corpus)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Ranking of documents**\n",
    "Now that we've created our document indexes, we can give it queries and see which documents are the most relevant:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.        , 0.93729472, 0.        ])"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "query = \"windy London\"\n",
    "tokenized_query = query.split(\" \")\n",
    "\n",
    "doc_scores = bm25.get_scores(tokenized_query)\n",
    "doc_scores\n",
    "# array([0.        , 0.93729472, 0.        ])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['It is quite windy in London',\n",
       " 'How is the weather today?',\n",
       " 'Hello there good man!']"
      ]
     },
     "execution_count": 113,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "docs = bm25.get_top_n(tokenized_query, corpus, n=3)\n",
    "docs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Reference\n",
    "- https://medium.com/@evertongomede/understanding-the-bm25-ranking-algorithm-19f6d45c6ce\n",
    "- https://www.analyticsvidhya.com/blog/2021/05/build-your-own-nlp-based-search-engine-using-bm25/\n",
    "- https://abishek21.medium.com/building-your-favourite-tv-series-search-engine-information-retrieval-using-bm25-ranking-8e8c54bcdb38"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Sentence-BERT\n",
    "\n",
    "[click here](https://github.com/chaklam-silpasuwanchai/Python-fo-Natural-Language-Processing/blob/main/Code/02%20-%20DL/case-studies/SentenceBert/S-BERT.ipynb)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Appendix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TF-IDF a: 0.48\n",
      "TF-IDF b: 0.0\n",
      "TF-IDF c: 0.0\n"
     ]
    }
   ],
   "source": [
    "a = \"purple is the best city in the forest\".split()\n",
    "b = \"there is an art to getting your way and throwing bananas on to the street is not it\".split()\n",
    "c = \"it is not often you find soggy bananas on the street\".split()\n",
    "\n",
    "import numpy as np\n",
    "def tfidf(word):\n",
    "    tf = []\n",
    "    count_n = 0\n",
    "    for sentence in [a, b, c]:\n",
    "        # calculate TF\n",
    "        t_count = len([x for x in sentence if word in sentence])\n",
    "        tf.append(t_count/len(sentence))\n",
    "        # count number of docs for IDF\n",
    "        count_n += 1 if word in sentence else 0\n",
    "    idf = np.log10(len([a, b, c]) / count_n)\n",
    "    return [round(_tf*idf, 2) for _tf in tf]\n",
    "\n",
    "tfidf_a, tfidf_b, tfidf_c = tfidf('forest')\n",
    "print(f\"TF-IDF a: {tfidf_a}\\nTF-IDF b: {tfidf_b}\\nTF-IDF c: {tfidf_c}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'the', 'is', 'of', 'Data', '21st', 'for', 'science', 'data', 'century', 'Science', 'machine', 'key', 'job', 'sexiest', 'learning'}\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>the</th>\n",
       "      <th>is</th>\n",
       "      <th>of</th>\n",
       "      <th>Data</th>\n",
       "      <th>21st</th>\n",
       "      <th>for</th>\n",
       "      <th>science</th>\n",
       "      <th>data</th>\n",
       "      <th>century</th>\n",
       "      <th>Science</th>\n",
       "      <th>machine</th>\n",
       "      <th>key</th>\n",
       "      <th>job</th>\n",
       "      <th>sexiest</th>\n",
       "      <th>learning</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   the  is  of  Data  21st  for  science  data  century  Science  machine  \\\n",
       "0    2   1   1     1     1    0        0     0        1        1        0   \n",
       "1    1   1   0     0     0    1        1     1        0        0        1   \n",
       "\n",
       "   key  job  sexiest  learning  \n",
       "0    0    1        1         0  \n",
       "1    1    0        0         1  "
      ]
     },
     "execution_count": 107,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "first_sentence = \"Data Science is the sexiest job of the 21st century\".split()\n",
    "second_sentence = \"machine learning is the key for data science\".split()\n",
    "total= set(first_sentence).union(set(second_sentence))\n",
    "print(total)\n",
    "\n",
    "wordDictA = dict.fromkeys(total, 0) \n",
    "wordDictB = dict.fromkeys(total, 0)\n",
    "for word in first_sentence:\n",
    "    wordDictA[word]+=1\n",
    "    \n",
    "for word in second_sentence:\n",
    "    wordDictB[word]+=1\n",
    "\n",
    "occurence = pd.DataFrame([wordDictA, wordDictB])\n",
    "occurence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>the</th>\n",
       "      <th>is</th>\n",
       "      <th>of</th>\n",
       "      <th>Data</th>\n",
       "      <th>21st</th>\n",
       "      <th>for</th>\n",
       "      <th>science</th>\n",
       "      <th>data</th>\n",
       "      <th>century</th>\n",
       "      <th>Science</th>\n",
       "      <th>machine</th>\n",
       "      <th>key</th>\n",
       "      <th>job</th>\n",
       "      <th>sexiest</th>\n",
       "      <th>learning</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.200</td>\n",
       "      <td>0.100</td>\n",
       "      <td>0.1</td>\n",
       "      <td>0.1</td>\n",
       "      <td>0.1</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.1</td>\n",
       "      <td>0.1</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.1</td>\n",
       "      <td>0.1</td>\n",
       "      <td>0.000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.125</td>\n",
       "      <td>0.125</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.125</td>\n",
       "      <td>0.125</td>\n",
       "      <td>0.125</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.125</td>\n",
       "      <td>0.125</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.125</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     the     is   of  Data  21st    for  science   data  century  Science  \\\n",
       "0  0.200  0.100  0.1   0.1   0.1  0.000    0.000  0.000      0.1      0.1   \n",
       "1  0.125  0.125  0.0   0.0   0.0  0.125    0.125  0.125      0.0      0.0   \n",
       "\n",
       "   machine    key  job  sexiest  learning  \n",
       "0    0.000  0.000  0.1      0.1     0.000  \n",
       "1    0.125  0.125  0.0      0.0     0.125  "
      ]
     },
     "execution_count": 108,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def tf(wordDict, doc):\n",
    "    tfDict = {}\n",
    "    corpusCount = len(doc)\n",
    "    for word, count in wordDict.items():\n",
    "        tfDict[word] = count/float(corpusCount)\n",
    "    return(tfDict)\n",
    "    \n",
    "#running our sentences through the tf function:\n",
    "tfFirst = tf(wordDictA, first_sentence)\n",
    "tfSecond = tf(wordDictB, second_sentence)\n",
    "\n",
    "#Converting to dataframe for visualization\n",
    "tf = pd.DataFrame([tfFirst, tfSecond])\n",
    "tf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     /home/todsavadt/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['Data',\n",
       " '21st',\n",
       " 'science',\n",
       " 'data',\n",
       " 'century',\n",
       " 'Science',\n",
       " 'machine',\n",
       " 'key',\n",
       " 'job',\n",
       " 'sexiest',\n",
       " 'learning']"
      ]
     },
     "execution_count": 109,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import nltk\n",
    "nltk.download('stopwords')\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "set(stopwords.words('english'))\n",
    "\n",
    "filtered_sentence = []\n",
    "for word in wordDictA:\n",
    "    if str(word) not in set(stopwords.words('english')):\n",
    "        filtered_sentence.append(word)\n",
    "\n",
    "filtered_sentence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'the': 0.3010299956639812,\n",
       " 'is': 0.3010299956639812,\n",
       " 'of': 0.3010299956639812,\n",
       " 'Data': 0.3010299956639812,\n",
       " '21st': 0.3010299956639812,\n",
       " 'for': 0.3010299956639812,\n",
       " 'science': 0.3010299956639812,\n",
       " 'data': 0.3010299956639812,\n",
       " 'century': 0.3010299956639812,\n",
       " 'Science': 0.3010299956639812,\n",
       " 'machine': 0.3010299956639812,\n",
       " 'key': 0.3010299956639812,\n",
       " 'job': 0.3010299956639812,\n",
       " 'sexiest': 0.3010299956639812,\n",
       " 'learning': 0.3010299956639812}"
      ]
     },
     "execution_count": 110,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import math\n",
    "\n",
    "def IDF(docList):\n",
    "    idfDict = {}\n",
    "    N = len(docList)\n",
    "    \n",
    "    idfDict = dict.fromkeys(docList[0].keys(), 0)\n",
    "    for word, val in idfDict.items():\n",
    "        idfDict[word] = math.log10(N / (float(val) + 1))\n",
    "        \n",
    "    return(idfDict)\n",
    "    \n",
    "#inputing our sentences in the log file\n",
    "idfs = IDF([wordDictA, wordDictB])\n",
    "idfs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>the</th>\n",
       "      <th>is</th>\n",
       "      <th>of</th>\n",
       "      <th>Data</th>\n",
       "      <th>21st</th>\n",
       "      <th>for</th>\n",
       "      <th>science</th>\n",
       "      <th>data</th>\n",
       "      <th>century</th>\n",
       "      <th>Science</th>\n",
       "      <th>machine</th>\n",
       "      <th>key</th>\n",
       "      <th>job</th>\n",
       "      <th>sexiest</th>\n",
       "      <th>learning</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.060206</td>\n",
       "      <td>0.030103</td>\n",
       "      <td>0.030103</td>\n",
       "      <td>0.030103</td>\n",
       "      <td>0.030103</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.030103</td>\n",
       "      <td>0.030103</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.030103</td>\n",
       "      <td>0.030103</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.037629</td>\n",
       "      <td>0.037629</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.037629</td>\n",
       "      <td>0.037629</td>\n",
       "      <td>0.037629</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.037629</td>\n",
       "      <td>0.037629</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.037629</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        the        is        of      Data      21st       for   science  \\\n",
       "0  0.060206  0.030103  0.030103  0.030103  0.030103  0.000000  0.000000   \n",
       "1  0.037629  0.037629  0.000000  0.000000  0.000000  0.037629  0.037629   \n",
       "\n",
       "       data   century   Science   machine       key       job   sexiest  \\\n",
       "0  0.000000  0.030103  0.030103  0.000000  0.000000  0.030103  0.030103   \n",
       "1  0.037629  0.000000  0.000000  0.037629  0.037629  0.000000  0.000000   \n",
       "\n",
       "   learning  \n",
       "0  0.000000  \n",
       "1  0.037629  "
      ]
     },
     "execution_count": 92,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def TF_IDF(tfBow, idfs):\n",
    "    tfidf = {}\n",
    "    for word, val in tfBow.items():\n",
    "        tfidf[word] = val*idfs[word]\n",
    "    return(tfidf)\n",
    "#running our two sentences through the IDF:\n",
    "idfFirst = TF_IDF(tfFirst, idfs)\n",
    "idfSecond = TF_IDF(tfSecond, idfs)\n",
    "\n",
    "#putting it in a dataframe\n",
    "idf = pd.DataFrame([idfFirst, idfSecond])\n",
    "idf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cosine Similarity A-B: 0.3062\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "def cosine_similarity(a, b):\n",
    "    dot_product = np.dot(a, b)\n",
    "    norm_a = np.linalg.norm(a)\n",
    "    norm_b = np.linalg.norm(b)\n",
    "    \n",
    "    similarity = dot_product / (norm_a * norm_b)\n",
    "    return round(similarity, 4)\n",
    "\n",
    "# Example usage:\n",
    "similarity_score = cosine_similarity(idf.iloc[0], idf.iloc[1])\n",
    "print(\"Cosine Similarity A-B:\", similarity_score)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Using TfidfVectorizer from Sklearn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cosine Similarity between the two sentences: 0.3528003588287327\n"
     ]
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "\n",
    "# Your sentences\n",
    "firstV = \"Data Science is the sexiest job of the 21st century\"\n",
    "secondV = \"machine learning is the key for data science\"\n",
    "\n",
    "# Create a TfidfVectorizer\n",
    "vectorizer = TfidfVectorizer()\n",
    "\n",
    "# Transform the sentences into TF-IDF vectors (sparse matrix)\n",
    "tfidf_matrix = vectorizer.fit_transform([firstV, secondV])\n",
    "\n",
    "# Use cosine_similarity with the sparse matrix directly\n",
    "similarity_matrix = cosine_similarity(tfidf_matrix, tfidf_matrix)\n",
    "\n",
    "# Print or use the similarity score\n",
    "print(\"Cosine Similarity between the two sentences:\", similarity_matrix[0, 1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>episode</th>\n",
       "      <th>Dialogue</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1x01 - The One Where Monica Gets A Roommate.7...</td>\n",
       "      <td>there s nothing to tell it s just some guy i w...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1x02 - The One With The Sonogram At The End.</td>\n",
       "      <td>you guys don t understand for us kissing is as...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1x03 - The One With The Thumb.720p HDTV.TvR.</td>\n",
       "      <td>hi guys hey pheebs oh oh how d it go um not so...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1x04 - The One With George Stephanopoulos.720...</td>\n",
       "      <td>all right phoebe if i were omnipotent for a da...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1x05 - The One With The East German Laundry D...</td>\n",
       "      <td>let it go it s not a big deal not a big deal i...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                             episode  \\\n",
       "0   1x01 - The One Where Monica Gets A Roommate.7...   \n",
       "1       1x02 - The One With The Sonogram At The End.   \n",
       "2       1x03 - The One With The Thumb.720p HDTV.TvR.   \n",
       "3   1x04 - The One With George Stephanopoulos.720...   \n",
       "4   1x05 - The One With The East German Laundry D...   \n",
       "\n",
       "                                            Dialogue  \n",
       "0  there s nothing to tell it s just some guy i w...  \n",
       "1  you guys don t understand for us kissing is as...  \n",
       "2  hi guys hey pheebs oh oh how d it go um not so...  \n",
       "3  all right phoebe if i were omnipotent for a da...  \n",
       "4  let it go it s not a big deal not a big deal i...  "
      ]
     },
     "execution_count": 127,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from rank_bm25 import BM25Okapi\n",
    "import numpy as np\n",
    "\n",
    "df = pd.read_csv(\"./data/Friends dialogues.csv\")\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {},
   "outputs": [],
   "source": [
    "corpus = df['Dialogue'].tolist()\n",
    "# corpus\n",
    "tokenized_corpus = [doc.split(\" \") for doc in corpus]\n",
    "\n",
    "bm25 = BM25Okapi(tokenized_corpus)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 8x14 - The One With The Secret Closet.\n"
     ]
    }
   ],
   "source": [
    "def search(query):\n",
    "    query=query.lower()\n",
    "    tokenized_query = query.split(\" \")\n",
    "    doc_scores = bm25.get_scores(tokenized_query)\n",
    "    index = np.argsort(-doc_scores)[:5][0]\n",
    "    return df.iloc[index][0]\n",
    "\n",
    "query = \"there is nothing\"\n",
    "episode = search(query)\n",
    "print(episode)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
