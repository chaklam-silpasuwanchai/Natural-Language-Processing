{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Retrieval-Augmented Generation (RAG)\n",
    "\n",
    "A combination of IR and text generation. In RAG, a model first retrieves relevant information (using IR techniques like dense embeddings) and then generates a response based on that information, often using a language model like GPT.\n",
    "\n",
    "IR is a component of RAG, but IR alone is not RAG because it lacks the generation step. RAG enhances traditional IR by allowing an AI to retrieve and synthesize responses dynamically.\n",
    "\n",
    "<img src=\"../figures/rag-lewis.png\" >"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# PART I : Retrieve"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install transformers\n",
    "# !pip install torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import transformers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Load datset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded 150 entries\n"
     ]
    }
   ],
   "source": [
    "dataset = []\n",
    "with open('datasets/cat-facts.txt', 'r') as file:\n",
    "  dataset = file.readlines()\n",
    "  print(f'Loaded {len(dataset)} entries')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load model directly\n",
    "from transformers import AutoTokenizer, AutoModel\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"BAAI/bge-small-en-v1.5\")\n",
    "model = AutoModel.from_pretrained(\"BAAI/bge-small-en-v1.5\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Preprocessing Vector Database"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Each element in the VECTOR_DB will be a tuple (chunk, embedding)\n",
    "# The embedding is a list of floats, for example: [0.1, 0.04, -0.34, 0.21, ...]\n",
    "VECTOR_DB = []\n",
    "\n",
    "def get_embedding(text):\n",
    "    inputs = tokenizer(text, return_tensors=\"pt\", padding=True, truncation=True)\n",
    "    with torch.no_grad():\n",
    "        outputs = model(**inputs)\n",
    "    return outputs.last_hidden_state[:, 0, :].squeeze().tolist()  # Use CLS token embedding\n",
    "\n",
    "def add_chunk_to_database(chunk):\n",
    "    embedding = get_embedding(chunk)\n",
    "    VECTOR_DB.append((chunk, embedding))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Added chunk 30/150 to the database\n",
      "Added chunk 60/150 to the database\n",
      "Added chunk 90/150 to the database\n",
      "Added chunk 120/150 to the database\n",
      "Added chunk 150/150 to the database\n"
     ]
    }
   ],
   "source": [
    "for i, chunk in enumerate(dataset):\n",
    "    add_chunk_to_database(chunk)\n",
    "    \n",
    "    if (i + 1) % 30 == 0 or i + 1 == len(dataset):  # Print every 30 chunks and at the last chunk\n",
    "        print(f'Added chunk {i+1}/{len(dataset)} to the database')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Retrieve Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cosine_similarity(a, b):\n",
    "    dot_product = sum([x * y for x, y in zip(a, b)])\n",
    "    norm_a = sum([x ** 2 for x in a]) ** 0.5\n",
    "    norm_b = sum([x ** 2 for x in b]) ** 0.5\n",
    "    return dot_product / (norm_a * norm_b)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def retrieve(query, top_n=5):\n",
    "    query_embedding = get_embedding(query)\n",
    "    # temporary list to store (chunk, similarity) pairs\n",
    "    similarities = []\n",
    "    for chunk, embedding in VECTOR_DB:\n",
    "        similarity = cosine_similarity(query_embedding, embedding)\n",
    "        similarities.append((chunk, similarity))\n",
    "    # sort by similarity in descending order, because higher similarity means more relevant chunks\n",
    "    similarities.sort(key=lambda x: x[1], reverse=True)\n",
    "    # finally, return the top N most relevant chunks\n",
    "    return similarities[:top_n]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ask me a question:  Do cat love fish?\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Retrieved knowledge:\n",
      " - (similarity: 0.71) Unlike dogs, cats do not have a sweet tooth. Scientists believe this is due to a mutation in a key taste receptor.\n",
      "\n",
      " - (similarity: 0.68) Contrary to popular belief, the cat is a social animal. A pet cat will respond and answer to speech, and seems to enjoy human companionship.\n",
      "\n",
      " - (similarity: 0.67) A cat lover is called an Ailurophilia (Greek: cat+lover).\n",
      "\n",
      " - (similarity: 0.65) Cats hate the water because their fur does not insulate well when itâ€™s wet. The Turkish Van, however, is one cat that likes swimming. Bred in central Asia, its coat has a unique texture that makes it water resistant.\n",
      "\n",
      " - (similarity: 0.65) Cats, especially older cats, do get cancer. Many times this disease can be treated successfully.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "input_query = input('Ask me a question: ') #Do cat love fish?\n",
    "retrieved_knowledge = retrieve(input_query)\n",
    "\n",
    "print('Retrieved knowledge:')\n",
    "for chunk, similarity in retrieved_knowledge:\n",
    "    print(f' - (similarity: {similarity:.2f}) {chunk}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# PART II : Read"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "instruction_prompt = f'''You are a helpful chatbot.\n",
    "Use only the following pieces of context to answer the question. Don't make up any new information:\n",
    "{'\\n'.join([f' - {chunk}' for chunk, similarity in retrieved_knowledge])}\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load model directly\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"meta-llama/Llama-3.2-1B\")\n",
    "model = AutoModelForCausalLM.from_pretrained(\"meta-llama/Llama-3.2-1B\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
