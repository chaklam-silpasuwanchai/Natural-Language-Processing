{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Retrieval-Augmented Generation (RAG)\n",
    "\n",
    "A combination of IR and text generation. In RAG, a model first retrieves relevant information (using IR techniques like dense embeddings) and then generates a response based on that information, often using a language model like GPT.\n",
    "\n",
    "IR is a component of RAG, but IR alone is not RAG because it lacks the generation step. RAG enhances traditional IR by allowing an AI to retrieve and synthesize responses dynamically.\n",
    "\n",
    "<img src=\"../figures/rag-lewis.png\" >"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install transformers\n",
    "# !pip install torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "import re\n",
    "from   random import *\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "import os\n",
    "import transformers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda\n"
     ]
    }
   ],
   "source": [
    "# Set GPU device\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"2\"\n",
    "\n",
    "os.environ['http_proxy']  = 'http://192.41.170.23:3128'\n",
    "os.environ['https_proxy'] = 'http://192.41.170.23:3128'\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "device\n",
    "print(device)\n",
    "\n",
    "#make our work comparable if restarted the kernel\n",
    "SEED = 1234\n",
    "torch.manual_seed(SEED)\n",
    "torch.backends.cudnn.deterministic = True\n",
    "\n",
    "# torch.cuda.get_device_name(0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# PART I : Retrieve"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Load datset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded 150 entries\n"
     ]
    }
   ],
   "source": [
    "dataset = []\n",
    "with open('datasets/cat-facts.txt', 'r') as file:\n",
    "  dataset = file.readlines()\n",
    "  print(f'Loaded {len(dataset)} entries')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load model directly\n",
    "from transformers import AutoTokenizer, AutoModel\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"BAAI/bge-small-en-v1.5\")\n",
    "model = AutoModel.from_pretrained(\"BAAI/bge-small-en-v1.5\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Preprocessing Vector Database"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Each element in the VECTOR_DB will be a tuple (chunk, embedding)\n",
    "# The embedding is a list of floats, for example: [0.1, 0.04, -0.34, 0.21, ...]\n",
    "VECTOR_DB = []\n",
    "\n",
    "def get_embedding(text):\n",
    "    inputs = tokenizer(text, return_tensors=\"pt\", padding=True, truncation=True)\n",
    "    with torch.no_grad():\n",
    "        outputs = model(**inputs)\n",
    "    return outputs.last_hidden_state[:, 0, :].squeeze().tolist()  # Use CLS token embedding\n",
    "\n",
    "def add_chunk_to_database(chunk):\n",
    "    embedding = get_embedding(chunk)\n",
    "    VECTOR_DB.append((chunk, embedding))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Added chunk 30/150 to the database\n",
      "Added chunk 60/150 to the database\n",
      "Added chunk 90/150 to the database\n",
      "Added chunk 120/150 to the database\n",
      "Added chunk 150/150 to the database\n"
     ]
    }
   ],
   "source": [
    "for i, chunk in enumerate(dataset):\n",
    "    add_chunk_to_database(chunk)\n",
    "    \n",
    "    if (i + 1) % 30 == 0 or i + 1 == len(dataset):  # Print every 30 chunks and at the last chunk\n",
    "        print(f'Added chunk {i+1}/{len(dataset)} to the database')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Retrieve Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cosine_similarity(a, b):\n",
    "    dot_product = sum([x * y for x, y in zip(a, b)])\n",
    "    norm_a = sum([x ** 2 for x in a]) ** 0.5\n",
    "    norm_b = sum([x ** 2 for x in b]) ** 0.5\n",
    "    return dot_product / (norm_a * norm_b)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def retrieve(query, top_n=5):\n",
    "    query_embedding = get_embedding(query)\n",
    "    # temporary list to store (chunk, similarity) pairs\n",
    "    similarities = []\n",
    "    for chunk, embedding in VECTOR_DB:\n",
    "        similarity = cosine_similarity(query_embedding, embedding)\n",
    "        similarities.append((chunk, similarity))\n",
    "    # sort by similarity in descending order, because higher similarity means more relevant chunks\n",
    "    similarities.sort(key=lambda x: x[1], reverse=True)\n",
    "    # finally, return the top N most relevant chunks\n",
    "    return similarities[:top_n]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ask me a question:  Do cat love fish?\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Retrieved knowledge:\n",
      " - (similarity: 0.71) Unlike dogs, cats do not have a sweet tooth. Scientists believe this is due to a mutation in a key taste receptor.\n",
      "\n",
      " - (similarity: 0.68) Contrary to popular belief, the cat is a social animal. A pet cat will respond and answer to speech, and seems to enjoy human companionship.\n",
      "\n",
      " - (similarity: 0.67) A cat lover is called an Ailurophilia (Greek: cat+lover).\n",
      "\n",
      " - (similarity: 0.65) Cats hate the water because their fur does not insulate well when it’s wet. The Turkish Van, however, is one cat that likes swimming. Bred in central Asia, its coat has a unique texture that makes it water resistant.\n",
      "\n",
      " - (similarity: 0.65) Cats, especially older cats, do get cancer. Many times this disease can be treated successfully.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "input_query = input('Ask me a question: ') #Do cat love fish?\n",
    "retrieved_knowledge = retrieve(input_query)\n",
    "\n",
    "print('Retrieved knowledge:')\n",
    "for chunk, similarity in retrieved_knowledge:\n",
    "    print(f' - (similarity: {similarity:.2f}) {chunk}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# PART II : Read"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "You are a helpful chatbot.\n",
      "Use only the following pieces of context to answer the question. Don't make up any new information:\n",
      " - Unlike dogs, cats do not have a sweet tooth. Scientists believe this is due to a mutation in a key taste receptor.\n",
      "\n",
      " - Contrary to popular belief, the cat is a social animal. A pet cat will respond and answer to speech, and seems to enjoy human companionship.\n",
      "\n",
      " - A cat lover is called an Ailurophilia (Greek: cat+lover).\n",
      "\n",
      " - Cats hate the water because their fur does not insulate well when it’s wet. The Turkish Van, however, is one cat that likes swimming. Bred in central Asia, its coat has a unique texture that makes it water resistant.\n",
      "\n",
      " - Cats, especially older cats, do get cancer. Many times this disease can be treated successfully.\n",
      "\n",
      "Chatbot response:\n"
     ]
    }
   ],
   "source": [
    "instruction_prompt = f'''You are a helpful chatbot.\n",
    "Use only the following pieces of context to answer the question. Don't make up any new information:\n",
    "{'\\n'.join([f' - {chunk}' for chunk, similarity in retrieved_knowledge])}\n",
    "Chatbot response:'''\n",
    "\n",
    "print(instruction_prompt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load model directly\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"unsloth/Llama-3.2-1B-Instruct\")\n",
    "model = AutoModelForCausalLM.from_pretrained(\"unsloth/Llama-3.2-1B-Instruct\", device_map=device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "inputs = tokenizer(instruction_prompt, return_tensors=\"pt\").to(device)\n",
    "\n",
    "# Generate\n",
    "generate_ids = model.generate(inputs.input_ids, max_length=512)\n",
    "result = tokenizer.batch_decode(\n",
    "    generate_ids, \n",
    "    skip_special_tokens=True, \n",
    "    clean_up_tokenization_spaces=False)[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "You are a helpful chatbot.\n",
      "Use only the following pieces of context to answer the question. Don't make up any new information:\n",
      " - Unlike dogs, cats do not have a sweet tooth. Scientists believe this is due to a mutation in a key taste receptor.\n",
      "\n",
      " - Contrary to popular belief, the cat is a social animal. A pet cat will respond and answer to speech, and seems to enjoy human companionship.\n",
      "\n",
      " - A cat lover is called an Ailurophilia (Greek: cat+lover).\n",
      "\n",
      " - Cats hate the water because their fur does not insulate well when it’s wet. The Turkish Van, however, is one cat that likes swimming. Bred in central Asia, its coat has a unique texture that makes it water resistant.\n",
      "\n",
      " - Cats, especially older cats, do get cancer. Many times this disease can be treated successfully.\n",
      "\n",
      "Chatbot response: Cats are not social animals. They do not respond to speech or enjoy human companionship. They are primarily solitary animals and prefer to spend their time alone. While some cats may show affection, this is not a universal trait among all cats. Cats are not typically considered to be Ailurophiles. Additionally, while some breeds, such as the Turkish Van, are known to be more tolerant of water, this is not a universal trait among all cats.\n"
     ]
    }
   ],
   "source": [
    "print(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
