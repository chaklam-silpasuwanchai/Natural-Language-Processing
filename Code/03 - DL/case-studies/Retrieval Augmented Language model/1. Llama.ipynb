{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "11846e5e-0869-4071-a855-2da89e3e1099",
   "metadata": {
    "tags": []
   },
   "source": [
    "# LLama index\n",
    "\n",
    "In this tutorial, we shall use `Llama index` as a framework to create a retrieval based QA.  `Llama index` is a very nice framework that provides an unified interface with all the large language models.\n",
    "\n",
    "The idea of retrieval based QA is simple, i.e., it would create a similarity based tables ahead called `index`.  During query, it would search the most relevant documents to answer the question."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "222a9f8a-1bee-4ff6-98d9-5f72966dcee2",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "#comment this if you are using puffer or tokyo\n",
    "import os\n",
    "os.environ['http_proxy']  = 'http://192.41.170.23:3128'\n",
    "os.environ['https_proxy'] = 'http://192.41.170.23:3128'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "ba43aeef-f44f-4027-8c27-2229caa9c2c1",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch       : 1.13.0\n",
      "langchain   : 0.0.171\n",
      "transformers: 4.21.3\n",
      "llama_index : 0.6.7\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#better print these versions, since they are very unstable currently....\n",
    "from watermark import watermark\n",
    "print(watermark(packages=\"torch,langchain,transformers,llama_index\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "23091480-4235-40a8-a4e1-0ba9d5f760f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers.utils import logging\n",
    "logging.set_verbosity(40)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f97f4345-e105-4330-9d91-2c9cfe799714",
   "metadata": {},
   "source": [
    "## 1. Define model\n",
    "\n",
    "First, we shall load the large language model that will be use for our question answering model.  We shall use `flan-t5-large` which is quite good.\n",
    "\n",
    "The goal of LlamaIndex is to provide a toolkit of data structures that can organize external information in a manner that is easily compatible with the prompt limitations of an LLM. Therefore LLMs are always used to construct the final answer. Depending on the type of index being used, LLMs may also be used during index construction, insertion, and query traversal.\n",
    "\n",
    "`LlamaIndex` uses` Langchai`n's LLM and `LLMChain` module to define the underlying abstraction. We introduce a wrapper class, `LLMPredictor`, for integration into `LlamaIndex`.\n",
    "\n",
    "It also introduce a `PromptHelper` class, to allow the user to explicitly set certain constraint parameters, such as maximum input size (default is 4096 for davinci models), number of generated output tokens, maximum chunk overlap, and more.\n",
    "\n",
    "By default, it use OpenAI's `text-davinci-003 model`. But you may choose to customize the underlying LLM being used."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "007bdbfc-191d-4664-ad2f-00aa99ed7b19",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from llama_index import LLMPredictor\n",
    "from langchain.llms.base import LLM\n",
    "from transformers import pipeline\n",
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "2403913b-ddc6-4a59-a6d5-cd51a97e3c33",
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Mapping, Optional, Any, List\n",
    "from langchain.callbacks.manager import CallbackManagerForLLMRun\n",
    "\n",
    "class CustomLLM(LLM):\n",
    "    \n",
    "    n: int\n",
    "    model_name = \"google/flan-t5-large\"\n",
    "    pipeline = pipeline(\"text2text-generation\", model=model_name, device=1, model_kwargs={\"torch_dtype\":torch.bfloat16})\n",
    "        \n",
    "    @property\n",
    "    def _llm_type(self) -> str:\n",
    "        return \"custom\"\n",
    "    \n",
    "    def _call(\n",
    "        self,\n",
    "        prompt: str,\n",
    "        stop: Optional[List[str]] = None,\n",
    "        run_manager: Optional[CallbackManagerForLLMRun] = None,\n",
    "    ) -> str:\n",
    "        if stop is not None:\n",
    "            raise ValueError(\"stop kwargs are not permitted.\")\n",
    "        \n",
    "        out = self.pipeline(prompt, max_length=9999)[0][\"generated_text\"]\n",
    "        return out[:self.n]\n",
    "    \n",
    "    @property\n",
    "    def _identifying_params(self) -> Mapping[str, Any]:\n",
    "        \"\"\"Get the identifying parameters.\"\"\"\n",
    "        return {\"n\": self.n}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "feec21af-a20e-49a4-965d-a0d48ca00018",
   "metadata": {},
   "outputs": [],
   "source": [
    "llm_predictor = LLMPredictor(llm=CustomLLM(n=500))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4b624d99-ccbd-4163-9738-8414380ebe95",
   "metadata": {},
   "source": [
    "## 2. Define data\n",
    "\n",
    "Here, I would like to define a `SimpleDirectoryReader` to read through my folders and convert into llama based documents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "1d03f552-1e6a-4c8d-9cc2-5074abf180b7",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from llama_index import SimpleDirectoryReader\n",
    "\n",
    "documents = SimpleDirectoryReader('txt').load_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "512b0dbe-f577-489f-939a-79d6b132d2a5",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# documents"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "15e822da-35ce-45ca-9e20-5a8970cbabb8",
   "metadata": {},
   "source": [
    "## 3. Embeddings\n",
    "\n",
    "Next, we shall define the embeddings to embed our documents.  There are many but here we just use `HuggingFaceEmbeddings`.\n",
    "\n",
    "Note: there are many good embeddings.....please try for example:  `hkunlp/instructor-base`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "b8cf9994-7970-4705-ac2a-0eb54637da13",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.embeddings.huggingface import HuggingFaceEmbeddings\n",
    "from llama_index import LangchainEmbedding\n",
    "\n",
    "hfemb = HuggingFaceEmbeddings()\n",
    "embed_model = LangchainEmbedding(hfemb)\n",
    "\n",
    "#note that huggingface embeddings use sentence transformers"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "93f5f296-a04f-44d6-897f-5e4ded00a46d",
   "metadata": {},
   "source": [
    "## 4. Create index\n",
    "\n",
    "Next, we shall combine everything into something called `ServiceContext` that specifies the language model and the embedding.\n",
    "\n",
    "Here index basically a vector stores containing all the similarity scores.\n",
    "\n",
    "PS: Please note that there are many types of index, such as tree and graph....I have not really fully understand different types and their advantages and limitations.  Please let me know if you figure it out!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "18ac12db-7c1d-4f32-bbad-e7354a141eaf",
   "metadata": {},
   "outputs": [],
   "source": [
    "from llama_index import ServiceContext, PromptHelper\n",
    "\n",
    "max_input_size = 4096 #max input size\n",
    "num_output     = 256 #number of output tokens\n",
    "max_chunk_overlap = 20 #chunk overlap\n",
    "prompt_helper = PromptHelper(max_input_size, num_output, max_chunk_overlap)\n",
    "\n",
    "service_context = ServiceContext.from_defaults(llm_predictor=llm_predictor, embed_model=embed_model, prompt_helper=prompt_helper)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "21033052-b4df-421f-9388-36e5f46d6871",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from llama_index import GPTVectorStoreIndex\n",
    "\n",
    "index = GPTVectorStoreIndex.from_documents(documents, service_context=service_context)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "66d8fdca-939e-4ed3-b639-787eaaf94d64",
   "metadata": {},
   "source": [
    "## 5. Save and load\n",
    "\n",
    "For production, we have to save to disk.\n",
    "\n",
    "By default, data is stored in-memory. To persist to disk:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "56601c52-68e1-4f34-8aa3-661c8e93a5a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "index.storage_context.persist(persist_dir = 'Store')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5b252d9c-67b4-454f-afea-238260b97e89",
   "metadata": {},
   "source": [
    "Let's see how to load"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "802eda33-439b-4902-8f93-18c1aedac5a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "from llama_index import StorageContext, load_index_from_storage\n",
    "\n",
    "# rebuild storage context\n",
    "storage_context = StorageContext.from_defaults(persist_dir = 'Store')\n",
    "# load index\n",
    "index = load_index_from_storage(storage_context, service_context=service_context)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2cd24286-1b35-4aff-bd94-6a94d6e33552",
   "metadata": {},
   "source": [
    "# Query time!  \n",
    "\n",
    "Last, we can query accordingly.  Yay!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "aaa9ba18-1667-41c2-8c32-ab7b1283154c",
   "metadata": {},
   "outputs": [],
   "source": [
    "query_engine = index.as_query_engine()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "10d8cb5e-de50-477a-8c03-7b34f9d6d128",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The Asian Institute of Technology\n"
     ]
    }
   ],
   "source": [
    "response = query_engine.query(\"What is AIT\")\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "c7b2c64d-4d90-4a4c-ae44-ec51e35a6d5e",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Transforming AIT to be a respectable international graduate institution whose research and education contribute to the development of Asia\n"
     ]
    }
   ],
   "source": [
    "response = query_engine.query(\"What is AIT's vision\")\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "d47f9668-3e03-4c0d-b166-b620e9b2ef1c",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1959\n"
     ]
    }
   ],
   "source": [
    "response = query_engine.query(\"When was AIT found?\")\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "84bd19fe-c27d-49e8-9545-21f0b6d3107a",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Bangkok, Thailand\n"
     ]
    }
   ],
   "source": [
    "response = query_engine.query(\"Where is AIT located at?\")\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "aed03c69-ddc7-41c2-8e43-62ca9e0346b5",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "None\n"
     ]
    }
   ],
   "source": [
    "response = query_engine.query(\"Hello, I want a coffee!\")  #try irrelevant question\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "a5ed0362-4031-4124-bf52-c0f03d31a23f",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "yes\n"
     ]
    }
   ],
   "source": [
    "response = query_engine.query(\"Is machine learning taught in AIT?\")\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "d0ded1d5-a711-4524-bbea-a107e6c71cee",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "no\n"
     ]
    }
   ],
   "source": [
    "response = query_engine.query(\"Is medicine taught in AIT?\")\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "1f304075-b491-4ffe-b633-c96f688375aa",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Computer Science/Computer Engineering/ICT\n"
     ]
    }
   ],
   "source": [
    "response = query_engine.query(\"What is the preferred background to study in DSAI\")\n",
    "print(response) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "0b64c551-3c28-4716-8055-60589c633b2d",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data Modeling and Management Machine Learning Business Intelligence and Analytics Computer Programming for Data Science and Artificial Intelligence Artificial Intelligence: Natural Language Understanding Elective courses Artificial Intelligence: Knowledge Representation and Reasoning\n"
     ]
    }
   ],
   "source": [
    "response = query_engine.query(\"What are the required courses in DSAI\")\n",
    "print(response) "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
