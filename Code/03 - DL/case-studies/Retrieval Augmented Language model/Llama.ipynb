{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "11846e5e-0869-4071-a855-2da89e3e1099",
   "metadata": {},
   "source": [
    "# LLama index\n",
    "\n",
    "In this tutorial, we shall use `Llama index` as a framework to create a retrieval based QA.  `Llama index` is a very nice framework that provides an unified interface with all the large language models.\n",
    "\n",
    "The idea of retrieval based QA is simple, i.e., it would create a similarity based tables ahead called `index`.  During query, it would search the most relevant documents to answer the question."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "222a9f8a-1bee-4ff6-98d9-5f72966dcee2",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "#comment this if you are using puffer or tokyo\n",
    "import os\n",
    "os.environ['http_proxy']  = 'http://192.41.170.23:3128'\n",
    "os.environ['https_proxy'] = 'http://192.41.170.23:3128'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "8b06f98e-63bf-489c-a6b7-75f827f7b711",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# !pip install python-dotenv\n",
    "# !pip install llama-index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "cc710885-ece1-47fd-b82d-f34ee16aac21",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# from dotenv import load_dotenv\n",
    "# load_dotenv() #create a .env file and put HUGGINGFACE_KEY = ... or OPENAI_API_KEY inside your .env file\n",
    "# here i comment out because i am not using open ai"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "f97f4345-e105-4330-9d91-2c9cfe799714",
   "metadata": {},
   "source": [
    "## 1. Define model\n",
    "\n",
    "First, we shall load the large language model that will be use for our question answering model.  We shall use `flan-t5-large` which is quite good.\n",
    "\n",
    "The goal of LlamaIndex is to provide a toolkit of data structures that can organize external information in a manner that is easily compatible with the prompt limitations of an LLM. Therefore LLMs are always used to construct the final answer. Depending on the type of index being used, LLMs may also be used during index construction, insertion, and query traversal.\n",
    "\n",
    "`LlamaIndex` uses` Langchai`n's LLM and `LLMChain` module to define the underlying abstraction. We introduce a wrapper class, `LLMPredictor`, for integration into `LlamaIndex`.\n",
    "\n",
    "It also introduce a `PromptHelper` class, to allow the user to explicitly set certain constraint parameters, such as maximum input size (default is 4096 for davinci models), number of generated output tokens, maximum chunk overlap, and more.\n",
    "\n",
    "By default, it use OpenAI's `text-davinci-003 model`. But you may choose to customize the underlying LLM being used."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "007bdbfc-191d-4664-ad2f-00aa99ed7b19",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from llama_index import LLMPredictor\n",
    "from langchain.llms.base import LLM\n",
    "from transformers import pipeline\n",
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "485d5bd0-0f64-4db5-b5b3-6f5ea29b6fbc",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "#I just copy from llama index github:  https://github.com/jerryjliu/llama_index/blob/main/docs/how_to/customization/custom_llms.md\n",
    "\n",
    "class ourLLM(LLM):\n",
    "    model_name = \"google/flan-t5-large\"\n",
    "    pipeline = pipeline(\"text2text-generation\", model=model_name, device=1, model_kwargs={\"torch_dtype\":torch.bfloat16})\n",
    "\n",
    "    def _call(self, prompt, stop=None):\n",
    "        return self.pipeline(prompt, max_length=9999)[0][\"generated_text\"]\n",
    " \n",
    "    def _identifying_params(self):\n",
    "        return {\"name_of_model\": self.model_name}\n",
    "\n",
    "    def _llm_type(self):\n",
    "        return \"custom\"\n",
    "\n",
    "llm_predictor = LLMPredictor(llm=ourLLM())"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "4b624d99-ccbd-4163-9738-8414380ebe95",
   "metadata": {},
   "source": [
    "## 2. Define data\n",
    "\n",
    "Here, I would like to define a `SimpleDirectoryReader` to read through my folders and convert into llama based documents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "1d03f552-1e6a-4c8d-9cc2-5074abf180b7",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from llama_index import SimpleDirectoryReader\n",
    "\n",
    "documents = SimpleDirectoryReader('data').load_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "512b0dbe-f577-489f-939a-79d6b132d2a5",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# documents"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "15e822da-35ce-45ca-9e20-5a8970cbabb8",
   "metadata": {},
   "source": [
    "## 3. Embeddings\n",
    "\n",
    "Next, we shall define the embeddings to embed our documents.  There are many but here we just use `HuggingFaceEmbeddings`.\n",
    "\n",
    "Note: there are many good embeddings.....please try for example:  `hkunlp/instructor-base`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "b8cf9994-7970-4705-ac2a-0eb54637da13",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.embeddings.huggingface import HuggingFaceEmbeddings\n",
    "from llama_index import LangchainEmbedding\n",
    "\n",
    "hfemb = HuggingFaceEmbeddings()\n",
    "embed_model = LangchainEmbedding(hfemb)\n",
    "\n",
    "#note that huggingface embeddings use sentence transformers"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "93f5f296-a04f-44d6-897f-5e4ded00a46d",
   "metadata": {},
   "source": [
    "## 4. Create index\n",
    "\n",
    "Next, we shall combine everything into something called `ServiceContext` that specifies the language model and the embedding.\n",
    "\n",
    "Here index basically a vector stores containing all the similarity scores.\n",
    "\n",
    "PS: Please note that there are many types of index, such as tree and graph....I have not really fully understand different types and their advantages and limitations.  Please let me know if you figure it out!\n",
    "\n",
    "PS2:  Chromadb is another very nice vector stores so definitely check out!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "18ac12db-7c1d-4f32-bbad-e7354a141eaf",
   "metadata": {},
   "outputs": [],
   "source": [
    "from llama_index import ServiceContext, PromptHelper\n",
    "\n",
    "max_input_size = 4096 #max input size\n",
    "num_output = 256 #number of output tokens\n",
    "max_chunk_overlap = 20 #chunk overlap\n",
    "prompt_helper = PromptHelper(max_input_size, num_output, max_chunk_overlap)\n",
    "\n",
    "service_context = ServiceContext.from_defaults(llm_predictor=llm_predictor, embed_model=embed_model, prompt_helper=prompt_helper)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "21033052-b4df-421f-9388-36e5f46d6871",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from llama_index import GPTVectorStoreIndex\n",
    "\n",
    "index = GPTVectorStoreIndex.from_documents(documents, service_context=service_context)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "66d8fdca-939e-4ed3-b639-787eaaf94d64",
   "metadata": {},
   "source": [
    "## 5. Save and load\n",
    "\n",
    "For production, we have to save to disk.\n",
    "\n",
    "By default, data is stored in-memory. To persist to disk:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "56601c52-68e1-4f34-8aa3-661c8e93a5a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "index.storage_context.persist(persist_dir = 'Store')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "5b252d9c-67b4-454f-afea-238260b97e89",
   "metadata": {},
   "source": [
    "Let's see how to load"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "802eda33-439b-4902-8f93-18c1aedac5a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "from llama_index import StorageContext, load_index_from_storage\n",
    "\n",
    "# rebuild storage context\n",
    "storage_context = StorageContext.from_defaults(persist_dir = 'Store')\n",
    "# load index\n",
    "index = load_index_from_storage(storage_context, service_context=service_context)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "2cd24286-1b35-4aff-bd94-6a94d6e33552",
   "metadata": {},
   "source": [
    "# Query time!  \n",
    "\n",
    "Last, we can query accordingly.  Yay!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "aaa9ba18-1667-41c2-8c32-ab7b1283154c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Token indices sequence length is longer than the specified maximum sequence length for this model (588 > 512). Running this sequence through the model will result in indexing errors\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The Asian Institute of Technology\n"
     ]
    }
   ],
   "source": [
    "query_engine = index.as_query_engine()\n",
    "response = query_engine.query(\"What is AIT\")\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "c7b2c64d-4d90-4a4c-ae44-ec51e35a6d5e",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Transforming AIT to be a respectable international graduate institution whose research and education contribute to the development of Asia\n"
     ]
    }
   ],
   "source": [
    "response = query_engine.query(\"What is AIT's vision\")\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "d47f9668-3e03-4c0d-b166-b620e9b2ef1c",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1959\n"
     ]
    }
   ],
   "source": [
    "response = query_engine.query(\"When was AIT found?\")\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "84bd19fe-c27d-49e8-9545-21f0b6d3107a",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Bangkok, Thailand\n"
     ]
    }
   ],
   "source": [
    "response = query_engine.query(\"Where is AIT located at?\")\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "aed03c69-ddc7-41c2-8e43-62ca9e0346b5",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "None\n"
     ]
    }
   ],
   "source": [
    "response = query_engine.query(\"Hello, I want a coffee!\")  #try irrelevant question\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "a5ed0362-4031-4124-bf52-c0f03d31a23f",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Token indices sequence length is longer than the specified maximum sequence length for this model (1046 > 1024). Running this sequence through the model will result in indexing errors\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "yes\n"
     ]
    }
   ],
   "source": [
    "response = query_engine.query(\"Is machine learning taught in AIT?\")\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "d0ded1d5-a711-4524-bbea-a107e6c71cee",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "no\n"
     ]
    }
   ],
   "source": [
    "response = query_engine.query(\"Is medicine taught in AIT?\")\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "1f304075-b491-4ffe-b633-c96f688375aa",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Computer Science/Computer Engineering/ICT\n"
     ]
    }
   ],
   "source": [
    "response = query_engine.query(\"What is the preferred background to study in DSAI\")\n",
    "print(response) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "0b64c551-3c28-4716-8055-60589c633b2d",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data Modeling and Management Machine Learning Business Intelligence and Analytics Computer Programming for Data Science and Artificial Intelligence Artificial Intelligence: Natural Language Understanding Elective courses Artificial Intelligence: Knowledge Representation and Reasoning\n"
     ]
    }
   ],
   "source": [
    "response = query_engine.query(\"What are the required courses in DSAI\")\n",
    "print(response) "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
