{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "11846e5e-0869-4071-a855-2da89e3e1099",
   "metadata": {},
   "source": [
    "# Langchain\n",
    "\n",
    "In this tutorial, we shall use `Langchain` as a framework to create a retrieval based QA.  While `Llama index` is very nice, `Langchain` provides more complex workflow."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "222a9f8a-1bee-4ff6-98d9-5f72966dcee2",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "#comment this if you are using puffer or tokyo\n",
    "import os\n",
    "os.environ['http_proxy']  = 'http://192.41.170.23:3128'\n",
    "os.environ['https_proxy'] = 'http://192.41.170.23:3128'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "e2670876-48b9-4050-b307-cc8ca7800bd9",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch       : 1.13.0\n",
      "langchain   : 0.0.171\n",
      "transformers: 4.21.3\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from watermark import watermark\n",
    "print(watermark(packages=\"torch,langchain,transformers\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "23091480-4235-40a8-a4e1-0ba9d5f760f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers.utils import logging\n",
    "logging.set_verbosity(40)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f97f4345-e105-4330-9d91-2c9cfe799714",
   "metadata": {},
   "source": [
    "## 1. Define model\n",
    "\n",
    "First, we shall load the large language model that will be use for our question answering model.  Instead of using`flan-t5-large`, I will use another model called `fastchat` which is even better."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "007bdbfc-191d-4664-ad2f-00aa99ed7b19",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from langchain.llms.base import LLM\n",
    "from transformers import pipeline\n",
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "485d5bd0-0f64-4db5-b5b3-6f5ea29b6fbc",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# from typing import Mapping, Optional, Any, List\n",
    "# from langchain.callbacks.manager import CallbackManagerForLLMRun\n",
    "\n",
    "# class CustomLLM(LLM):\n",
    "    \n",
    "#     n: int\n",
    "#     model_name = \"google/flan-t5-large\"\n",
    "#     pipeline = pipeline(\"text2text-generation\", model=model_name, device=1, model_kwargs={\"torch_dtype\":torch.bfloat16})\n",
    "        \n",
    "#     @property\n",
    "#     def _llm_type(self) -> str:\n",
    "#         return \"custom\"\n",
    "    \n",
    "#     def _call(\n",
    "#         self,\n",
    "#         prompt: str,\n",
    "#         stop: Optional[List[str]] = None,\n",
    "#         run_manager: Optional[CallbackManagerForLLMRun] = None,\n",
    "#     ) -> str:\n",
    "#         if stop is not None:\n",
    "#             raise ValueError(\"stop kwargs are not permitted.\")\n",
    "        \n",
    "#         out = self.pipeline(prompt, max_length=9999)[0][\"generated_text\"]\n",
    "#         return out[:self.n]\n",
    "    \n",
    "#     @property\n",
    "#     def _identifying_params(self) -> Mapping[str, Any]:\n",
    "#         \"\"\"Get the identifying parameters.\"\"\"\n",
    "#         return {\"n\": self.n}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "a81d2c8f-c3ad-4bd5-9597-2d9dcfba2590",
   "metadata": {},
   "outputs": [],
   "source": [
    "# llm=CustomLLM(n=500)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "b16836fd-c599-4c00-a193-bf7430b9c896",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Device has 2 GPUs available. Provide device={deviceId} to `from_model_id` to use availableGPUs for execution. deviceId is -1 (default) for CPU and can be a positive integer associated with CUDA device id.\n"
     ]
    }
   ],
   "source": [
    "from langchain import HuggingFacePipeline\n",
    "\n",
    "llm = HuggingFacePipeline.from_model_id(device=1,\n",
    "                                        model_id= 'lmsys/fastchat-t5-3b-v1.0', \n",
    "                                        task= 'text2text-generation',\n",
    "                                        \n",
    "                                        model_kwargs={ \"max_length\": 256, \"temperature\": 0,\n",
    "                                                      \"torch_dtype\":torch.float32,\n",
    "                                                      \"repetition_penalty\": 1.3})"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4b624d99-ccbd-4163-9738-8414380ebe95",
   "metadata": {},
   "source": [
    "## 2. Define data\n",
    "\n",
    "Here, I would like to define a `langchain.document_loaders.DirectoryLoader` to read through my folders.  Note that I have defined multiple types of documents such as `txt` and `pdf`.  Of course, I can also read other types such as web.   As you can see, this is not available in `Llama index`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "3c4af1f2-f5e8-46fa-8c5d-1cff6cc53083",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from langchain.document_loaders import DirectoryLoader\n",
    "\n",
    "txt_loader = DirectoryLoader('txt', glob=\"**/*.txt\")\n",
    "pdf_loader = DirectoryLoader('pdf', glob=\"**/*.pdf\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "4e358295-2d1e-453d-be39-97cc4839045a",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "detectron2 is not installed. Cannot use the hi_res partitioning strategy. Falling back to partitioning with another strategy.\n",
      "Falling back to partitioning with ocr_only.\n"
     ]
    }
   ],
   "source": [
    "#take all the loader\n",
    "loaders = [pdf_loader, txt_loader]\n",
    "\n",
    "#lets create document \n",
    "documents = []\n",
    "for loader in loaders:\n",
    "    documents.extend(loader.load())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "512b0dbe-f577-489f-939a-79d6b132d2a5",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(documents)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fa5b7f59-b84b-4991-b40d-02d110c1170f",
   "metadata": {
    "tags": []
   },
   "source": [
    "Now that we've loaded our document, we need to split the text up so that we don't run into token limits when we retrieve relevant document information.\n",
    "\n",
    "To do so, we can use the `CharacterTextSplitter` from `LangChain` and in this case we'll split the documents in chunk sizes of 1000 characters, with no overlap between the chunks.\n",
    "\n",
    "The text_splitter is used to split the documents and store the resulting chunks in a variable called `texts`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "cc2ff2be-4acb-4bf0-812d-173573026acf",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Created a chunk of size 1050, which is longer than the specified 1000\n",
      "Created a chunk of size 1002, which is longer than the specified 1000\n",
      "Created a chunk of size 1064, which is longer than the specified 1000\n",
      "Created a chunk of size 1099, which is longer than the specified 1000\n",
      "Created a chunk of size 1032, which is longer than the specified 1000\n",
      "Created a chunk of size 1065, which is longer than the specified 1000\n",
      "Created a chunk of size 1215, which is longer than the specified 1000\n",
      "Created a chunk of size 1026, which is longer than the specified 1000\n",
      "Created a chunk of size 1018, which is longer than the specified 1000\n",
      "Created a chunk of size 1295, which is longer than the specified 1000\n"
     ]
    }
   ],
   "source": [
    "from langchain.text_splitter import CharacterTextSplitter\n",
    "\n",
    "text_splitter = CharacterTextSplitter(chunk_size=1000, chunk_overlap=0)\n",
    "texts = text_splitter.split_documents(documents)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "15e822da-35ce-45ca-9e20-5a8970cbabb8",
   "metadata": {},
   "source": [
    "## 3. Embeddings\n",
    "\n",
    "Next, we shall define the embeddings to embed our documents.  There are many but here we just use `HuggingFaceEmbeddings`.\n",
    "\n",
    "Note: there are many good embeddings.....please try for example:  `hkunlp/instructor-base`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "b8cf9994-7970-4705-ac2a-0eb54637da13",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.embeddings.huggingface import HuggingFaceEmbeddings\n",
    "\n",
    "hfemb = HuggingFaceEmbeddings()\n",
    "#note that huggingface embeddings use sentence transformers"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "93f5f296-a04f-44d6-897f-5e4ded00a46d",
   "metadata": {},
   "source": [
    "## 4. Create vector store\n",
    "\n",
    "Next, we create the vector store containing embeddings of each document, which would facilitate the search.  Here I use `Chromadb` which is a very efficient vector store."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "e1c77a5b-c353-4170-bd4b-fef518671917",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using embedded DuckDB with persistence: data will be stored in: db\n"
     ]
    }
   ],
   "source": [
    "from langchain.vectorstores import Chroma\n",
    "\n",
    "persist_directory = 'db'\n",
    "\n",
    "vectorstore = Chroma.from_documents(documents=texts, embedding=hfemb, persist_directory=persist_directory)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9df2606d-c754-4135-b8d0-374f8e78d5c0",
   "metadata": {},
   "source": [
    "## 5. Query time\n",
    "\n",
    "Now that we have the LLM and the vectorstores, there are many ways to run it.  In Langchain, it's called `chain`.   Here we shall use a simple one called `RetrievalQA`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "205b8091-f828-43cd-adf0-d1e4add0327c",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from langchain.chains import RetrievalQA\n",
    "\n",
    "qa = RetrievalQA.from_chain_type(llm=llm, chain_type=\"stuff\", retriever=vectorstore.as_retriever())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "42a7efaa-a3d3-413c-87da-347001ac67ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt_trick = \" Try to elaborate as much as you can.\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "722ba685-447f-4e2d-90b3-0b303ebd2b40",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'<pad>  AIT  is  located  just  north  of  Bangkok,  Thailand.\\n'"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "query = \"Where is AIT?\" + prompt_trick\n",
    "qa.run(query)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "fadcf00a-1dcf-4819-91c7-01ac8d83ccdd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'<pad>  The  Asian  Institute  of  Technology  (AIT)  is  an  international  English-speaking  postgraduate  institution,  focusing  on  engineering,  environment,  and  management  studies.  It  was  founded  in  1959  and  offers  rigorous  academic,  research,  and  experiential  outreach  programs  to  prepare  graduates  for  professional  success  and  leadership  roles  in  Asia  and  beyond.  AIT  has  a  global  reputation  and  emphasizes  its  global  connections,  injection  of  innovation  into  research  and  teaching,  relevance  to  industry,  and  nurturing  of  entrepreneurship.  It  operates  as  a  multicultural  community  where  a  cosmopolitan  approach  to  living  and  learning  is  the  rule.\\n'"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "query = \"What is AIT?\" + prompt_trick\n",
    "qa.run(query)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "84703443-31f5-4002-bb64-aea87285e4ae",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'<pad> Based  on  the  context,  it  seems  that  AIT  does  a  wide  range  of  research.  The  research  document  provided  by  Professor  Sudip  K.  Rakshit  provides  a  sampling  of  the  work  that  AIT  is  currently  engaged  in,  which  includes  various  fields  such  as  engineering,  computer  science,  mathematics,  and  social  sciences.  These  fields  are  mentioned  to  be  areas  where  AIT  has  made  significant  advances  and  are  being  shared  with  the  outside  world  through  this  document.\\n'"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "query = \"What kind of research does AIT do?\" + prompt_trick\n",
    "qa.run(query)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
