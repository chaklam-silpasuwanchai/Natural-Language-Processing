{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "11846e5e-0869-4071-a855-2da89e3e1099",
   "metadata": {},
   "source": [
    "# Langchain\n",
    "\n",
    "In this tutorial, we shall use `Langchain` as a framework to create a retrieval based QA.  While `Llama index` is very nice, `Langchain` provides more complex workflow."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "222a9f8a-1bee-4ff6-98d9-5f72966dcee2",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "#comment this if you are using puffer or tokyo\n",
    "import os\n",
    "os.environ['http_proxy']  = 'http://192.41.170.23:3128'\n",
    "os.environ['https_proxy'] = 'http://192.41.170.23:3128'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "e2670876-48b9-4050-b307-cc8ca7800bd9",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch       : 1.13.0\n",
      "langchain   : 0.0.171\n",
      "transformers: 4.21.3\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from watermark import watermark\n",
    "print(watermark(packages=\"torch,langchain,transformers\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "23091480-4235-40a8-a4e1-0ba9d5f760f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers.utils import logging\n",
    "logging.set_verbosity(40)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f97f4345-e105-4330-9d91-2c9cfe799714",
   "metadata": {},
   "source": [
    "## 1. Define model\n",
    "\n",
    "First, we shall load the large language model that will be use for our question answering model.  We shall use `flan-t5-large` which is quite good."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "007bdbfc-191d-4664-ad2f-00aa99ed7b19",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from langchain.llms.base import LLM\n",
    "from transformers import pipeline\n",
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "485d5bd0-0f64-4db5-b5b3-6f5ea29b6fbc",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from typing import Mapping, Optional, Any, List\n",
    "from langchain.callbacks.manager import CallbackManagerForLLMRun\n",
    "\n",
    "class CustomLLM(LLM):\n",
    "    \n",
    "    n: int\n",
    "    model_name = \"google/flan-t5-large\"\n",
    "    pipeline = pipeline(\"text2text-generation\", model=model_name, device=1, model_kwargs={\"torch_dtype\":torch.bfloat16})\n",
    "        \n",
    "    @property\n",
    "    def _llm_type(self) -> str:\n",
    "        return \"custom\"\n",
    "    \n",
    "    def _call(\n",
    "        self,\n",
    "        prompt: str,\n",
    "        stop: Optional[List[str]] = None,\n",
    "        run_manager: Optional[CallbackManagerForLLMRun] = None,\n",
    "    ) -> str:\n",
    "        if stop is not None:\n",
    "            raise ValueError(\"stop kwargs are not permitted.\")\n",
    "        \n",
    "        out = self.pipeline(prompt, max_length=9999)[0][\"generated_text\"]\n",
    "        return out[:self.n]\n",
    "    \n",
    "    @property\n",
    "    def _identifying_params(self) -> Mapping[str, Any]:\n",
    "        \"\"\"Get the identifying parameters.\"\"\"\n",
    "        return {\"n\": self.n}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a81d2c8f-c3ad-4bd5-9597-2d9dcfba2590",
   "metadata": {},
   "outputs": [],
   "source": [
    "llm=CustomLLM(n=500)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4b624d99-ccbd-4163-9738-8414380ebe95",
   "metadata": {},
   "source": [
    "## 2. Define data\n",
    "\n",
    "Here, I would like to define a `langchain.document_loaders.DirectoryLoader` to read through my folders.  Note that I have defined multiple types of documents such as `txt` and `pdf`.  Of course, I can also read other types such as web.   As you can see, this is not available in `Llama index`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3c4af1f2-f5e8-46fa-8c5d-1cff6cc53083",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from langchain.document_loaders import DirectoryLoader\n",
    "\n",
    "txt_loader = DirectoryLoader('txt', glob=\"**/*.txt\")\n",
    "pdf_loader = DirectoryLoader('pdf', glob=\"**/*.pdf\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "4e358295-2d1e-453d-be39-97cc4839045a",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "detectron2 is not installed. Cannot use the hi_res partitioning strategy. Falling back to partitioning with another strategy.\n",
      "Falling back to partitioning with ocr_only.\n"
     ]
    }
   ],
   "source": [
    "#take all the loader\n",
    "loaders = [pdf_loader, txt_loader]\n",
    "\n",
    "#lets create document \n",
    "documents = []\n",
    "for loader in loaders:\n",
    "    documents.extend(loader.load())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "512b0dbe-f577-489f-939a-79d6b132d2a5",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(documents)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fa5b7f59-b84b-4991-b40d-02d110c1170f",
   "metadata": {
    "tags": []
   },
   "source": [
    "To improve the search efficiency, we will split the documents into smaller documents using `CharacterTextSplitter`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "cc2ff2be-4acb-4bf0-812d-173573026acf",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Created a chunk of size 1050, which is longer than the specified 1000\n",
      "Created a chunk of size 1002, which is longer than the specified 1000\n",
      "Created a chunk of size 1064, which is longer than the specified 1000\n",
      "Created a chunk of size 1099, which is longer than the specified 1000\n",
      "Created a chunk of size 1032, which is longer than the specified 1000\n",
      "Created a chunk of size 1065, which is longer than the specified 1000\n",
      "Created a chunk of size 1215, which is longer than the specified 1000\n",
      "Created a chunk of size 1026, which is longer than the specified 1000\n",
      "Created a chunk of size 1018, which is longer than the specified 1000\n",
      "Created a chunk of size 1295, which is longer than the specified 1000\n"
     ]
    }
   ],
   "source": [
    "from langchain.text_splitter import CharacterTextSplitter\n",
    "\n",
    "text_splitter = CharacterTextSplitter(chunk_size=1000, chunk_overlap=0)\n",
    "texts = text_splitter.split_documents(documents)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "15e822da-35ce-45ca-9e20-5a8970cbabb8",
   "metadata": {},
   "source": [
    "## 3. Embeddings\n",
    "\n",
    "Next, we shall define the embeddings to embed our documents.  There are many but here we just use `HuggingFaceEmbeddings`.\n",
    "\n",
    "Note: there are many good embeddings.....please try for example:  `hkunlp/instructor-base`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "b8cf9994-7970-4705-ac2a-0eb54637da13",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.embeddings.huggingface import HuggingFaceEmbeddings\n",
    "\n",
    "hfemb = HuggingFaceEmbeddings()\n",
    "#note that huggingface embeddings use sentence transformers"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "93f5f296-a04f-44d6-897f-5e4ded00a46d",
   "metadata": {},
   "source": [
    "## 4. Create vector store\n",
    "\n",
    "Next, we create the vector store containing embeddings of each document, which would facilitate the search.  Here I use `Chromadb` which is a very efficient vector store."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "e1c77a5b-c353-4170-bd4b-fef518671917",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using embedded DuckDB with persistence: data will be stored in: db\n"
     ]
    }
   ],
   "source": [
    "from langchain.vectorstores import Chroma\n",
    "\n",
    "persist_directory = 'db'\n",
    "\n",
    "vectorstore = Chroma.from_documents(documents=texts, embedding=hfemb, persist_directory=persist_directory)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9df2606d-c754-4135-b8d0-374f8e78d5c0",
   "metadata": {},
   "source": [
    "## 5. Query time\n",
    "\n",
    "Now that we have the LLM (i.e., Flan-T5) and the vectorstores, there are many ways to run it.  In Langchain, it's called `chain`.   Here we shall use a simple one called `RetrievalQA`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "205b8091-f828-43cd-adf0-d1e4add0327c",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from langchain.chains import RetrievalQA\n",
    "\n",
    "qa = RetrievalQA.from_chain_type(llm=llm, chain_type=\"stuff\", retriever=vectorstore.as_retriever())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "722ba685-447f-4e2d-90b3-0b303ebd2b40",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Asia'"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "query = \"Where is AIT\"\n",
    "qa.run(query)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "fadcf00a-1dcf-4819-91c7-01ac8d83ccdd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'AIT remains a magnet for International Organizations and institutions of higher learning, when it comes to partnerships in education, research and development and outreach activities'"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "query = \"What is AIT\"\n",
    "qa.run(query)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "84703443-31f5-4002-bb64-aea87285e4ae",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'regional and transnational research projects'"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "query = \"What kind of research does AIT do\"\n",
    "qa.run(query)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
