{
  "cells": [
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "bSz5jzj61nHc"
      },
      "source": [
        "# BERT\n",
        "\n",
        "We shall implement BERT.  For this tutorial, you may want to first look at my Transformers tutorial to get a basic understanding of Transformers. \n",
        "\n",
        "For BERT, the main difference is on how we process the datasets, i.e., masking.   Aside from that, the backbone model is still the Transformers."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "-8kZmr4ItGUj"
      },
      "outputs": [],
      "source": [
        "import math\n",
        "import re\n",
        "from   random import *\n",
        "import numpy as np\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "import torch.nn.functional as F\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 1. Data\n",
        "\n",
        "For simplicity, we shall use very simple data like this."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "['martin luther king jr (born michael king jr; january 15 1929 – april 4 1968) was an american baptist minister and activist who became the most visible spokesman and leader in the american civil rights movement from 1955 until his assassination in 1968',\n",
              " 'king advanced civil rights through nonviolence and civil disobedience inspired by his christian beliefs and the nonviolent activism of mahatma gandhi',\n",
              " \"he was the son of early civil rights activist and minister martin luther king sr\\n\\nking participated in and led marches for blacks' right to vote desegregation labor rights and other basic civil rights[1]\",\n",
              " 'king led the 1955 montgomery bus boycott and later became the first president of the southern christian leadership conference (sclc)',\n",
              " 'as president of the sclc he led the unsuccessful albany movement in albany georgia and helped organize some of the nonviolent 1963 protests in birmingham alabama',\n",
              " 'king helped organize the 1963 march on washington where he delivered his famous \"i have a dream\" speech on the steps of the lincoln memorial']"
            ]
          },
          "execution_count": 2,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "import spacy\n",
        "\n",
        "with open (\"data/wiki_king.txt\", \"r\") as f:\n",
        "    raw_text = f.read()\n",
        "nlp = spacy.load(\"en_core_web_sm\")\n",
        "doc = nlp(raw_text)\n",
        "sentences = list(doc.sents)\n",
        "\n",
        "text = [x.text.lower() for x in sentences] #lower case\n",
        "text = [re.sub(\"[.,!?\\\\-]\", '', x) for x in text] #clean all symbols\n",
        "\n",
        "text"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "martin luther king jr (born michael king jr; january 15 1929 – april 4 1968) was an american baptist minister and activist who became the most visible spokesman and leader in the american civil rights movement from 1955 until his assassination in 1968 _____\n",
            "['martin', 'luther', 'king', 'jr', '(born', 'michael', 'king', 'jr;', 'january', '15', '1929', '–', 'april', '4', '1968)', 'was', 'an', 'american', 'baptist', 'minister', 'and', 'activist', 'who', 'became', 'the', 'most', 'visible', 'spokesman', 'and', 'leader', 'in', 'the', 'american', 'civil', 'rights', 'movement', 'from', '1955', 'until', 'his', 'assassination', 'in', '1968']\n",
            "king advanced civil rights through nonviolence and civil disobedience inspired by his christian beliefs and the nonviolent activism of mahatma gandhi _____\n",
            "['king', 'advanced', 'civil', 'rights', 'through', 'nonviolence', 'and', 'civil', 'disobedience', 'inspired', 'by', 'his', 'christian', 'beliefs', 'and', 'the', 'nonviolent', 'activism', 'of', 'mahatma', 'gandhi']\n",
            "he was the son of early civil rights activist and minister martin luther king sr\n",
            "\n",
            "king participated in and led marches for blacks' right to vote desegregation labor rights and other basic civil rights[1] _____\n",
            "['he', 'was', 'the', 'son', 'of', 'early', 'civil', 'rights', 'activist', 'and', 'minister', 'martin', 'luther', 'king', 'sr', 'king', 'participated', 'in', 'and', 'led', 'marches', 'for', \"blacks'\", 'right', 'to', 'vote', 'desegregation', 'labor', 'rights', 'and', 'other', 'basic', 'civil', 'rights[1]']\n",
            "king led the 1955 montgomery bus boycott and later became the first president of the southern christian leadership conference (sclc) _____\n",
            "['king', 'led', 'the', '1955', 'montgomery', 'bus', 'boycott', 'and', 'later', 'became', 'the', 'first', 'president', 'of', 'the', 'southern', 'christian', 'leadership', 'conference', '(sclc)']\n",
            "as president of the sclc he led the unsuccessful albany movement in albany georgia and helped organize some of the nonviolent 1963 protests in birmingham alabama _____\n",
            "['as', 'president', 'of', 'the', 'sclc', 'he', 'led', 'the', 'unsuccessful', 'albany', 'movement', 'in', 'albany', 'georgia', 'and', 'helped', 'organize', 'some', 'of', 'the', 'nonviolent', '1963', 'protests', 'in', 'birmingham', 'alabama']\n",
            "king helped organize the 1963 march on washington where he delivered his famous \"i have a dream\" speech on the steps of the lincoln memorial _____\n",
            "['king', 'helped', 'organize', 'the', '1963', 'march', 'on', 'washington', 'where', 'he', 'delivered', 'his', 'famous', '\"i', 'have', 'a', 'dream\"', 'speech', 'on', 'the', 'steps', 'of', 'the', 'lincoln', 'memorial']\n"
          ]
        }
      ],
      "source": [
        "for sentence in text:\n",
        "    print(sentence, \"_____\")\n",
        "    words = sentence.split()\n",
        "    print(words)"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Making vocabs\n",
        "\n",
        "Before making the vocabs, let's remove all question marks and perios, etc, then turn everything to lowercase, and then simply split the text. "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "AhX8b1ydtrVf"
      },
      "outputs": [],
      "source": [
        "# combine everything into one to make vocabs\n",
        "word_list = list(set(\" \".join(text).split()))\n",
        "word2id = {'[PAD]': 0, '[CLS]': 1, '[SEP]': 2, '[MASK]': 3}  #special tokens.\n",
        "\n",
        "#create the word2id\n",
        "for i, w in enumerate(word_list):\n",
        "    word2id[w] = i + 4  #because 0-3 are already occupied\n",
        "    id2word = {i: w for i, w in enumerate(word2id)}\n",
        "    vocab_size = len(word2id)\n",
        "\n",
        "#list of all tokens for whole text\n",
        "token_list = list()\n",
        "for sentence in sentences:\n",
        "    arr = [word2id[word] for sentence in text for word in sentence.split()]\n",
        "    token_list.append(arr)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "[Martin Luther King Jr. (born Michael King Jr.; January 15, 1929 – April 4, 1968) was an American Baptist minister and activist who became the most visible spokesman and leader in the American civil rights movement from 1955 until his assassination in 1968.,\n",
              " King advanced civil rights through nonviolence and civil disobedience, inspired by his Christian beliefs and the nonviolent activism of Mahatma Gandhi.,\n",
              " He was the son of early civil rights activist and minister Martin Luther King Sr.\n",
              " \n",
              " King participated in and led marches for blacks' right to vote, desegregation, labor rights, and other basic civil rights.[1],\n",
              " King led the 1955 Montgomery bus boycott and later became the first president of the Southern Christian Leadership Conference (SCLC).,\n",
              " As president of the SCLC, he led the unsuccessful Albany Movement in Albany, Georgia, and helped organize some of the nonviolent 1963 protests in Birmingham, Alabama.,\n",
              " King helped organize the 1963 March on Washington, where he delivered his famous \"I Have a Dream\" speech on the steps of the Lincoln Memorial.]"
            ]
          },
          "execution_count": 5,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "#take a look at sentences\n",
        "sentences"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pZ42SFLKtsv_",
        "outputId": "16c28ac8-8349-48ab-f1d3-a9431e658349"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "[[58,\n",
              "  8,\n",
              "  100,\n",
              "  97,\n",
              "  102,\n",
              "  85,\n",
              "  100,\n",
              "  77,\n",
              "  29,\n",
              "  18,\n",
              "  87,\n",
              "  16,\n",
              "  44,\n",
              "  42,\n",
              "  99,\n",
              "  92,\n",
              "  68,\n",
              "  37,\n",
              "  76,\n",
              "  107,\n",
              "  31,\n",
              "  22,\n",
              "  19,\n",
              "  48,\n",
              "  10,\n",
              "  20,\n",
              "  61,\n",
              "  96,\n",
              "  31,\n",
              "  69,\n",
              "  45,\n",
              "  10,\n",
              "  37,\n",
              "  7,\n",
              "  86,\n",
              "  4,\n",
              "  67,\n",
              "  88,\n",
              "  9,\n",
              "  30,\n",
              "  11,\n",
              "  45,\n",
              "  40,\n",
              "  100,\n",
              "  66,\n",
              "  7,\n",
              "  86,\n",
              "  101,\n",
              "  52,\n",
              "  31,\n",
              "  7,\n",
              "  75,\n",
              "  103,\n",
              "  6,\n",
              "  30,\n",
              "  32,\n",
              "  21,\n",
              "  31,\n",
              "  10,\n",
              "  78,\n",
              "  65,\n",
              "  72,\n",
              "  83,\n",
              "  73,\n",
              "  56,\n",
              "  92,\n",
              "  10,\n",
              "  53,\n",
              "  72,\n",
              "  57,\n",
              "  7,\n",
              "  86,\n",
              "  22,\n",
              "  31,\n",
              "  107,\n",
              "  58,\n",
              "  8,\n",
              "  100,\n",
              "  39,\n",
              "  100,\n",
              "  64,\n",
              "  45,\n",
              "  31,\n",
              "  23,\n",
              "  106,\n",
              "  27,\n",
              "  55,\n",
              "  17,\n",
              "  38,\n",
              "  25,\n",
              "  51,\n",
              "  60,\n",
              "  86,\n",
              "  31,\n",
              "  63,\n",
              "  84,\n",
              "  7,\n",
              "  105,\n",
              "  100,\n",
              "  23,\n",
              "  10,\n",
              "  88,\n",
              "  70,\n",
              "  35,\n",
              "  98,\n",
              "  31,\n",
              "  81,\n",
              "  48,\n",
              "  10,\n",
              "  93,\n",
              "  74,\n",
              "  72,\n",
              "  10,\n",
              "  49,\n",
              "  32,\n",
              "  89,\n",
              "  46,\n",
              "  43,\n",
              "  12,\n",
              "  74,\n",
              "  72,\n",
              "  10,\n",
              "  28,\n",
              "  56,\n",
              "  23,\n",
              "  10,\n",
              "  80,\n",
              "  13,\n",
              "  4,\n",
              "  45,\n",
              "  13,\n",
              "  104,\n",
              "  31,\n",
              "  47,\n",
              "  34,\n",
              "  82,\n",
              "  72,\n",
              "  10,\n",
              "  78,\n",
              "  90,\n",
              "  5,\n",
              "  45,\n",
              "  14,\n",
              "  54,\n",
              "  100,\n",
              "  47,\n",
              "  34,\n",
              "  10,\n",
              "  90,\n",
              "  36,\n",
              "  50,\n",
              "  79,\n",
              "  62,\n",
              "  56,\n",
              "  71,\n",
              "  30,\n",
              "  24,\n",
              "  26,\n",
              "  33,\n",
              "  94,\n",
              "  59,\n",
              "  95,\n",
              "  50,\n",
              "  10,\n",
              "  15,\n",
              "  72,\n",
              "  10,\n",
              "  41,\n",
              "  91],\n",
              " [58,\n",
              "  8,\n",
              "  100,\n",
              "  97,\n",
              "  102,\n",
              "  85,\n",
              "  100,\n",
              "  77,\n",
              "  29,\n",
              "  18,\n",
              "  87,\n",
              "  16,\n",
              "  44,\n",
              "  42,\n",
              "  99,\n",
              "  92,\n",
              "  68,\n",
              "  37,\n",
              "  76,\n",
              "  107,\n",
              "  31,\n",
              "  22,\n",
              "  19,\n",
              "  48,\n",
              "  10,\n",
              "  20,\n",
              "  61,\n",
              "  96,\n",
              "  31,\n",
              "  69,\n",
              "  45,\n",
              "  10,\n",
              "  37,\n",
              "  7,\n",
              "  86,\n",
              "  4,\n",
              "  67,\n",
              "  88,\n",
              "  9,\n",
              "  30,\n",
              "  11,\n",
              "  45,\n",
              "  40,\n",
              "  100,\n",
              "  66,\n",
              "  7,\n",
              "  86,\n",
              "  101,\n",
              "  52,\n",
              "  31,\n",
              "  7,\n",
              "  75,\n",
              "  103,\n",
              "  6,\n",
              "  30,\n",
              "  32,\n",
              "  21,\n",
              "  31,\n",
              "  10,\n",
              "  78,\n",
              "  65,\n",
              "  72,\n",
              "  83,\n",
              "  73,\n",
              "  56,\n",
              "  92,\n",
              "  10,\n",
              "  53,\n",
              "  72,\n",
              "  57,\n",
              "  7,\n",
              "  86,\n",
              "  22,\n",
              "  31,\n",
              "  107,\n",
              "  58,\n",
              "  8,\n",
              "  100,\n",
              "  39,\n",
              "  100,\n",
              "  64,\n",
              "  45,\n",
              "  31,\n",
              "  23,\n",
              "  106,\n",
              "  27,\n",
              "  55,\n",
              "  17,\n",
              "  38,\n",
              "  25,\n",
              "  51,\n",
              "  60,\n",
              "  86,\n",
              "  31,\n",
              "  63,\n",
              "  84,\n",
              "  7,\n",
              "  105,\n",
              "  100,\n",
              "  23,\n",
              "  10,\n",
              "  88,\n",
              "  70,\n",
              "  35,\n",
              "  98,\n",
              "  31,\n",
              "  81,\n",
              "  48,\n",
              "  10,\n",
              "  93,\n",
              "  74,\n",
              "  72,\n",
              "  10,\n",
              "  49,\n",
              "  32,\n",
              "  89,\n",
              "  46,\n",
              "  43,\n",
              "  12,\n",
              "  74,\n",
              "  72,\n",
              "  10,\n",
              "  28,\n",
              "  56,\n",
              "  23,\n",
              "  10,\n",
              "  80,\n",
              "  13,\n",
              "  4,\n",
              "  45,\n",
              "  13,\n",
              "  104,\n",
              "  31,\n",
              "  47,\n",
              "  34,\n",
              "  82,\n",
              "  72,\n",
              "  10,\n",
              "  78,\n",
              "  90,\n",
              "  5,\n",
              "  45,\n",
              "  14,\n",
              "  54,\n",
              "  100,\n",
              "  47,\n",
              "  34,\n",
              "  10,\n",
              "  90,\n",
              "  36,\n",
              "  50,\n",
              "  79,\n",
              "  62,\n",
              "  56,\n",
              "  71,\n",
              "  30,\n",
              "  24,\n",
              "  26,\n",
              "  33,\n",
              "  94,\n",
              "  59,\n",
              "  95,\n",
              "  50,\n",
              "  10,\n",
              "  15,\n",
              "  72,\n",
              "  10,\n",
              "  41,\n",
              "  91],\n",
              " [58,\n",
              "  8,\n",
              "  100,\n",
              "  97,\n",
              "  102,\n",
              "  85,\n",
              "  100,\n",
              "  77,\n",
              "  29,\n",
              "  18,\n",
              "  87,\n",
              "  16,\n",
              "  44,\n",
              "  42,\n",
              "  99,\n",
              "  92,\n",
              "  68,\n",
              "  37,\n",
              "  76,\n",
              "  107,\n",
              "  31,\n",
              "  22,\n",
              "  19,\n",
              "  48,\n",
              "  10,\n",
              "  20,\n",
              "  61,\n",
              "  96,\n",
              "  31,\n",
              "  69,\n",
              "  45,\n",
              "  10,\n",
              "  37,\n",
              "  7,\n",
              "  86,\n",
              "  4,\n",
              "  67,\n",
              "  88,\n",
              "  9,\n",
              "  30,\n",
              "  11,\n",
              "  45,\n",
              "  40,\n",
              "  100,\n",
              "  66,\n",
              "  7,\n",
              "  86,\n",
              "  101,\n",
              "  52,\n",
              "  31,\n",
              "  7,\n",
              "  75,\n",
              "  103,\n",
              "  6,\n",
              "  30,\n",
              "  32,\n",
              "  21,\n",
              "  31,\n",
              "  10,\n",
              "  78,\n",
              "  65,\n",
              "  72,\n",
              "  83,\n",
              "  73,\n",
              "  56,\n",
              "  92,\n",
              "  10,\n",
              "  53,\n",
              "  72,\n",
              "  57,\n",
              "  7,\n",
              "  86,\n",
              "  22,\n",
              "  31,\n",
              "  107,\n",
              "  58,\n",
              "  8,\n",
              "  100,\n",
              "  39,\n",
              "  100,\n",
              "  64,\n",
              "  45,\n",
              "  31,\n",
              "  23,\n",
              "  106,\n",
              "  27,\n",
              "  55,\n",
              "  17,\n",
              "  38,\n",
              "  25,\n",
              "  51,\n",
              "  60,\n",
              "  86,\n",
              "  31,\n",
              "  63,\n",
              "  84,\n",
              "  7,\n",
              "  105,\n",
              "  100,\n",
              "  23,\n",
              "  10,\n",
              "  88,\n",
              "  70,\n",
              "  35,\n",
              "  98,\n",
              "  31,\n",
              "  81,\n",
              "  48,\n",
              "  10,\n",
              "  93,\n",
              "  74,\n",
              "  72,\n",
              "  10,\n",
              "  49,\n",
              "  32,\n",
              "  89,\n",
              "  46,\n",
              "  43,\n",
              "  12,\n",
              "  74,\n",
              "  72,\n",
              "  10,\n",
              "  28,\n",
              "  56,\n",
              "  23,\n",
              "  10,\n",
              "  80,\n",
              "  13,\n",
              "  4,\n",
              "  45,\n",
              "  13,\n",
              "  104,\n",
              "  31,\n",
              "  47,\n",
              "  34,\n",
              "  82,\n",
              "  72,\n",
              "  10,\n",
              "  78,\n",
              "  90,\n",
              "  5,\n",
              "  45,\n",
              "  14,\n",
              "  54,\n",
              "  100,\n",
              "  47,\n",
              "  34,\n",
              "  10,\n",
              "  90,\n",
              "  36,\n",
              "  50,\n",
              "  79,\n",
              "  62,\n",
              "  56,\n",
              "  71,\n",
              "  30,\n",
              "  24,\n",
              "  26,\n",
              "  33,\n",
              "  94,\n",
              "  59,\n",
              "  95,\n",
              "  50,\n",
              "  10,\n",
              "  15,\n",
              "  72,\n",
              "  10,\n",
              "  41,\n",
              "  91],\n",
              " [58,\n",
              "  8,\n",
              "  100,\n",
              "  97,\n",
              "  102,\n",
              "  85,\n",
              "  100,\n",
              "  77,\n",
              "  29,\n",
              "  18,\n",
              "  87,\n",
              "  16,\n",
              "  44,\n",
              "  42,\n",
              "  99,\n",
              "  92,\n",
              "  68,\n",
              "  37,\n",
              "  76,\n",
              "  107,\n",
              "  31,\n",
              "  22,\n",
              "  19,\n",
              "  48,\n",
              "  10,\n",
              "  20,\n",
              "  61,\n",
              "  96,\n",
              "  31,\n",
              "  69,\n",
              "  45,\n",
              "  10,\n",
              "  37,\n",
              "  7,\n",
              "  86,\n",
              "  4,\n",
              "  67,\n",
              "  88,\n",
              "  9,\n",
              "  30,\n",
              "  11,\n",
              "  45,\n",
              "  40,\n",
              "  100,\n",
              "  66,\n",
              "  7,\n",
              "  86,\n",
              "  101,\n",
              "  52,\n",
              "  31,\n",
              "  7,\n",
              "  75,\n",
              "  103,\n",
              "  6,\n",
              "  30,\n",
              "  32,\n",
              "  21,\n",
              "  31,\n",
              "  10,\n",
              "  78,\n",
              "  65,\n",
              "  72,\n",
              "  83,\n",
              "  73,\n",
              "  56,\n",
              "  92,\n",
              "  10,\n",
              "  53,\n",
              "  72,\n",
              "  57,\n",
              "  7,\n",
              "  86,\n",
              "  22,\n",
              "  31,\n",
              "  107,\n",
              "  58,\n",
              "  8,\n",
              "  100,\n",
              "  39,\n",
              "  100,\n",
              "  64,\n",
              "  45,\n",
              "  31,\n",
              "  23,\n",
              "  106,\n",
              "  27,\n",
              "  55,\n",
              "  17,\n",
              "  38,\n",
              "  25,\n",
              "  51,\n",
              "  60,\n",
              "  86,\n",
              "  31,\n",
              "  63,\n",
              "  84,\n",
              "  7,\n",
              "  105,\n",
              "  100,\n",
              "  23,\n",
              "  10,\n",
              "  88,\n",
              "  70,\n",
              "  35,\n",
              "  98,\n",
              "  31,\n",
              "  81,\n",
              "  48,\n",
              "  10,\n",
              "  93,\n",
              "  74,\n",
              "  72,\n",
              "  10,\n",
              "  49,\n",
              "  32,\n",
              "  89,\n",
              "  46,\n",
              "  43,\n",
              "  12,\n",
              "  74,\n",
              "  72,\n",
              "  10,\n",
              "  28,\n",
              "  56,\n",
              "  23,\n",
              "  10,\n",
              "  80,\n",
              "  13,\n",
              "  4,\n",
              "  45,\n",
              "  13,\n",
              "  104,\n",
              "  31,\n",
              "  47,\n",
              "  34,\n",
              "  82,\n",
              "  72,\n",
              "  10,\n",
              "  78,\n",
              "  90,\n",
              "  5,\n",
              "  45,\n",
              "  14,\n",
              "  54,\n",
              "  100,\n",
              "  47,\n",
              "  34,\n",
              "  10,\n",
              "  90,\n",
              "  36,\n",
              "  50,\n",
              "  79,\n",
              "  62,\n",
              "  56,\n",
              "  71,\n",
              "  30,\n",
              "  24,\n",
              "  26,\n",
              "  33,\n",
              "  94,\n",
              "  59,\n",
              "  95,\n",
              "  50,\n",
              "  10,\n",
              "  15,\n",
              "  72,\n",
              "  10,\n",
              "  41,\n",
              "  91],\n",
              " [58,\n",
              "  8,\n",
              "  100,\n",
              "  97,\n",
              "  102,\n",
              "  85,\n",
              "  100,\n",
              "  77,\n",
              "  29,\n",
              "  18,\n",
              "  87,\n",
              "  16,\n",
              "  44,\n",
              "  42,\n",
              "  99,\n",
              "  92,\n",
              "  68,\n",
              "  37,\n",
              "  76,\n",
              "  107,\n",
              "  31,\n",
              "  22,\n",
              "  19,\n",
              "  48,\n",
              "  10,\n",
              "  20,\n",
              "  61,\n",
              "  96,\n",
              "  31,\n",
              "  69,\n",
              "  45,\n",
              "  10,\n",
              "  37,\n",
              "  7,\n",
              "  86,\n",
              "  4,\n",
              "  67,\n",
              "  88,\n",
              "  9,\n",
              "  30,\n",
              "  11,\n",
              "  45,\n",
              "  40,\n",
              "  100,\n",
              "  66,\n",
              "  7,\n",
              "  86,\n",
              "  101,\n",
              "  52,\n",
              "  31,\n",
              "  7,\n",
              "  75,\n",
              "  103,\n",
              "  6,\n",
              "  30,\n",
              "  32,\n",
              "  21,\n",
              "  31,\n",
              "  10,\n",
              "  78,\n",
              "  65,\n",
              "  72,\n",
              "  83,\n",
              "  73,\n",
              "  56,\n",
              "  92,\n",
              "  10,\n",
              "  53,\n",
              "  72,\n",
              "  57,\n",
              "  7,\n",
              "  86,\n",
              "  22,\n",
              "  31,\n",
              "  107,\n",
              "  58,\n",
              "  8,\n",
              "  100,\n",
              "  39,\n",
              "  100,\n",
              "  64,\n",
              "  45,\n",
              "  31,\n",
              "  23,\n",
              "  106,\n",
              "  27,\n",
              "  55,\n",
              "  17,\n",
              "  38,\n",
              "  25,\n",
              "  51,\n",
              "  60,\n",
              "  86,\n",
              "  31,\n",
              "  63,\n",
              "  84,\n",
              "  7,\n",
              "  105,\n",
              "  100,\n",
              "  23,\n",
              "  10,\n",
              "  88,\n",
              "  70,\n",
              "  35,\n",
              "  98,\n",
              "  31,\n",
              "  81,\n",
              "  48,\n",
              "  10,\n",
              "  93,\n",
              "  74,\n",
              "  72,\n",
              "  10,\n",
              "  49,\n",
              "  32,\n",
              "  89,\n",
              "  46,\n",
              "  43,\n",
              "  12,\n",
              "  74,\n",
              "  72,\n",
              "  10,\n",
              "  28,\n",
              "  56,\n",
              "  23,\n",
              "  10,\n",
              "  80,\n",
              "  13,\n",
              "  4,\n",
              "  45,\n",
              "  13,\n",
              "  104,\n",
              "  31,\n",
              "  47,\n",
              "  34,\n",
              "  82,\n",
              "  72,\n",
              "  10,\n",
              "  78,\n",
              "  90,\n",
              "  5,\n",
              "  45,\n",
              "  14,\n",
              "  54,\n",
              "  100,\n",
              "  47,\n",
              "  34,\n",
              "  10,\n",
              "  90,\n",
              "  36,\n",
              "  50,\n",
              "  79,\n",
              "  62,\n",
              "  56,\n",
              "  71,\n",
              "  30,\n",
              "  24,\n",
              "  26,\n",
              "  33,\n",
              "  94,\n",
              "  59,\n",
              "  95,\n",
              "  50,\n",
              "  10,\n",
              "  15,\n",
              "  72,\n",
              "  10,\n",
              "  41,\n",
              "  91],\n",
              " [58,\n",
              "  8,\n",
              "  100,\n",
              "  97,\n",
              "  102,\n",
              "  85,\n",
              "  100,\n",
              "  77,\n",
              "  29,\n",
              "  18,\n",
              "  87,\n",
              "  16,\n",
              "  44,\n",
              "  42,\n",
              "  99,\n",
              "  92,\n",
              "  68,\n",
              "  37,\n",
              "  76,\n",
              "  107,\n",
              "  31,\n",
              "  22,\n",
              "  19,\n",
              "  48,\n",
              "  10,\n",
              "  20,\n",
              "  61,\n",
              "  96,\n",
              "  31,\n",
              "  69,\n",
              "  45,\n",
              "  10,\n",
              "  37,\n",
              "  7,\n",
              "  86,\n",
              "  4,\n",
              "  67,\n",
              "  88,\n",
              "  9,\n",
              "  30,\n",
              "  11,\n",
              "  45,\n",
              "  40,\n",
              "  100,\n",
              "  66,\n",
              "  7,\n",
              "  86,\n",
              "  101,\n",
              "  52,\n",
              "  31,\n",
              "  7,\n",
              "  75,\n",
              "  103,\n",
              "  6,\n",
              "  30,\n",
              "  32,\n",
              "  21,\n",
              "  31,\n",
              "  10,\n",
              "  78,\n",
              "  65,\n",
              "  72,\n",
              "  83,\n",
              "  73,\n",
              "  56,\n",
              "  92,\n",
              "  10,\n",
              "  53,\n",
              "  72,\n",
              "  57,\n",
              "  7,\n",
              "  86,\n",
              "  22,\n",
              "  31,\n",
              "  107,\n",
              "  58,\n",
              "  8,\n",
              "  100,\n",
              "  39,\n",
              "  100,\n",
              "  64,\n",
              "  45,\n",
              "  31,\n",
              "  23,\n",
              "  106,\n",
              "  27,\n",
              "  55,\n",
              "  17,\n",
              "  38,\n",
              "  25,\n",
              "  51,\n",
              "  60,\n",
              "  86,\n",
              "  31,\n",
              "  63,\n",
              "  84,\n",
              "  7,\n",
              "  105,\n",
              "  100,\n",
              "  23,\n",
              "  10,\n",
              "  88,\n",
              "  70,\n",
              "  35,\n",
              "  98,\n",
              "  31,\n",
              "  81,\n",
              "  48,\n",
              "  10,\n",
              "  93,\n",
              "  74,\n",
              "  72,\n",
              "  10,\n",
              "  49,\n",
              "  32,\n",
              "  89,\n",
              "  46,\n",
              "  43,\n",
              "  12,\n",
              "  74,\n",
              "  72,\n",
              "  10,\n",
              "  28,\n",
              "  56,\n",
              "  23,\n",
              "  10,\n",
              "  80,\n",
              "  13,\n",
              "  4,\n",
              "  45,\n",
              "  13,\n",
              "  104,\n",
              "  31,\n",
              "  47,\n",
              "  34,\n",
              "  82,\n",
              "  72,\n",
              "  10,\n",
              "  78,\n",
              "  90,\n",
              "  5,\n",
              "  45,\n",
              "  14,\n",
              "  54,\n",
              "  100,\n",
              "  47,\n",
              "  34,\n",
              "  10,\n",
              "  90,\n",
              "  36,\n",
              "  50,\n",
              "  79,\n",
              "  62,\n",
              "  56,\n",
              "  71,\n",
              "  30,\n",
              "  24,\n",
              "  26,\n",
              "  33,\n",
              "  94,\n",
              "  59,\n",
              "  95,\n",
              "  50,\n",
              "  10,\n",
              "  15,\n",
              "  72,\n",
              "  10,\n",
              "  41,\n",
              "  91]]"
            ]
          },
          "execution_count": 6,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "#take a look at token_list\n",
        "token_list"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "martin\n",
            "luther\n",
            "king\n",
            "jr\n",
            "(born\n",
            "michael\n",
            "king\n",
            "jr;\n",
            "january\n",
            "15\n",
            "1929\n",
            "–\n",
            "april\n",
            "4\n",
            "1968)\n",
            "was\n",
            "an\n",
            "american\n",
            "baptist\n",
            "minister\n",
            "and\n",
            "activist\n",
            "who\n",
            "became\n",
            "the\n",
            "most\n",
            "visible\n",
            "spokesman\n",
            "and\n",
            "leader\n",
            "in\n",
            "the\n",
            "american\n",
            "civil\n",
            "rights\n",
            "movement\n",
            "from\n",
            "1955\n",
            "until\n",
            "his\n",
            "assassination\n",
            "in\n",
            "1968\n",
            "king\n",
            "advanced\n",
            "civil\n",
            "rights\n",
            "through\n",
            "nonviolence\n",
            "and\n",
            "civil\n",
            "disobedience\n",
            "inspired\n",
            "by\n",
            "his\n",
            "christian\n",
            "beliefs\n",
            "and\n",
            "the\n",
            "nonviolent\n",
            "activism\n",
            "of\n",
            "mahatma\n",
            "gandhi\n",
            "he\n",
            "was\n",
            "the\n",
            "son\n",
            "of\n",
            "early\n",
            "civil\n",
            "rights\n",
            "activist\n",
            "and\n",
            "minister\n",
            "martin\n",
            "luther\n",
            "king\n",
            "sr\n",
            "king\n",
            "participated\n",
            "in\n",
            "and\n",
            "led\n",
            "marches\n",
            "for\n",
            "blacks'\n",
            "right\n",
            "to\n",
            "vote\n",
            "desegregation\n",
            "labor\n",
            "rights\n",
            "and\n",
            "other\n",
            "basic\n",
            "civil\n",
            "rights[1]\n",
            "king\n",
            "led\n",
            "the\n",
            "1955\n",
            "montgomery\n",
            "bus\n",
            "boycott\n",
            "and\n",
            "later\n",
            "became\n",
            "the\n",
            "first\n",
            "president\n",
            "of\n",
            "the\n",
            "southern\n",
            "christian\n",
            "leadership\n",
            "conference\n",
            "(sclc)\n",
            "as\n",
            "president\n",
            "of\n",
            "the\n",
            "sclc\n",
            "he\n",
            "led\n",
            "the\n",
            "unsuccessful\n",
            "albany\n",
            "movement\n",
            "in\n",
            "albany\n",
            "georgia\n",
            "and\n",
            "helped\n",
            "organize\n",
            "some\n",
            "of\n",
            "the\n",
            "nonviolent\n",
            "1963\n",
            "protests\n",
            "in\n",
            "birmingham\n",
            "alabama\n",
            "king\n",
            "helped\n",
            "organize\n",
            "the\n",
            "1963\n",
            "march\n",
            "on\n",
            "washington\n",
            "where\n",
            "he\n",
            "delivered\n",
            "his\n",
            "famous\n",
            "\"i\n",
            "have\n",
            "a\n",
            "dream\"\n",
            "speech\n",
            "on\n",
            "the\n",
            "steps\n",
            "of\n",
            "the\n",
            "lincoln\n",
            "memorial\n"
          ]
        }
      ],
      "source": [
        "#testing one sentence\n",
        "for tokens in token_list[0]:\n",
        "    print(id2word[tokens])"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 2. Data loader\n",
        "\n",
        "We gonna make dataloader.  Inside here, we need to make two types of embeddings: **token embedding** and **segment embedding**\n",
        "\n",
        "1. **Token embedding** - Given “The cat is walking. The dog is barking”, we add [CLS] and [SEP] >> “[CLS] the cat is walking [SEP] the dog is barking”. \n",
        "\n",
        "2. **Segment embedding**\n",
        "A segment embedding separates two sentences, i.e., [0 0 0 0 1 1 1 1 ]\n",
        "\n",
        "3. **Masking**\n",
        "As mentioned in the original paper, BERT randomly assigns masks to 15% of the sequence. In this 15%, 80% is replaced with masks, while 10% is replaced with random tokens, and the rest 10% is left as is.  Here we specified `max_pred` \n",
        "\n",
        "4. **Padding**\n",
        "Once we mask, we will add padding. For simplicity, here we padded until some specified `max_len`. \n",
        "\n",
        "Note:  `positive` and `negative` are just simply counts to keep track of the batch size.  `positive` refers to two sentences that are really next to one another."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {},
      "outputs": [],
      "source": [
        "batch_size = 6\n",
        "max_mask   = 5  # max masked tokens when 15% exceed, it will only be max_pred\n",
        "max_len    = 1000 # maximum of length to be padded; "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "id": "TtyOOmRntu8w"
      },
      "outputs": [],
      "source": [
        "def make_batch():\n",
        "    batch = []\n",
        "    positive = negative = 0  #count of batch size;  we want to have half batch that are positive pairs (i.e., next sentence pairs)\n",
        "    while positive != batch_size/2 or negative != batch_size/2:\n",
        "        \n",
        "        #randomly choose two sentence so we can put [SEP]\n",
        "        tokens_a_index, tokens_b_index= randrange(len(sentences)), randrange(len(sentences))\n",
        "        #retrieve the two sentences\n",
        "        tokens_a, tokens_b= token_list[tokens_a_index], token_list[tokens_b_index]\n",
        "\n",
        "        #1. token embedding - append CLS and SEP\n",
        "        input_ids = [word2id['[CLS]']] + tokens_a + [word2id['[SEP]']] + tokens_b + [word2id['[SEP]']]\n",
        "\n",
        "        #2. segment embedding - [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1]\n",
        "        segment_ids = [0] * (1 + len(tokens_a) + 1) + [1] * (len(tokens_b) + 1)\n",
        "\n",
        "        #3. mask language modeling\n",
        "        #masked 15%, but should be at least 1 but does not exceed max_mask\n",
        "        n_pred =  min(max_mask, max(1, int(round(len(input_ids) * 0.15))))\n",
        "        #get the pos that excludes CLS and SEP and shuffle them\n",
        "        cand_maked_pos = [i for i, token in enumerate(input_ids) if token != word2id['[CLS]'] and token != word2id['[SEP]']]\n",
        "        shuffle(cand_maked_pos)\n",
        "        masked_tokens, masked_pos = [], []\n",
        "        #simply loop and change the input_ids to [MASK]\n",
        "        for pos in cand_maked_pos[:n_pred]:\n",
        "            masked_pos.append(pos)  #remember the position\n",
        "            masked_tokens.append(input_ids[pos]) #remember the tokens\n",
        "            #80% replace with a [MASK], but 10% will replace with a random token\n",
        "            if random() < 0.1:  # 10%\n",
        "                index = randint(0, vocab_size - 1) # random index in vocabulary\n",
        "                input_ids[pos] = word2id[id2word[index]] # replace\n",
        "            elif random() < 0.9:  # 80%\n",
        "                input_ids[pos] = word2id['[MASK]'] # make mask\n",
        "            else:  #10% do nothing\n",
        "                pass\n",
        "\n",
        "        # pad the input_ids and segment ids until the max len\n",
        "        n_pad = max_len - len(input_ids)\n",
        "        input_ids.extend([0] * n_pad)\n",
        "        segment_ids.extend([0] * n_pad)\n",
        "\n",
        "        # pad the masked_tokens and masked_pos to make sure the lenth is max_mask\n",
        "        if max_mask > n_pred:\n",
        "            n_pad = max_mask - n_pred\n",
        "            masked_tokens.extend([0] * n_pad)\n",
        "            masked_pos.extend([0] * n_pad)\n",
        "\n",
        "        #check if first sentence is really comes before the second sentence\n",
        "        #also make sure positive is exactly half the batch size\n",
        "        if tokens_a_index + 1 == tokens_b_index and positive < batch_size / 2:\n",
        "            batch.append([input_ids, segment_ids, masked_tokens, masked_pos, True]) # IsNext\n",
        "            positive += 1\n",
        "        elif tokens_a_index + 1 != tokens_b_index and negative < batch_size/2:\n",
        "            batch.append([input_ids, segment_ids, masked_tokens, masked_pos, False]) # NotNext\n",
        "            negative += 1\n",
        "            \n",
        "    return batch"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "id": "Q7_HC-Y0jC3K"
      },
      "outputs": [],
      "source": [
        "batch = make_batch()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "6"
            ]
          },
          "execution_count": 11,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "#len of batch\n",
        "len(batch)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "(torch.Size([6, 1000]),\n",
              " torch.Size([6, 1000]),\n",
              " torch.Size([6, 5]),\n",
              " torch.Size([6, 5]),\n",
              " torch.Size([6]))"
            ]
          },
          "execution_count": 12,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "#we can deconstruct using map and zip\n",
        "input_ids, segment_ids, masked_tokens, masked_pos, isNext = map(torch.LongTensor, zip(*batch))\n",
        "input_ids.shape, segment_ids.shape, masked_tokens.shape, masked_pos.shape, isNext.shape"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 3. Model\n",
        "\n",
        "BERT has the following components:\n",
        "\n",
        "- Embedding layers\n",
        "- Attention Mask\n",
        "- Encoder layer\n",
        "- Multi-head attention\n",
        "- Scaled dot product attention\n",
        "- Position-wise feed-forward network\n",
        "- BERT (assembling all the components)"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 3.1 Embedding\n",
        "\n",
        "Here we simply generate the positional embedding, and sum the token embedding, positional embedding, and segment embedding together.\n",
        "\n",
        "<img src = \"figures/BERT_embed.png\" width=500>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {},
      "outputs": [],
      "source": [
        "class Embedding(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(Embedding, self).__init__()\n",
        "        self.tok_embed = nn.Embedding(vocab_size, d_model)  # token embedding\n",
        "        self.pos_embed = nn.Embedding(max_len, d_model)      # position embedding\n",
        "        self.seg_embed = nn.Embedding(n_segments, d_model)  # segment(token type) embedding\n",
        "        self.norm = nn.LayerNorm(d_model)\n",
        "\n",
        "    def forward(self, x, seg):\n",
        "        #x, seg: (bs, len)\n",
        "        seq_len = x.size(1)\n",
        "        pos = torch.arange(seq_len, dtype=torch.long)\n",
        "        pos = pos.unsqueeze(0).expand_as(x)  # (len,) -> (bs, len)\n",
        "        embedding = self.tok_embed(x) + self.pos_embed(pos) + self.seg_embed(seg)\n",
        "        return self.norm(embedding)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 3.2 Attention mask"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "id": "s1PGksqBNuZM"
      },
      "outputs": [],
      "source": [
        "def get_attn_pad_mask(seq_q, seq_k):\n",
        "    batch_size, len_q = seq_q.size()\n",
        "    batch_size, len_k = seq_k.size()\n",
        "    # eq(zero) is PAD token\n",
        "    pad_attn_mask = seq_k.data.eq(0).unsqueeze(1)  # batch_size x 1 x len_k(=len_q), one is masking\n",
        "    return pad_attn_mask.expand(batch_size, len_q, len_k)  # batch_size x len_q x len_k"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Testing the attention mask"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "torch.Size([6, 1000, 1000])\n"
          ]
        }
      ],
      "source": [
        "print(get_attn_pad_mask(input_ids, input_ids).shape)"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 3.3 Encoder\n",
        "\n",
        "The encoder has two main components: \n",
        "\n",
        "- Multi-head Attention\n",
        "- Position-wise feed-forward network\n",
        "\n",
        "First let's make the wrapper called `EncoderLayer`"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {},
      "outputs": [],
      "source": [
        "class EncoderLayer(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(EncoderLayer, self).__init__()\n",
        "        self.enc_self_attn = MultiHeadAttention()\n",
        "        self.pos_ffn       = PoswiseFeedForwardNet()\n",
        "\n",
        "    def forward(self, enc_inputs, enc_self_attn_mask):\n",
        "        enc_outputs, attn = self.enc_self_attn(enc_inputs, enc_inputs, enc_inputs, enc_self_attn_mask) # enc_inputs to same Q,K,V\n",
        "        enc_outputs = self.pos_ffn(enc_outputs) # enc_outputs: [batch_size x len_q x d_model]\n",
        "        return enc_outputs, attn"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Let's define the scaled dot attention, to be used inside the multihead attention"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {},
      "outputs": [],
      "source": [
        "class ScaledDotProductAttention(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(ScaledDotProductAttention, self).__init__()\n",
        "\n",
        "    def forward(self, Q, K, V, attn_mask):\n",
        "        scores = torch.matmul(Q, K.transpose(-1, -2)) / np.sqrt(d_k) # scores : [batch_size x n_heads x len_q(=len_k) x len_k(=len_q)]\n",
        "        scores.masked_fill_(attn_mask, -1e9) # Fills elements of self tensor with value where mask is one.\n",
        "        attn = nn.Softmax(dim=-1)(scores)\n",
        "        context = torch.matmul(attn, V)\n",
        "        return context, attn "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Let's define the parameters first"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {},
      "outputs": [],
      "source": [
        "n_layers = 6    # number of Encoder of Encoder Layer\n",
        "n_heads  = 8    # number of heads in Multi-Head Attention\n",
        "d_model  = 768  # Embedding Size\n",
        "d_ff = 768 * 4  # 4*d_model, FeedForward dimension\n",
        "d_k = d_v = 64  # dimension of K(=Q), V\n",
        "n_segments = 2"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Here is the Multiheadattention."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {},
      "outputs": [],
      "source": [
        "class MultiHeadAttention(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(MultiHeadAttention, self).__init__()\n",
        "        self.W_Q = nn.Linear(d_model, d_k * n_heads)\n",
        "        self.W_K = nn.Linear(d_model, d_k * n_heads)\n",
        "        self.W_V = nn.Linear(d_model, d_v * n_heads)\n",
        "    def forward(self, Q, K, V, attn_mask):\n",
        "        # q: [batch_size x len_q x d_model], k: [batch_size x len_k x d_model], v: [batch_size x len_k x d_model]\n",
        "        residual, batch_size = Q, Q.size(0)\n",
        "        # (B, S, D) -proj-> (B, S, D) -split-> (B, S, H, W) -trans-> (B, H, S, W)\n",
        "        q_s = self.W_Q(Q).view(batch_size, -1, n_heads, d_k).transpose(1,2)  # q_s: [batch_size x n_heads x len_q x d_k]\n",
        "        k_s = self.W_K(K).view(batch_size, -1, n_heads, d_k).transpose(1,2)  # k_s: [batch_size x n_heads x len_k x d_k]\n",
        "        v_s = self.W_V(V).view(batch_size, -1, n_heads, d_v).transpose(1,2)  # v_s: [batch_size x n_heads x len_k x d_v]\n",
        "\n",
        "        attn_mask = attn_mask.unsqueeze(1).repeat(1, n_heads, 1, 1) # attn_mask : [batch_size x n_heads x len_q x len_k]\n",
        "\n",
        "        # context: [batch_size x n_heads x len_q x d_v], attn: [batch_size x n_heads x len_q(=len_k) x len_k(=len_q)]\n",
        "        context, attn = ScaledDotProductAttention()(q_s, k_s, v_s, attn_mask)\n",
        "        context = context.transpose(1, 2).contiguous().view(batch_size, -1, n_heads * d_v) # context: [batch_size x len_q x n_heads * d_v]\n",
        "        output = nn.Linear(n_heads * d_v, d_model)(context)\n",
        "        return nn.LayerNorm(d_model)(output + residual), attn # output: [batch_size x len_q x d_model]\n"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Here is the PoswiseFeedForwardNet."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "metadata": {},
      "outputs": [],
      "source": [
        "class PoswiseFeedForwardNet(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(PoswiseFeedForwardNet, self).__init__()\n",
        "        self.fc1 = nn.Linear(d_model, d_ff)\n",
        "        self.fc2 = nn.Linear(d_ff, d_model)\n",
        "\n",
        "    def forward(self, x):\n",
        "        # (batch_size, len_seq, d_model) -> (batch_size, len_seq, d_ff) -> (batch_size, len_seq, d_model)\n",
        "        return self.fc2(F.gelu(self.fc1(x)))\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 3.4 Putting them together"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "metadata": {
        "id": "OZ0TJ84W4SZw"
      },
      "outputs": [],
      "source": [
        "class BERT(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(BERT, self).__init__()\n",
        "        self.embedding = Embedding()\n",
        "        self.layers = nn.ModuleList([EncoderLayer() for _ in range(n_layers)])\n",
        "        self.fc = nn.Linear(d_model, d_model)\n",
        "        self.activ = nn.Tanh()\n",
        "        self.linear = nn.Linear(d_model, d_model)\n",
        "        self.norm = nn.LayerNorm(d_model)\n",
        "        self.classifier = nn.Linear(d_model, 2)\n",
        "        # decoder is shared with embedding layer\n",
        "        embed_weight = self.embedding.tok_embed.weight\n",
        "        n_vocab, n_dim = embed_weight.size()\n",
        "        self.decoder = nn.Linear(n_dim, n_vocab, bias=False)\n",
        "        self.decoder.weight = embed_weight\n",
        "        self.decoder_bias = nn.Parameter(torch.zeros(n_vocab))\n",
        "\n",
        "    def forward(self, input_ids, segment_ids, masked_pos):\n",
        "        output = self.embedding(input_ids, segment_ids)\n",
        "        enc_self_attn_mask = get_attn_pad_mask(input_ids, input_ids)\n",
        "        for layer in self.layers:\n",
        "            output, enc_self_attn = layer(output, enc_self_attn_mask)\n",
        "        # output : [batch_size, len, d_model], attn : [batch_size, n_heads, d_mode, d_model]\n",
        "        \n",
        "        # 1. predict next sentence\n",
        "        # it will be decided by first token(CLS)\n",
        "        h_pooled   = self.activ(self.fc(output[:, 0])) # [batch_size, d_model]\n",
        "        logits_nsp = self.classifier(h_pooled) # [batch_size, 2]\n",
        "\n",
        "        # 2. predict the masked token\n",
        "        masked_pos = masked_pos[:, :, None].expand(-1, -1, output.size(-1)) # [batch_size, max_pred, d_model]\n",
        "        h_masked = torch.gather(output, 1, masked_pos) # masking position [batch_size, max_pred, d_model]\n",
        "        h_masked  = self.norm(F.gelu(self.linear(h_masked)))\n",
        "        logits_lm = self.decoder(h_masked) + self.decoder_bias # [batch_size, max_pred, n_vocab]\n",
        "\n",
        "        return logits_lm, logits_nsp"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 4. Training"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 22,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8UAG3SEP4UbU",
        "outputId": "bc6f202f-df37-4fac-843c-fb86bdb777b2"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch: 00 cost = 72.510841\n",
            "Epoch: 100 cost = 5.921447\n",
            "Epoch: 200 cost = 4.454564\n",
            "Epoch: 300 cost = 3.973870\n",
            "Epoch: 400 cost = 3.750157\n"
          ]
        }
      ],
      "source": [
        "num_epoch = 500\n",
        "model = BERT()\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
        "\n",
        "batch = make_batch()\n",
        "input_ids, segment_ids, masked_tokens, masked_pos, isNext = map(torch.LongTensor, zip(*batch))\n",
        "\n",
        "for epoch in range(num_epoch):\n",
        "    optimizer.zero_grad()\n",
        "    logits_lm, logits_nsp = model(input_ids, segment_ids, masked_pos)    \n",
        "    #logits_lm: (bs, max_mask, vocab_size) ==> (6, 5, 34)\n",
        "    #logits_nsp: (bs, yes/no) ==> (6, 2)\n",
        "\n",
        "    #1. mlm loss\n",
        "    #logits_lm.transpose: (bs, vocab_size, max_mask) vs. masked_tokens: (bs, max_mask)\n",
        "    loss_lm = criterion(logits_lm.transpose(1, 2), masked_tokens) # for masked LM\n",
        "    loss_lm = (loss_lm.float()).mean()\n",
        "    #2. nsp loss\n",
        "    #logits_nsp: (bs, 2) vs. isNext: (bs, )\n",
        "    loss_nsp = criterion(logits_nsp, isNext) # for sentence classification\n",
        "    \n",
        "    #3. combine loss\n",
        "    loss = loss_lm + loss_nsp\n",
        "    if epoch % 100 == 0:\n",
        "        print('Epoch:', '%02d' % (epoch), 'loss =', '{:.6f}'.format(loss))\n",
        "    loss.backward()\n",
        "    optimizer.step()"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 5. Inference\n",
        "\n",
        "Since our dataset is very small, it won't work very well, but just for the sake of demonstration."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 24,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "uD3K8T6B4YJp",
        "outputId": "e057405b-1f78-431c-fa71-032a738fb848"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "['[CLS]', 'martin', 'luther', '[MASK]', 'jr', '(born', 'michael', 'king', 'jr;', 'january', '15', '[MASK]', '–', 'april', '4', '1968)', 'was', 'an', 'american', 'baptist', 'minister', 'and', 'activist', 'who', 'became', 'the', 'most', 'visible', 'spokesman', 'and', 'leader', 'in', 'the', 'american', 'civil', 'rights', 'movement', 'from', '1955', 'until', 'his', 'assassination', 'in', '1968', 'king', 'advanced', 'civil', 'rights', 'through', 'nonviolence', 'and', 'civil', 'disobedience', 'inspired', 'by', 'his', 'christian', 'beliefs', 'and', 'the', 'nonviolent', 'activism', 'of', 'mahatma', 'gandhi', 'he', 'was', 'the', 'son', 'of', 'early', 'civil', 'rights', 'activist', 'and', 'minister', 'martin', 'luther', 'king', 'sr', 'king', 'participated', 'in', 'and', 'led', 'marches', 'for', \"blacks'\", 'right', 'to', 'vote', 'desegregation', 'labor', 'rights', 'and', 'other', 'basic', 'civil', 'rights[1]', 'king', 'led', 'the', '1955', 'montgomery', 'bus', 'boycott', 'and', 'later', 'became', 'the', 'first', 'president', 'of', 'the', 'southern', 'christian', 'leadership', 'conference', '(sclc)', 'as', 'president', 'of', 'the', 'sclc', 'he', 'led', 'the', 'unsuccessful', 'albany', 'movement', 'in', 'albany', 'georgia', 'and', 'helped', 'organize', 'some', 'of', 'the', 'nonviolent', '1963', 'protests', 'in', 'birmingham', 'alabama', 'king', 'helped', 'organize', 'the', '1963', 'march', 'on', 'washington', 'where', 'he', 'delivered', 'his', 'famous', '\"i', 'have', 'a', 'dream\"', 'speech', 'on', 'the', 'steps', 'of', 'the', 'lincoln', 'memorial', '[SEP]', 'martin', 'luther', 'king', 'jr', '(born', 'michael', 'king', 'jr;', 'january', '15', '1929', '–', 'april', '4', '1968)', 'was', '[MASK]', 'american', 'baptist', 'minister', 'and', 'activist', 'who', 'became', 'the', 'most', 'visible', 'spokesman', 'and', 'leader', 'in', 'the', 'american', 'civil', 'rights', 'movement', 'from', '1955', 'until', 'his', 'assassination', 'in', '1968', 'king', 'advanced', 'civil', 'rights', 'through', 'nonviolence', 'and', 'civil', 'disobedience', 'inspired', 'by', 'his', 'christian', 'beliefs', 'and', 'the', 'nonviolent', 'activism', 'of', 'mahatma', 'gandhi', 'he', 'was', 'the', 'son', 'of', 'early', 'civil', '[MASK]', 'activist', 'and', 'minister', 'martin', 'luther', 'king', 'sr', 'king', 'participated', 'in', 'and', 'led', 'marches', 'for', \"blacks'\", 'right', 'to', 'vote', 'desegregation', 'labor', 'rights', 'and', 'other', 'basic', 'civil', 'rights[1]', 'king', 'led', 'the', '1955', 'montgomery', 'bus', 'boycott', 'and', 'later', 'became', 'the', 'first', 'president', 'of', 'the', 'southern', 'christian', 'leadership', 'conference', '(sclc)', 'as', 'president', 'of', 'the', 'sclc', 'he', 'led', 'the', 'unsuccessful', 'albany', 'movement', 'in', 'albany', 'georgia', 'and', 'helped', 'organize', 'some', 'of', 'the', 'nonviolent', '1963', 'protests', 'in', 'birmingham', 'alabama', 'king', 'helped', 'organize', 'the', '1963', 'march', 'on', 'washington', 'where', 'he', 'delivered', 'his', 'famous', '\"i', 'have', 'a', 'dream\"', 'speech', 'on', 'the', 'steps', 'of', 'the', 'lincoln', 'memorial', '[SEP]']\n",
            "masked tokens (words) :  ['an', 'and', 'rights', '1929', 'king']\n",
            "masked tokens list :  [68, 31, 86, 87, 100]\n",
            "masked tokens (words) :  ['and', 'and', 'and', 'and', 'and']\n",
            "predict masked tokens list :  [31, 31, 31, 31, 31]\n",
            "0\n",
            "isNext :  False\n",
            "predict isNext :  False\n"
          ]
        }
      ],
      "source": [
        "# Predict mask tokens ans isNext\n",
        "input_ids, segment_ids, masked_tokens, masked_pos, isNext = map(torch.LongTensor, zip(batch[2]))\n",
        "print([id2word[w.item()] for w in input_ids[0] if id2word[w.item()] != '[PAD]'])\n",
        "\n",
        "logits_lm, logits_nsp = model(input_ids, segment_ids, masked_pos)\n",
        "#logits_lm:  (1, max_mask, vocab_size) ==> (1, 5, 34)\n",
        "#logits_nsp: (1, yes/no) ==> (1, 2)\n",
        "\n",
        "#predict masked tokens\n",
        "#max the probability along the vocab dim (2), [1] is the indices of the max, and [0] is the first value\n",
        "logits_lm = logits_lm.data.max(2)[1][0].data.numpy() \n",
        "#note that zero is padding we add to the masked_tokens\n",
        "print('masked tokens (words) : ',[id2word[pos.item()] for pos in masked_tokens[0]])\n",
        "print('masked tokens list : ',[pos.item() for pos in masked_tokens[0]])\n",
        "print('masked tokens (words) : ',[id2word[pos.item()] for pos in logits_lm])\n",
        "print('predict masked tokens list : ', [pos for pos in logits_lm])\n",
        "\n",
        "#predict nsp\n",
        "logits_nsp = logits_nsp.data.max(1)[1][0].data.numpy()\n",
        "print(logits_nsp)\n",
        "print('isNext : ', True if isNext else False)\n",
        "print('predict isNext : ',True if logits_nsp else False)"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.6"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
