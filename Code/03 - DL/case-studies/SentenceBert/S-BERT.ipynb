{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# [Sentence-BERT](https://arxiv.org/pdf/1908.10084.pdf)\n",
    "\n",
    "[Reference Code](https://www.pinecone.io/learn/series/nlp/train-sentence-transformers-softmax/)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import datasets\n",
    "\n",
    "snli = datasets.load_dataset('snli', split='train')\n",
    "snli"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "m_nli = datasets.load_dataset('glue', 'mnli', split='train')\n",
    "m_nli"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "m_nli = m_nli.remove_columns(['idx'])\n",
    "snli = snli.cast(m_nli.features)\n",
    "dataset = datasets.concatenate_datasets([snli, m_nli])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(len(dataset))\n",
    "# there are -1 values in the label feature, these are where no class could be decided so we remove\n",
    "dataset = dataset.filter(\n",
    "    lambda x: 0 if x['label'] == -1 else 1\n",
    ")\n",
    "print(len(dataset))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import BertTokenizer\n",
    "\n",
    "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_cols = ['label']\n",
    "\n",
    "for part in ['premise', 'hypothesis']:\n",
    "    dataset = dataset.map(\n",
    "        lambda x: tokenizer(\n",
    "            x[part], max_length=128, padding='max_length',\n",
    "            truncation=True\n",
    "        ), batched=True\n",
    "    )\n",
    "    for col in ['input_ids', 'attention_mask']:\n",
    "        dataset = dataset.rename_column(\n",
    "            col, part+'_'+col\n",
    "        )\n",
    "        all_cols.append(part+'_'+col)\n",
    "print(all_cols)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"python\\n# covert dataset features to PyTorch tensors\\ndataset.set_format(type='torch', columns=all_cols)\\n\\n# initialize the dataloader\\nbatch_size = 16\\nloader = torch.utils.data.DataLoader(\\n    dataset, batch_size=batch_size, shuffle=True\\n)\\n\""
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''python\n",
    "# covert dataset features to PyTorch tensors\n",
    "dataset.set_format(type='torch', columns=all_cols)\n",
    "\n",
    "# initialize the dataloader\n",
    "batch_size = 16\n",
    "loader = torch.utils.data.DataLoader(\n",
    "    dataset, batch_size=batch_size, shuffle=True\n",
    ")\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# start from a pretrained bert-base-uncased model\n",
    "model = BertModel.from_pretrained('bert-base-uncased')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define mean pooling function\n",
    "def mean_pool(token_embeds, attention_mask):\n",
    "    # reshape attention_mask to cover 768-dimension embeddings\n",
    "    in_mask = attention_mask.unsqueeze(-1).expand(\n",
    "        token_embeds.size()\n",
    "    ).float()\n",
    "    # perform mean-pooling but exclude padding tokens (specified by in_mask)\n",
    "    pool = torch.sum(token_embeds * in_mask, 1) / torch.clamp(\n",
    "        in_mask.sum(1), min=1e-9\n",
    "    )\n",
    "    return pool"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "uv_abs = torch.abs(torch.sub(u, v))  # produces |u-v| tensor\n",
    "# then we concatenate\n",
    "x = torch.cat([u, v, uv_abs], dim=-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# we would initialize the feed-forward NN first\n",
    "ffnn = torch.nn.Linear(768*3, 3)\n",
    "\t...\n",
    "# then later in the code process our concatenated vector with it\n",
    "x = ffnn(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# as before, we would initialize the loss function first\n",
    "loss_func = torch.nn.CrossEntropyLoss()\n",
    "\t...\n",
    "# then later in the code add them to the process\n",
    "x = loss_func(x, label)  # label is our *true* 0, 1, 2 class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers.optimization import get_linear_schedule_with_warmup\n",
    "\n",
    "# we would initialize everything first\n",
    "optim = torch.optim.Adam(model.parameters(), lr=2e-5)\n",
    "# and setup a warmup for the first ~10% steps\n",
    "total_steps = int(len(dataset) / batch_size)\n",
    "warmup_steps = int(0.1 * total_steps)\n",
    "scheduler = get_linear_schedule_with_warmup(\n",
    "\t\toptim, num_warmup_steps=warmup_steps,\n",
    "  \tnum_training_steps=total_steps - warmup_steps\n",
    ")\n",
    "\t...\n",
    "# then during the training loop we update the scheduler per step\n",
    "scheduler.step()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm.auto import tqdm\n",
    "\n",
    "# 1 epoch should be enough, increase if wanted\n",
    "for epoch in range(1):\n",
    "    model.train()  # make sure model is in training mode\n",
    "    # initialize the dataloader loop with tqdm (tqdm == progress bar)\n",
    "    loop = tqdm(loader, leave=True)\n",
    "    for batch in loop:\n",
    "        # zero all gradients on each new step\n",
    "        optim.zero_grad()\n",
    "        # prepare batches and more all to the active device\n",
    "        inputs_ids_a = batch['premise_input_ids'].to(device)\n",
    "        inputs_ids_b = batch['hypothesis_input_ids'].to(device)\n",
    "        attention_a = batch['premise_attention_mask'].to(device)\n",
    "        attention_b = batch['hypothesis_attention_mask'].to(device)\n",
    "        label = batch['label'].to(device)\n",
    "        # extract token embeddings from BERT\n",
    "        u = model(\n",
    "            inputs_ids_a, attention_mask=attention_a\n",
    "        )[0]  # all token embeddings A\n",
    "        v = model(\n",
    "            inputs_ids_b, attention_mask=attention_b\n",
    "        )[0]  # all token embeddings B\n",
    "        # get the mean pooled vectors\n",
    "        u = mean_pool(u, attention_a)\n",
    "        v = mean_pool(v, attention_b)\n",
    "        # build the |u-v| tensor\n",
    "        uv = torch.sub(u, v)\n",
    "        uv_abs = torch.abs(uv)\n",
    "        # concatenate u, v, |u-v|\n",
    "        x = torch.cat([u, v, uv_abs], dim=-1)\n",
    "        # process concatenated tensor through FFNN\n",
    "        x = ffnn(x)\n",
    "        # calculate the 'softmax-loss' between predicted and true label\n",
    "        loss = loss_func(x, label)\n",
    "        # using loss, calculate gradients and then optimize\n",
    "        loss.backward()\n",
    "        optim.step()\n",
    "        # update learning rate scheduler\n",
    "        scheduler.step()\n",
    "        # update the TDQM progress bar\n",
    "        loop.set_description(f'Epoch {epoch}')\n",
    "        loop.set_postfix(loss=loss.item())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "model_path = './sbert_test_a'\n",
    "\n",
    "if not os.path.exists(model_path):\n",
    "    os.mkdir(model_path)\n",
    "\n",
    "model.save_pretrained(model_path)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
