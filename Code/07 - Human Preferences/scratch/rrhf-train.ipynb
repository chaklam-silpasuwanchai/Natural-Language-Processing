{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# [RRHF: Rank Responses to Align Language Models with Human Feedback without tears](https://arxiv.org/abs/2304.05302)\n",
    "\n",
    "- Reinforcement Learning from Human Feedback (RLHF) enables the alignment of LLMs with human preference, improving the quality of interactions between humans and language models.\n",
    "- Recent practice of RLHF uses PPO to enable the large language model optimization of such alignment.\n",
    "- However, implementing PPO is non-trivial (where the training procedure requires interactive between policy, behavior policy, reward, value model) and it is also tedious to tuning many hyper-parameters. \n",
    "-  Motivation is to simplify the alignment between language models with human preference, and proposed paradigm RRHF (Rank Response from Human Feedback) can achieve such alignment as easily as conventional fine-tuning. \n",
    "- It is simpler than PPO from the aspects of coding, model counts, and hyperparameters. [github](https://github.com/GanjinZero/RRHF)\n",
    "\n",
    "<img src=\"../figures/workflow-rrhf.png\" title=\"rrhf\" width=600/>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "device(type='cuda')"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os\n",
    "import torch\n",
    "# Set GPU device\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"1\"\n",
    "\n",
    "os.environ['http_proxy']  = 'http://192.41.170.23:3128'\n",
    "os.environ['https_proxy'] = 'http://192.41.170.23:3128'\n",
    "\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.10/dist-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "from transformers import (\n",
    "    AutoModelForCausalLM,\n",
    "    AutoTokenizer,\n",
    "    HfArgumentParser,\n",
    "    TrainingArguments\n",
    ")\n",
    "from dataclasses import dataclass, field\n",
    "from typing import Optional, Dict, Sequence\n",
    "\n",
    "@dataclass\n",
    "class ModelArguments:\n",
    "    model_name_or_path: Optional[str] = field(default=\"facebook/opt-125m\")\n",
    "\n",
    "\n",
    "@dataclass\n",
    "class DataArguments:\n",
    "    data_path: str = field(default='./data/alpaca_responses_hh.json', metadata={\"help\": \"Path to the training data.\"})\n",
    "    stop_response: bool = field(default=False)\n",
    "\n",
    "\n",
    "@dataclass\n",
    "class TrainingArguments(TrainingArguments):\n",
    "    output_dir : str = field(default='rrhf_model')\n",
    "    cache_dir: Optional[str] = field(default=None)\n",
    "    optim: str = field(default=\"adamw_torch\")\n",
    "    model_max_length: int = field(\n",
    "        default=512,\n",
    "        metadata={\"help\": \"Maximum sequence length. Sequences will be right padded (and possibly truncated).\"},\n",
    "    )\n",
    "    rrhf_weight: float = field(default=100.0)\n",
    "    length_penalty: float = field(default=1.0)\n",
    "    only_use_provide: bool = field(default=False)\n",
    "    only_use_sample: bool = field(default=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# parser = HfArgumentParser((ModelArguments, DataArguments, TrainingArguments))\n",
    "# model_args, data_args, training_args = parser.parse_args_into_dataclasses()\n",
    "model_args = ModelArguments()\n",
    "data_args = DataArguments()\n",
    "training_args = TrainingArguments()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "IGNORE_INDEX = -100\n",
    "DEFAULT_PAD_TOKEN = \"[PAD]\"\n",
    "DEFAULT_EOS_TOKEN = \"</s>\"\n",
    "DEFAULT_BOS_TOKEN = \"</s>\"\n",
    "DEFAULT_UNK_TOKEN = \"</s>\"\n",
    "PROMPT_DICT = {\n",
    "    \"prompt_input\": (\n",
    "        \"Below is an instruction that describes a task, paired with an input that provides further context. \"\n",
    "        \"Write a response that appropriately completes the request.\\n\\n\"\n",
    "        \"### Instruction:\\n{instruction}\\n\\n### Input:\\n{input}\\n\\n### Response:\"\n",
    "    ),\n",
    "    \"prompt_no_input\": (\n",
    "        \"Below is an instruction that describes a task. \"\n",
    "        \"Write a response that appropriately completes the request.\\n\\n\"\n",
    "        \"### Instruction:\\n{instruction}\\n\\n### Response:\"\n",
    "    ),\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    model_args.model_name_or_path,\n",
    "    cache_dir=training_args.cache_dir,\n",
    ")\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(\n",
    "    model_args.model_name_or_path,\n",
    "    cache_dir=training_args.cache_dir,\n",
    "    model_max_length=training_args.model_max_length,\n",
    "    padding_side=\"right\",\n",
    "    use_fast=False,\n",
    ")\n",
    "if tokenizer.pad_token is None:\n",
    "    smart_tokenizer_and_embedding_resize(\n",
    "        special_tokens_dict=dict(pad_token=DEFAULT_PAD_TOKEN),\n",
    "        tokenizer=tokenizer,\n",
    "        model=model,\n",
    "    )\n",
    "if \"llama\" in model_args.model_name_or_path:\n",
    "    tokenizer.add_special_tokens(\n",
    "        {\n",
    "            \"eos_token\": DEFAULT_EOS_TOKEN,\n",
    "            \"bos_token\": DEFAULT_BOS_TOKEN,\n",
    "            \"unk_token\": DEFAULT_UNK_TOKEN,\n",
    "        }\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import (\n",
    "    PreTrainedTokenizer,\n",
    "    PreTrainedModel\n",
    ")\n",
    "from torch.utils.data import Dataset\n",
    "import json\n",
    "\n",
    "def _single_tokenize(text, tokenizer, max_len=None):\n",
    "    if max_len is None:\n",
    "        max_len = tokenizer.model_max_length\n",
    "    toked = tokenizer(\n",
    "            text,\n",
    "            return_tensors=\"pt\",\n",
    "            padding=\"longest\",\n",
    "            max_length=max_len,\n",
    "            truncation=True,\n",
    "        )\n",
    "    return toked['input_ids'][0]\n",
    "\n",
    "def stop_response(res):\n",
    "    stops = ['\\n\\nHuman:', '\\n\\nAssistant:', '\\n\\nhuman:', '\\n\\nassistant:']\n",
    "    for stop in stops:\n",
    "        if res.find(stop) >= 0:\n",
    "            res = res[:res.find(stop)].strip()\n",
    "    return res\n",
    "\n",
    "def smart_tokenizer_and_embedding_resize(\n",
    "    special_tokens_dict: Dict,\n",
    "    tokenizer: PreTrainedTokenizer,\n",
    "    model: PreTrainedModel,\n",
    "):\n",
    "    \"\"\"Resize tokenizer and embedding.\n",
    "\n",
    "    Note: This is the unoptimized version that may make your embedding size not be divisible by 64.\n",
    "    \"\"\"\n",
    "    num_new_tokens = tokenizer.add_special_tokens(special_tokens_dict)\n",
    "    model.resize_token_embeddings(len(tokenizer))\n",
    "\n",
    "    if num_new_tokens > 0:\n",
    "        input_embeddings = model.get_input_embeddings().weight.data\n",
    "        output_embeddings = model.get_output_embeddings().weight.data\n",
    "\n",
    "        input_embeddings_avg = input_embeddings[:-num_new_tokens].mean(dim=0, keepdim=True)\n",
    "        output_embeddings_avg = output_embeddings[:-num_new_tokens].mean(dim=0, keepdim=True)\n",
    "\n",
    "        input_embeddings[-num_new_tokens:] = input_embeddings_avg\n",
    "        output_embeddings[-num_new_tokens:] = output_embeddings_avg\n",
    "\n",
    "class ScoreDataset(Dataset):\n",
    "    \"\"\"Dataset for supervised fine-tuning.\"\"\"\n",
    "\n",
    "    def __init__(self, data_path: str, tokenizer: PreTrainedTokenizer):\n",
    "        super(ScoreDataset, self).__init__()\n",
    "        print(\"Loading data...\")\n",
    "        with open(data_path, 'r') as f:\n",
    "            lines = f.readlines()\n",
    "        self.data = [json.loads(line.strip()) for line in lines[:100]] #I test only 100 samples\n",
    " \n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "\n",
    "    def __getitem__(self, i):\n",
    "        return dict(input_ids=self.data[i])\n",
    "    \n",
    "@dataclass\n",
    "class DataCollatorForSupervisedDataset(object):\n",
    "    \"\"\"Collate examples for supervised fine-tuning.\"\"\"\n",
    "\n",
    "    tokenizer: PreTrainedTokenizer\n",
    "    stop_response: bool\n",
    "\n",
    "    def __call__(self, instances):\n",
    "\n",
    "        idxs = []\n",
    "        all_scores = []\n",
    "        input_ids = []\n",
    "        score_mask = []\n",
    "        labels = []\n",
    "        for idx, ins in enumerate(instances):\n",
    "\n",
    "            ins = ins['input_ids'] # hack\n",
    "            query = ins['query']\n",
    "            responses = ins['responses']\n",
    "            scores = ins['scores']\n",
    "            all_scores.append(scores)\n",
    "            idxs.append([idx] * len(scores))\n",
    "\n",
    "            query_input_ids = _single_tokenize(query, self.tokenizer)\n",
    "            query_target = torch.LongTensor([IGNORE_INDEX] * (query_input_ids.shape[0] - 1))\n",
    "            dummy_target = torch.LongTensor([IGNORE_INDEX])\n",
    "            for res in responses:\n",
    "                if self.stop_response:\n",
    "                    r = stop_response(res)\n",
    "                else:\n",
    "                    r = res\n",
    "                res_input_ids = _single_tokenize(r + self.tokenizer.eos_token, self.tokenizer, max_len=self.tokenizer.model_max_length-query_input_ids.shape[0]) # eos here\n",
    "                input_ids.append(torch.cat((query_input_ids, res_input_ids), dim=0))\n",
    "                labels.append(torch.cat((query_target, res_input_ids, dummy_target), dim=0))\n",
    "\n",
    "        input_ids = torch.nn.utils.rnn.pad_sequence(\n",
    "            input_ids, batch_first=True, padding_value=self.tokenizer.pad_token_id\n",
    "        )\n",
    "        labels = torch.nn.utils.rnn.pad_sequence(\n",
    "            labels, batch_first=True, padding_value=IGNORE_INDEX\n",
    "        )\n",
    "        return dict(\n",
    "            input_ids=input_ids,\n",
    "            attention_mask=input_ids.ne(self.tokenizer.pad_token_id),\n",
    "            labels=labels,\n",
    "            idxs=torch.LongTensor(idxs),\n",
    "            scores=torch.FloatTensor(all_scores),\n",
    "        )\n",
    "    \n",
    "def make_supervised_data_module(tokenizer: PreTrainedTokenizer, data_args) -> Dict:\n",
    "    \"\"\"Make dataset and collator for supervised fine-tuning.\"\"\"\n",
    "    train_dataset = ScoreDataset(tokenizer=tokenizer, data_path=data_args.data_path)\n",
    "    data_collator = DataCollatorForSupervisedDataset(tokenizer=tokenizer, stop_response=data_args.stop_response)\n",
    "    return dict(train_dataset=train_dataset, eval_dataset=None, data_collator=data_collator)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading data...\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'train_dataset': <__main__.ScoreDataset at 0x7f4c209ea470>,\n",
       " 'eval_dataset': None,\n",
       " 'data_collator': DataCollatorForSupervisedDataset(tokenizer=GPT2Tokenizer(name_or_path='facebook/opt-125m', vocab_size=50265, model_max_length=512, is_fast=False, padding_side='right', truncation_side='right', special_tokens={'bos_token': '</s>', 'eos_token': '</s>', 'unk_token': '</s>', 'pad_token': '<pad>'}, clean_up_tokenization_spaces=True),  added_tokens_decoder={\n",
       " \t1: AddedToken(\"<pad>\", rstrip=False, lstrip=False, single_word=False, normalized=True, special=True),\n",
       " \t2: AddedToken(\"</s>\", rstrip=False, lstrip=False, single_word=False, normalized=True, special=True),\n",
       " }, stop_response=False)}"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_module = make_supervised_data_module(\n",
    "    tokenizer=tokenizer, \n",
    "    data_args=data_args)\n",
    "\n",
    "data_module"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'query': ' \\n\\nHuman: Can you describe the steps to clean fingerprints and smudges from a laptop screen\\n\\nAssistant: Yes, certainly. To clean your screen, you first need to use a microfiber cloth or soft, damp cloth to gently wipe down the surface of the screen. Next, you’ll want to grab a soft, lint-free, microfiber cleaning cloth and gently rub it back and forth across the screen to remove fingerprints and smudges.\\n\\nHuman: Can I spray isopropyl alcohol onto the cloth and clean it that way?\\n\\nAssistant:',\n",
       " 'responses': [\" You can, but it's not necessary. The microfiber cloth should be effective enough on its own.\",\n",
       "  ' No, that is not recommended. Isopropyl alcohol can damage the screen and should not be used for cleaning.',\n",
       "  \" You can, but it's not necessary. The microfiber cloth should be effective enough on its own. Just make sure to use gentle, circular motions when cleaning the screen.\",\n",
       "  ' No, that is not recommended. Isopropyl alcohol can damage the screen and should not be used for cleaning. You should use a clean cloth and water instead.',\n",
       "  ' Yes, you can do that to help the cloth pick up even more dirt from the screen. Be sure to always use a clean, soft cloth, not a piece of scratchy, roughened, or textured material, and make sure it’s lint-free.',\n",
       "  ' Yes, you can spray it directly onto the cloth.'],\n",
       " 'scores': [-0.8408203125,\n",
       "  -0.85400390625,\n",
       "  -0.57421875,\n",
       "  -0.62841796875,\n",
       "  -0.492431640625,\n",
       "  -0.8798828125]}"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_module['train_dataset'].data[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-12-28 05:23:56.170704: I tensorflow/core/util/port.cc:113] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n",
      "2023-12-28 05:23:56.188188: E external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:9261] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
      "2023-12-28 05:23:56.188213: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:607] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
      "2023-12-28 05:23:56.189055: E external/local_xla/xla/stream_executor/cuda/cuda_blas.cc:1515] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
      "2023-12-28 05:23:56.192662: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 AVX_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2023-12-28 05:23:57.116071: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn.functional as F\n",
    "from transformers import Trainer\n",
    "\n",
    "class RRHFTrainer(Trainer):\n",
    "    def gather_logits_labels(self, logits, labels):\n",
    "        mask = (labels != -100).float()\n",
    "        new_logits = logits.clone()  # Create a copy to avoid in-place modification\n",
    "        labels[labels == -100] = 0 \n",
    "        output = torch.gather(new_logits, dim=-1, index=labels.unsqueeze(-1)).squeeze(-1)\n",
    "        output = output * mask # B * L\n",
    "        return output\n",
    "\n",
    "    def get_score(self, logit_label, labels):\n",
    "        mask = (labels != -100).float()\n",
    "        length = mask.sum(-1)\n",
    "        scores = logit_label.sum(-1) / (length ** self.args.length_penalty)\n",
    "        return scores\n",
    "\n",
    "    def rrhf_loss(self, scores, idxs, rw_scores):\n",
    "        diff = scores.unsqueeze(0) - scores.unsqueeze(-1) # b * b\n",
    "        rw_diff = rw_scores.unsqueeze(0) - rw_scores.unsqueeze(-1) # b * b\n",
    "        aval = torch.bitwise_and(rw_diff > 0, diff < 0)[0]\n",
    "        return -diff[aval].sum()\n",
    "\n",
    "    def sft_loss(self, logit_label, idxs, rw_scores):\n",
    "        max_idx = torch.argmax(rw_scores)\n",
    "        return -logit_label[max_idx].mean()\n",
    "\n",
    "    def compute_loss(self, model, inputs, return_outputs=False):\n",
    "        if self.args.only_use_provide:\n",
    "            inputs['input_ids'] = inputs['input_ids'][-2:]\n",
    "            inputs['attention_mask'] = inputs['attention_mask'][-2:]\n",
    "            inputs['labels'] = inputs['labels'][-2:]\n",
    "            inputs[\"idxs\"] = inputs[\"idxs\"][:,-2:]\n",
    "            inputs[\"scores\"] = inputs[\"scores\"][:,-2:]\n",
    "            \n",
    "        if self.args.only_use_sample:\n",
    "            inputs['input_ids'] = inputs['input_ids'][:-2]\n",
    "            inputs['attention_mask'] = inputs['attention_mask'][:-2]\n",
    "            inputs['labels'] = inputs['labels'][:-2]\n",
    "            inputs[\"idxs\"] = inputs[\"idxs\"][:,:-2]\n",
    "            inputs[\"scores\"] = inputs[\"scores\"][:,:-2]\n",
    "        \n",
    "        logits = model(input_ids=inputs.get('input_ids'), attention_mask=inputs.get('attention_mask'))[0] # (batch * cand) * L * V\n",
    "        logits = F.log_softmax(logits, dim=-1)\n",
    "        logit_label = self.gather_logits_labels(logits, inputs.get(\"labels\"))\n",
    "        scores = self.get_score(logit_label, inputs.get(\"labels\"))\n",
    "        \n",
    "        rrhf_loss = self.rrhf_loss(scores, inputs.get(\"idxs\"), inputs.get(\"scores\"))\n",
    "        sft_loss = self.sft_loss(logit_label, inputs.get(\"idxs\"), inputs.get(\"scores\"))\n",
    "        loss = self.args.rrhf_weight * rrhf_loss + sft_loss\n",
    "        \n",
    "        return (loss, scores) if return_outputs else loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer = RRHFTrainer(\n",
    "    model=model, \n",
    "    tokenizer=tokenizer, \n",
    "    args=training_args, \n",
    "    **data_module)\n",
    "\n",
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
