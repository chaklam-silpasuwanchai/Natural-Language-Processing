{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# [InstructGPT : Training language models to follow instructions with human feedback](https://arxiv.org/pdf/2203.02155.pdf)\n",
    "\n",
    "\n",
    "### Reference Code \n",
    "- https://github.com/xrsrke/instructGOOSE/tree/main\n",
    "\n",
    "<img src=\"../figures/instructGPT.png\" title=\"instructGPT\" />\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip3 install instruct_goose"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.10/dist-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "device(type='cuda')"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from datasets import load_dataset\n",
    "from tqdm.auto import tqdm\n",
    "\n",
    "import torch\n",
    "from torch.utils.data import DataLoader, random_split\n",
    "from torch import optim\n",
    "\n",
    "import os\n",
    "# Set GPU device\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"1\"\n",
    "\n",
    "os.environ['http_proxy']  = 'http://192.41.170.23:3128'\n",
    "os.environ['https_proxy'] = 'http://192.41.170.23:3128'\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "device"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Load dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Found cached dataset imdb (/home/todsavadt/.cache/huggingface/datasets/imdb/plain_text/1.0.0/d613c88cf8fa3bab83b4ded3713f1f74830d1100e171db75bbddb80b3345c9c0)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Dataset({\n",
       "    features: ['text', 'label'],\n",
       "    num_rows: 25000\n",
       "})"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset = load_dataset(\"imdb\", split=\"train\")\n",
    "dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset, _ = random_split(\n",
    "    dataset, \n",
    "    lengths=[10, len(dataset) - 10]\n",
    ") # for demenstration purposes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataloader = DataLoader(\n",
    "    dataset, \n",
    "    batch_size=16, \n",
    "    shuffle=True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# for batch in train_dataloader:\n",
    "#     break\n",
    "\n",
    "# batch['text'], batch['label']"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Load the pre-trained model and tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
    "from instruct_goose import (\n",
    "    Agent, \n",
    "    RewardModel, \n",
    "    RLHFTrainer, \n",
    "    RLHFConfig, \n",
    "    create_reference_model\n",
    ")\n",
    "\n",
    "model_name_or_path = \"gpt2\"\n",
    "\n",
    "model_base = AutoModelForCausalLM.from_pretrained(model_name_or_path) # for demonstration purposes\n",
    "reward_model = RewardModel(model_name_or_path)\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name_or_path, padding_side=\"left\")\n",
    "eos_token_id = tokenizer.eos_token_id\n",
    "tokenizer.pad_token = tokenizer.eos_token"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Create the RL-based language model agent and the reference model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Agent(model_base)\n",
    "ref_model = create_reference_model(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ref_model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model.to(device)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "max_new_tokens = 20\n",
    "generation_kwargs = {\n",
    "    \"min_length\":-1,\n",
    "    \"top_k\": 0.0,\n",
    "    \"top_p\": 1.0,\n",
    "    \"do_sample\": True,\n",
    "    \"pad_token_id\": tokenizer.eos_token_id,\n",
    "    \"max_new_tokens\": max_new_tokens\n",
    "}\n",
    "\n",
    "config = RLHFConfig()\n",
    "trainer = RLHFTrainer(model, ref_model, config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\ndef compute_loss(\\n    self,\\n    query_ids: TensorType[\"batch_size\", \"seq_len\"],\\n    query_attention_mask: TensorType[\"batch_size\", \"seq_len\"],\\n    response_ids: TensorType[\"batch_size\", \"seq_len\"],\\n    response_attention_mask: TensorType[\"batch_size\", \"seq_len\"],\\n    rewards: TensorType[\"batch_size\"],\\n) -> TensorType[\"1\"]:\\n    \"\"\"Calculate PPO\\'s loss.\"\"\"\\n    logprobs, values, entropies, ref_logprobs = self.forward(\\n        query_ids=query_ids,\\n        query_attention_mask=query_attention_mask,\\n        response_ids=response_ids,\\n        response_attention_mask=response_attention_mask\\n    )\\n\\n    ratio = (logprobs - ref_logprobs).exp()\\n    clipped_ratio = torch.clamp(ratio, min=1-self.epsilon, max=1+self.epsilon)\\n\\n    advantages, returns = self.compute_advantage_and_return(rewards, values)\\n    value_loss = (values - returns).pow(2).mean()\\n\\n    pg_loss_1 = ratio * advantages\\n    pg_loss_2 = ratio * clipped_ratio\\n    pg_loss = torch.min(pg_loss_1, pg_loss_2).mean()\\n\\n    loss = pg_loss - self.ent_coef * entropies.mean() + self.vf_coef * value_loss\\n    return loss\\n'"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''\n",
    "def forward(\n",
    "        self,\n",
    "        query_ids: TensorType[\"batch_size\", \"seq_len\"],\n",
    "        query_attention_mask: TensorType[\"batch_size\", \"seq_len\"],\n",
    "        response_ids: TensorType[\"batch_size\", \"seq_len\"],\n",
    "        response_attention_mask: TensorType[\"batch_size\", \"seq_len\"]\n",
    "    ) -> Tuple[\n",
    "        TensorType[\"batch_size\"], # main model's logprobs\n",
    "        TensorType[\"batch_size\"], # entropy\n",
    "        TensorType[\"batch_size\"], # value\n",
    "        TensorType[\"batch_size\"], # reference model's log prob\n",
    "    ]:\n",
    "        input_ids = torch.cat([query_ids, response_ids], dim=1)\n",
    "        attention_mask = torch.cat([query_attention_mask, response_attention_mask], dim=1)\n",
    "\n",
    "        _, logprobs, entropy, value = self.model(\n",
    "            input_ids=input_ids,\n",
    "            attention_mask=attention_mask\n",
    "        )\n",
    "        _, ref_logprob, _, _ = self.ref_model(\n",
    "            input_ids=input_ids,\n",
    "            attention_mask=attention_mask\n",
    "        )\n",
    "\n",
    "        return logprobs, entropy, value, ref_logprob\n",
    "\n",
    "def compute_loss(\n",
    "    self,\n",
    "    query_ids: TensorType[\"batch_size\", \"seq_len\"],\n",
    "    query_attention_mask: TensorType[\"batch_size\", \"seq_len\"],\n",
    "    response_ids: TensorType[\"batch_size\", \"seq_len\"],\n",
    "    response_attention_mask: TensorType[\"batch_size\", \"seq_len\"],\n",
    "    rewards: TensorType[\"batch_size\"],\n",
    ") -> TensorType[\"1\"]:\n",
    "    \"\"\"Calculate PPO's loss.\"\"\"\n",
    "    logprobs, values, entropies, ref_logprobs = self.forward(\n",
    "        query_ids=query_ids,\n",
    "        query_attention_mask=query_attention_mask,\n",
    "        response_ids=response_ids,\n",
    "        response_attention_mask=response_attention_mask\n",
    "    )\n",
    "\n",
    "    ratio = (logprobs - ref_logprobs).exp()\n",
    "    clipped_ratio = torch.clamp(ratio, min=1-self.epsilon, max=1+self.epsilon)\n",
    "\n",
    "    advantages, returns = self.compute_advantage_and_return(rewards, values)\n",
    "    value_loss = (values - returns).pow(2).mean()\n",
    "\n",
    "    pg_loss_1 = ratio * advantages\n",
    "    pg_loss_2 = ratio * clipped_ratio\n",
    "    pg_loss = torch.min(pg_loss_1, pg_loss_2).mean()\n",
    "\n",
    "    loss = pg_loss - self.ent_coef * entropies.mean() + self.vf_coef * value_loss\n",
    "    return loss\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_epochs = 3 # for demonstration purposes\n",
    "optimizer = optim.Adam(model.parameters(), lr=1e-3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1/1 [00:34<00:00, 34.01s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1 | loss = -7.5626\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1/1 [00:31<00:00, 31.00s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 2 | loss = 2.6821\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1/1 [00:26<00:00, 26.60s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 3 | loss = 0.0515\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1/1 [00:26<00:00, 26.12s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 4 | loss = -0.8420\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1/1 [00:30<00:00, 30.54s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 5 | loss = 16.6571\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "for epoch in range(num_epochs):\n",
    "    for step, batch in enumerate(tqdm(train_dataloader, leave=True)):\n",
    "        inputs = tokenizer(\n",
    "            batch[\"text\"], \n",
    "            padding=True, \n",
    "            truncation=True, \n",
    "            return_tensors=\"pt\"\n",
    "        )\n",
    "        \n",
    "        response_ids = model.generate(\n",
    "            inputs[\"input_ids\"], attention_mask=inputs[\"attention_mask\"],\n",
    "            **generation_kwargs\n",
    "        )\n",
    "        \n",
    "        # extract the generated text\n",
    "        response_ids = response_ids[:, -max_new_tokens:]\n",
    "        response_attention_mask = torch.ones_like(response_ids)\n",
    "\n",
    "        # evaluate from the reward model\n",
    "        with torch.no_grad():\n",
    "            text_input_ids = torch.stack([torch.concat([q, r]) for q, r in zip(inputs[\"input_ids\"], response_ids)], dim=0)\n",
    "            rewards = reward_model(text_input_ids)\n",
    "\n",
    "        # calculate PPO loss\n",
    "        loss = trainer.compute_loss(\n",
    "            query_ids=inputs[\"input_ids\"],\n",
    "            query_attention_mask=inputs[\"attention_mask\"],\n",
    "            response_ids=response_ids,\n",
    "            response_attention_mask=response_attention_mask,\n",
    "            rewards=rewards\n",
    "        )\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        print(f'Epoch: {epoch+1} | loss = {loss:.4f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated text:\n",
      " I stumbled on this series rather by accident. After half an episode, I was hooked. American Gothic was a dark, strange series with Gary Cole as the mysterious, probably evil Sheriff Buck who is trying to gain control of his illegitimate son Caleb, played by Lucas Black. I was impressed with Gary Cole's sinister sheriff and I was even more impressed with Lucas Black. Lucas Black's Caleb was able to stand up against Sheriff Buck, one of the most frightening characters ever created for a TV series. I have rarely seen a child actor with as much presence or talent as Lucas Black. If you were not lucky enough to see Lucas in American Gothic, see him in Slingblade.<br /><br />It was a remarkable show with many ambiguities and mysteries that were never explained during it's short run. chance chance chances chance on chance free chance\n",
      " chance no chance hope chance not chance name chance opportunity chance one chanceon chance that chance life chance test chance time chance promise chance or chancechance chance any chance so chance be chance's chance bet chance survival chance trade chance risk chance but chancement chance if chance happen chance when chance of chance mean chance guarantee chance best chance sure chance every chance even chance/ chancey chancefall chance' chance make chance people chance change chance share\n"
     ]
    }
   ],
   "source": [
    "# Encode input text\n",
    "input_text = dataset[0]['text']\n",
    "input_ids = tokenizer.encode(input_text, return_tensors=\"pt\")\n",
    "\n",
    "# Generate output\n",
    "output = model_base.generate(input_ids, max_length=256, num_beams=5, no_repeat_ngram_size=2, top_k=50, top_p=0.95, temperature=0.7)\n",
    "\n",
    "# Decode and print the generated text\n",
    "generated_text = tokenizer.decode(output[0], skip_special_tokens=True)\n",
    "print(\"Generated text:\\n\", generated_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
