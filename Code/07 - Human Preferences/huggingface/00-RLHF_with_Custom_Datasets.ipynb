{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Implementing RLHF with Custom Datasets\n",
    "\n",
    "Reinforcement Learning with Human Feedback (RLHF) is a popular approach in the field of natural language processing that aims to optimize language models for human preferences directly, rather than solely relying on traditional training methods such as supervised or unsupervised learning. With the recent public release of ChatGPT, RLHF has become a hot topic in both academic and industrial language modeling circles.\n",
    "\n",
    "<img src=\"../figures/RLHF_w_custom_dataset.png\" title=\"Learning to summarize with feedback\" />\n",
    "\n",
    "[Reference Code](https://github.com/HumanSignal/RLHF/blob/master/tutorials/RLHF_with_Custom_Datasets.ipynb)\n",
    "\n",
    "In this notebook, we will explore how to implement RLHF using the trlX library and create a custom dataset with Label Studio. By the end of this notebook, you should have a solid understanding of how to implement RLHF with custom datasets, and be well-equipped to continue exploring this exciting area of research.\n",
    "\n",
    "The notebook will be structured as follows:\n",
    "\n",
    "1. Introduction to RLHF and trlX\n",
    "2. Setting up the environment and installing necessary libraries\n",
    "3. Creating a custom dataset\n",
    "4. Labeling our dataset with Label Studio\n",
    "5. Training a preference model with our custom dataset\n",
    "6. Tune our language model with our preference model using trlX\n",
    "7. References\n",
    "\n",
    "Let's get started!"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "id": "Q-feXdy31kBh"
   },
   "source": [
    "## 1. Introduction to RLHF and trlX\n",
    "Implementing RLHF with custom datasets can be a daunting task for those unfamiliar with the necessary tools and techniques. The primary objective of this notebook is to showcase a technique for reducing bias when fine-tuning Language Models (LLMs) using feedback from humans. To achieve this goal, we will be using a minimal set of tools, including Huggingface, GPT2, Label Studio, Weights and Biases, and trlX.\n",
    "\n",
    "Our aim is to provide the most efficient and straightforward method for creating a pipeline that moves from raw data to a real-world RLHF system. We will walk through the process step-by-step, including an introduction to RLHF and trlX, setting up the environment, creating a custom dataset, labeling our dataset with Label Studio, training a preference model with our custom dataset, and finally, tuning our language model with our preference model using trlX.\n",
    "\n",
    "Training Approach for RLHF ([Learning to summarize from human feedback](https://arxiv.org/pdf/2009.01325.pdf)): \n",
    "1. Collect human feedback \n",
    "2. Train a reward model\n",
    "3. Optimize LLM against the reward model\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "id": "GREPeLY_1Z1K"
   },
   "source": [
    "## 2. Setting up the environment and installing necessary libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip3 install mpi4py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "id": "RDp7AlEJW1wK"
   },
   "outputs": [],
   "source": [
    "# !git clone https://github.com/CarperAI/trlx.git\n",
    "# !git config --global --add safe.directory /content/trlx && cd /content/trlx && pip install -e .\n",
    "\n",
    "# uninstall scikit_learn + jax to avoid numpy issues\n",
    "# !pip uninstall -y scikit_learn jax\n",
    "\n",
    "# import os\n",
    "\n",
    "# run within repo\n",
    "# os.chdir('/content/trlx/examples/summarize_rlhf/')\n",
    "# print(os.getcwd())\n",
    "\n",
    "# !pip install -r requirements.txt\n",
    "# !pip install mpi4py\n",
    "\n",
    "# run within reward model directory\n",
    "# os.chdir('/content/trlx/examples/summarize_rlhf/reward_model/')\n",
    "# print(os.getcwd())"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "id": "54TgX16_1s9Z"
   },
   "source": [
    "## 3. Creating a custom dataset \n",
    "In this section we will create a custom dataset for training our reward model. In the case of fine-tuning a LLM for human preference, our data tends to look like this: \n",
    "\n",
    "```json\n",
    "{\n",
    "    \"prompt\": \"The quick brown fox...\",\n",
    "    \"answer1\": \"jumps over the lazy dog.\",\n",
    "    \"answer2\": \"bags few lynx.\",\n",
    "}\n",
    "```\n",
    "\n",
    "The labeler will provide feedback on which selection is preferred, given the prompt. This is the human feedback that will be incorporated into the system. This ranking by human labelers provides allows us to learn a model that scores the quality of our language model's responses.  \n",
    "\n",
    "In this example, we'll show you how to create your own dataset. We'll start with a set of prompts, generate predictions for them using GPT-2, and then have users rank the predictions generated. \n",
    "\n",
    "Note: Due to the compute limitations of colab, we'll be using GPT-2 for this notebook. Thus, the quality of our predictions will not refelect much quality. If you have access to more hardware, then you can swap the GPT-2 model with a larger one like [GPT-J](https://huggingface.co/EleutherAI/gpt-j-6b) or others. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "id": "8LnoKRKydmBT"
   },
   "outputs": [],
   "source": [
    "from transformers import pipeline, set_seed\n",
    "import json\n",
    "\n",
    "def generate_examples(prompt_list, model_name='gpt2', max_length=50, num_return_sequences=2, seed=42):\n",
    "    generator = pipeline('text-generation', model=model_name, device=0)\n",
    "    set_seed(seed)\n",
    "    examples = []\n",
    "    for prompt in prompt_list:\n",
    "        result = generator(prompt, max_length=max_length, num_return_sequences=num_return_sequences)\n",
    "        example = {'prompt': prompt}\n",
    "        for i, res in enumerate(result):\n",
    "            answer = res['generated_text'].lstrip().removeprefix(prompt).strip()\n",
    "            example[f'answer{i + 1}'] = answer\n",
    "        examples.append(example)\n",
    "        print(json.dumps(example, indent=2))\n",
    "    return examples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "id": "jZKnXbSsrCly"
   },
   "outputs": [],
   "source": [
    "prompts = [\n",
    "    \"What is the latest news on the stock market?\",\n",
    "    \"What is the current state of the economy?\",\n",
    "    \"What are the latest developments in technology?\",\n",
    "    \"What is the political situation in the Middle East?\",\n",
    "    \"What are the latest trends in fashion and beauty?\",\n",
    "    \"What are the top travel destinations for this year?\",\n",
    "    \"What are some healthy recipes for a vegan diet?\",\n",
    "    \"What are the most important events happening in the world today?\",\n",
    "    \"What are some tips for improving mental health?\",\n",
    "    \"What are the best ways to save money for retirement?\",\n",
    "    \"What are some popular new books or movies?\",\n",
    "    \"What are some effective ways to reduce stress?\",\n",
    "    \"What are the latest developments in artificial intelligence?\",\n",
    "    \"What are some top-rated restaurants in your city?\",\n",
    "    \"What are the best ways to stay fit and healthy?\",\n",
    "    \"What are some tips for successful entrepreneurship?\",\n",
    "    \"What are some effective ways to improve productivity?\",\n",
    "    \"What are the latest developments in climate change research?\",\n",
    "    \"What are some top-rated TV shows or movies on streaming services?\",\n",
    "    \"What are some fun activities to do on weekends?\",\n",
    "    \"What are some effective ways to manage time and prioritize tasks?\",\n",
    "    \"What are the latest trends in home decor and design?\",\n",
    "    \"What are the best ways to develop a successful career?\",\n",
    "    \"What are some popular new products or gadgets?\",\n",
    "    \"What are some effective ways to improve communication skills?\",\n",
    "    \"What are some tips for successful relationships?\",\n",
    "    \"What are the latest developments in space exploration?\",\n",
    "    \"What are some top-rated online courses or certifications?\",\n",
    "    \"What are some effective ways to improve public speaking skills?\",\n",
    "    \"What are the latest trends in digital marketing?\",\n",
    "    \"What are some fun and creative DIY projects?\",\n",
    "    \"What are some effective ways to improve leadership skills?\"\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "gQzB4SoHfYao"
   },
   "outputs": [],
   "source": [
    "generated_examples = generate_examples(prompts)\n",
    "\n",
    "# Save generated examples to import in Label Studio\n",
    "with open('ls_input_data.json', 'w') as f:\n",
    "    json.dump(generated_examples, f, indent=2)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4. Labeling our dataset with Label Studio\n",
    "\n",
    "<img src=\"../figures/label_studio.png\" title=\"Label Studio\" />"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we have generated some examples, we will label them in Label Studio. \n",
    "Once we have the results of our human labels, we can export the data and train our Preference Model. \n",
    "\n",
    "1. First, we can start Label Studio following the instructions [here](https://labelstud.io/guide/install.html). \n",
    "\n",
    "2. Once we have label studio running, we can create a new project with the [Pariwise Classification template](https://labelstud.io/templates/pairwise_comparison.html). The templates themselves are really flexible, so we'll do some minor edits to make it look a little nicer. The configuration for this template is shown below. \n",
    "\n",
    "```xml\n",
    "<?xml version=\"1.0\" encoding=\"UTF-8\"?>\n",
    "<View>\n",
    "   <Style>* { box-sizing: border-box; margin: 0; padding: 0; } body { font-family: 'Roboto', sans-serif; line-height: 1.6; background-color: #f0f0f0; } .container { margin: 0 auto; padding: 20px; background-color: #ffffff; border-radius: 5px; box-shadow: 0 4px 8px 0 rgba(0, 0, 0, 0.1), 0 6px 20px 0 rgba(0, 0, 0, 0.1); } .prompt { padding: 20px; background-color: #0084ff; color: #ffffff; border-radius: 5px; margin-bottom: 20px; box-shadow: 0 2px 4px 0 rgba(0, 0, 0, 0.1), 0 3px 10px 0 rgba(0, 0, 0, 0.1); } .answers { display: flex; justify-content: space-between; flex-wrap: wrap; gap: 20px; } .answer-box { flex-basis: 49%; padding: 20px; background-color: rgba(44, 62, 80, 0.9); color: #ffffff; border-radius: 5px; box-shadow: 0 2px 4px 0 rgba(0, 0, 0, 0.1), 0 3px 10px 0 rgba(0, 0, 0, 0.1); } .answer-box p { word-wrap: break-word; } .answer-box:hover { background-color: rgba(52, 73, 94, 0.9); cursor: pointer; transition: all 0.3s ease; } .lsf-richtext__line:hover { background: unset; } .answer-box .lsf-object { padding: 20px }</Style>\n",
    "   <View className=\"container\">\n",
    "      <View className=\"prompt\">\n",
    "         <Text name=\"prompt\" value=\"$prompt\" />\n",
    "      </View>\n",
    "      <View className=\"answers\">\n",
    "         <Pairwise name=\"pw\" toName=\"answer1,answer2\" selectionStyle=\"background-color: #27ae60; box-shadow: 0 4px 8px 0 rgba(0, 0, 0, 0.2), 0 6px 20px 0 rgba(0, 0, 0, 0.2); border: 2px solid #2ecc71; cursor: pointer; transition: all 0.3s ease;\" />\n",
    "         <View className=\"answer-box\">\n",
    "            <Text name=\"answer1\" value=\"$answer1\" />\n",
    "         </View>\n",
    "         <View className=\"answer-box\">\n",
    "            <Text name=\"answer2\" value=\"$answer2\" />\n",
    "         </View>\n",
    "      </View>\n",
    "   </View>\n",
    "</View>\n",
    "```\n",
    "\n",
    "3. Next we'll drag and drop to upload our data, and we're off! \n",
    "\n",
    "4. Once we're finished labeling our data, we can export it and we're ready to train our preference model. \n",
    "\n",
    "Note: If you're using colab, upload the dataset into the root directory, and your file will be located at a path in `/content/...`, like `/content/project-7-at-2023-04-12-22-24-4c78f924.json`."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "id": "BfFUYE34atPL"
   },
   "source": [
    "## 5. Training a preference model with our custom dataset\n",
    "Now we're ready to train our preference model. We'll create a dataset from our labels, initialize our model from the pretrained LM, and then begin training. \n",
    "\n",
    "When we finally train our model, we can connect with Weights and Biases to log our training metrics. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "KJfgbIhUknRz"
   },
   "outputs": [],
   "source": [
    "import codecs\n",
    "\n",
    "# This file is generated by Label Studio after completing annotations\n",
    "data_path = '/content/ls_export_data.json'\n",
    "\n",
    "with codecs.open(data_path, 'r', encoding='utf-8') as f:\n",
    "      data = json.load(f)\n",
    "print(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "id": "6bu2klzkXwie"
   },
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "import torch\n",
    "from datasets import load_dataset\n",
    "from reward_model import GPTRewardModel\n",
    "from torch.utils.data import Dataset\n",
    "from tqdm import tqdm\n",
    "from transformers import AutoTokenizer, Trainer, TrainingArguments\n",
    "\n",
    "def create_comparison_dataset_ls(path: str):\n",
    "    with codecs.open(data_path, 'r', encoding='utf-8') as f:\n",
    "          data = json.load(f)\n",
    "    pairs = []\n",
    "    for sample in data:\n",
    "        chosen = None\n",
    "        rejected = None\n",
    "        for annotation in sample['annotations']:\n",
    "            if annotation['result'][0]['value']['selected'] == 'left':\n",
    "                chosen = sample['data']['prompt'] + '\\n' + sample['data']['answer1']\n",
    "                rejected = sample['data']['prompt'] + '\\n' + sample['data']['answer2']\n",
    "            else:\n",
    "                chosen = sample['data']['prompt'] + '\\n' + sample['data']['answer2']\n",
    "                rejected = sample['data']['prompt'] + '\\n' + sample['data']['answer1']\n",
    "            pair = {\n",
    "                'chosen': chosen,\n",
    "                'rejected': rejected\n",
    "            }\n",
    "            pairs.append(pair)\n",
    "    return pairs\n",
    "\n",
    "class PairwiseDataset(Dataset):\n",
    "    def __init__(self, pairs, tokenizer, max_length):\n",
    "        self.chosen_input_ids = []\n",
    "        self.chosen_attn_masks = []\n",
    "        self.rejected_input_ids = []\n",
    "        self.rejected_attn_masks = []\n",
    "        for pair in tqdm(pairs):\n",
    "            chosen, rejected = pair[\"chosen\"], pair[\"rejected\"]\n",
    "            chosen_encodings_dict = tokenizer(\n",
    "                \"<|startoftext|>\" + chosen + \"<|endoftext|>\",\n",
    "                truncation=True,\n",
    "                max_length=max_length,\n",
    "                padding=\"max_length\",\n",
    "                return_tensors=\"pt\",\n",
    "            )\n",
    "            rejected_encodings_dict = tokenizer(\n",
    "                \"<|startoftext|>\" + rejected + \"<|endoftext|>\",\n",
    "                truncation=True,\n",
    "                max_length=max_length,\n",
    "                padding=\"max_length\",\n",
    "                return_tensors=\"pt\",\n",
    "            )\n",
    "            self.chosen_input_ids.append(chosen_encodings_dict[\"input_ids\"])\n",
    "            self.chosen_attn_masks.append(chosen_encodings_dict[\"attention_mask\"])\n",
    "            self.rejected_input_ids.append(rejected_encodings_dict[\"input_ids\"])\n",
    "            self.rejected_attn_masks.append(rejected_encodings_dict[\"attention_mask\"])\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.chosen_input_ids)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        return (\n",
    "            self.chosen_input_ids[idx],\n",
    "            self.chosen_attn_masks[idx],\n",
    "            self.rejected_input_ids[idx],\n",
    "            self.rejected_attn_masks[idx],\n",
    "        )\n",
    "\n",
    "\n",
    "class DataCollatorReward:\n",
    "    def __call__(self, data):\n",
    "        batch = {}\n",
    "        batch[\"input_ids\"] = torch.cat([f[0] for f in data] + [f[2] for f in data])\n",
    "        batch[\"attention_mask\"] = torch.cat([f[1] for f in data] + [f[3] for f in data])\n",
    "        batch[\"labels\"] = torch.tensor([0] * len(data) + [1] * len(data))\n",
    "        return batch\n",
    "\n",
    "\n",
    "def compute_metrics(eval_preds):\n",
    "    chosen_end_scores = eval_preds.predictions[0]  # chosen scores\n",
    "    rejected_end_scores = eval_preds.predictions[1]  # rejected scores\n",
    "\n",
    "    result = {}\n",
    "    acc = sum(chosen_end_scores > rejected_end_scores) / len(rejected_end_scores)\n",
    "    result[\"accuracy\"] = acc\n",
    "\n",
    "    return result\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "rC8CG9c1Ze5o"
   },
   "outputs": [],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained(\"gpt2\")\n",
    "tokenizer.pad_token = tokenizer.eos_token\n",
    "\n",
    "if not os.path.exists(\"rm_checkpoint\"):\n",
    "    os.mkdir(\"rm_checkpoint\")\n",
    "\n",
    "# Initialize the reward model from the GPT-2 model (optionally SFT GPT-2)\n",
    "model = GPTRewardModel(\"gpt2\")\n",
    "\n",
    "# Freeze the first 70% of the hidden layers of the reward model backbone\n",
    "layers = model.transformer.h\n",
    "num_layers = len(layers)\n",
    "num_unfrozen = int(0.3 * num_layers)\n",
    "for layer in layers[:-num_unfrozen]:\n",
    "    layer.requires_grad_(False)\n",
    "\n",
    "# Create the comparisons datasets\n",
    "pairs = create_comparison_dataset_ls(data_path)\n",
    "train_size = int(0.8 * len(pairs))  # 80% training, 20% validation\n",
    "train_pairs = pairs[0:train_size]\n",
    "val_pairs = pairs[train_size:]\n",
    "\n",
    "\n",
    "# Make pairwise datasets for training\n",
    "max_length = 550\n",
    "train_dataset = PairwiseDataset(train_pairs, tokenizer, max_length=max_length)\n",
    "val_dataset = PairwiseDataset(val_pairs, tokenizer, max_length=max_length)\n",
    "\n",
    "# Create the collator to gather batches of pairwise comparisons\n",
    "data_collator = DataCollatorReward()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 324
    },
    "id": "6YkKOleIcoQF",
    "outputId": "2e69b9d4-97ba-4ee5-878f-84684d4d2b6f"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.9/dist-packages/transformers/optimization.py:391: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='50' max='50' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [50/50 01:34, Epoch 28/50]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Accuracy</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>10</td>\n",
       "      <td>0.948900</td>\n",
       "      <td>0.980852</td>\n",
       "      <td>0.142857</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>20</td>\n",
       "      <td>0.950700</td>\n",
       "      <td>0.978537</td>\n",
       "      <td>0.142857</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>30</td>\n",
       "      <td>0.966700</td>\n",
       "      <td>0.973360</td>\n",
       "      <td>0.142857</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>40</td>\n",
       "      <td>0.954500</td>\n",
       "      <td>0.966342</td>\n",
       "      <td>0.142857</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>50</td>\n",
       "      <td>0.907900</td>\n",
       "      <td>0.955675</td>\n",
       "      <td>0.142857</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "TrainOutput(global_step=50, training_loss=0.9457474708557129, metrics={'train_runtime': 96.6809, 'train_samples_per_second': 12.929, 'train_steps_per_second': 0.517, 'total_flos': 0.0, 'train_loss': 0.9457474708557129, 'epoch': 28.57})"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "training_args = TrainingArguments(\n",
    "    output_dir=\"rm_checkpoint/\",\n",
    "    num_train_epochs=50,\n",
    "    logging_steps=10,\n",
    "    gradient_accumulation_steps=4,\n",
    "    save_strategy=\"steps\",\n",
    "    evaluation_strategy=\"steps\",\n",
    "    per_device_train_batch_size=4,\n",
    "    per_device_eval_batch_size=4,\n",
    "    eval_accumulation_steps=1,\n",
    "    eval_steps=10,\n",
    "    save_steps=10,\n",
    "    warmup_steps=100,\n",
    "    logging_dir=\"./logs\",\n",
    "    fp16=True,\n",
    "    bf16=False,\n",
    "    learning_rate=1e-5,\n",
    "    # deepspeed=\"ds_config_gpt_j.json\",\n",
    "    save_total_limit=1\n",
    ")\n",
    "\n",
    "Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=train_dataset,\n",
    "    compute_metrics=compute_metrics,\n",
    "    eval_dataset=val_dataset,\n",
    "    data_collator=data_collator,\n",
    ").train()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "id": "Z5KzwZ0lifpX"
   },
   "source": [
    "## 6. Tune language model using PPO with our preference model\n",
    "\n",
    "\n",
    "Once we have our reward model, we can traing our model using PPO. We can find more details about this setup with the trlX libarary [here](https://github.com/CarperAI/trlx/tree/main/examples/summarize_rlhf). \n",
    "\n",
    "```\n",
    "accelerate launch --config_file configs/default_accelerate_config.yaml trlx_gptj_text_summarization.py\n",
    "```\n",
    "\n",
    "Note: Due to [limitations in the trlX library](https://github.com/CarperAI/trlx/issues/211), training the language model cannot be performed in a Colab environment. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "InsgE8PRtT0W",
    "outputId": "ce8ad2ae-da2c-4e05-e77d-7ba508f799b0"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/content/trlx/examples/summarize_rlhf\n"
     ]
    }
   ],
   "source": [
    "# chang to summarize example directory\n",
    "os.chdir('/content/trlx/examples/summarize_rlhf/')\n",
    "print(os.getcwd())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "id": "ZmGb3nMRu1-V"
   },
   "outputs": [],
   "source": [
    "import os\n",
    "from typing import List\n",
    "\n",
    "import torch\n",
    "from datasets import load_dataset\n",
    "from reward_model.reward_model import GPTRewardModel\n",
    "from tqdm import tqdm\n",
    "from transformers import AutoTokenizer\n",
    "\n",
    "import trlx\n",
    "from trlx.data.configs import (\n",
    "    ModelConfig,\n",
    "    OptimizerConfig,\n",
    "    SchedulerConfig,\n",
    "    TokenizerConfig,\n",
    "    TrainConfig,\n",
    "    TRLConfig,\n",
    ")\n",
    "from trlx.models.modeling_ppo import PPOConfig"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "UziQ7Gyjs-ra"
   },
   "outputs": [],
   "source": [
    "REWARD_CHECKPOINT_PATH = \"reward_model/rm_checkpoint/checkpoint-50/pytorch_model.bin\"\n",
    "SFT_MODEL_PATH = \"gpt2\"\n",
    "\n",
    "config = TRLConfig(\n",
    "    train=TrainConfig(\n",
    "        seq_length=550,\n",
    "        epochs=50,\n",
    "        total_steps=100000,\n",
    "        batch_size=4,\n",
    "        checkpoint_interval=10000,\n",
    "        eval_interval=200,\n",
    "        pipeline=\"PromptPipeline\",\n",
    "        trainer=\"AcceleratePPOTrainer\",\n",
    "    ),\n",
    "    model=ModelConfig(\n",
    "        model_path=\"gpt2\",\n",
    "        num_layers_unfrozen=8,\n",
    "    ),\n",
    "    tokenizer=TokenizerConfig(\n",
    "        tokenizer_path=\"gpt2\",\n",
    "        truncation_side=\"right\",\n",
    "    ),\n",
    "    optimizer=OptimizerConfig(\n",
    "        name=\"adamw\",\n",
    "        kwargs={\n",
    "            \"lr\": 5.0e-6,\n",
    "            \"betas\": [0.9, 0.999],\n",
    "            \"eps\": 1.0e-8,\n",
    "            \"weight_decay\": 0.01,\n",
    "        },\n",
    "    ),\n",
    "    scheduler=SchedulerConfig(\n",
    "        name=\"cosine_annealing\",\n",
    "        kwargs={\n",
    "            \"T_max\": 100000,\n",
    "            \"eta_min\": 5.0e-6,\n",
    "        },\n",
    "    ),\n",
    "    method=PPOConfig(\n",
    "        name=\"PPOConfig\",\n",
    "        num_rollouts=128,\n",
    "        chunk_size=16,\n",
    "        ppo_epochs=4,\n",
    "        init_kl_coef=0.1,\n",
    "        target=6,\n",
    "        horizon=10000,\n",
    "        gamma=1,\n",
    "        lam=0.95,\n",
    "        cliprange=0.2,\n",
    "        cliprange_value=0.2,\n",
    "        vf_coef=0.2,\n",
    "        scale_reward=None,\n",
    "        ref_mean=None,\n",
    "        ref_std=None,\n",
    "        cliprange_reward=10,\n",
    "        gen_kwargs={\n",
    "            \"max_new_tokens\": 50,\n",
    "        },\n",
    "    ),\n",
    ")\n",
    "\n",
    "\n",
    "# Load the pre-trained reward model\n",
    "rw_tokenizer = AutoTokenizer.from_pretrained(\"gpt2\")\n",
    "rw_tokenizer.pad_token = rw_tokenizer.eos_token\n",
    "rw_model = GPTRewardModel(SFT_MODEL_PATH)\n",
    "rw_model.load_state_dict(torch.load(REWARD_CHECKPOINT_PATH))\n",
    "rw_model.half()\n",
    "rw_model.eval()\n",
    "# rw_device = torch.device(\"cuda:{}\".format(1))  # set reward model device\n",
    "# rw_model.to(rw_device)\n",
    "\n",
    "def get_scores(samples: List[str]):\n",
    "    scores_list = []\n",
    "    batch_size = 2\n",
    "    for i in range(0, len(samples), batch_size):\n",
    "        sub_samples = samples[i : i + batch_size]\n",
    "        sub_samples = [\"<|startoftext|>\" + chosen + \"<|endoftext|>\" for chosen in sub_samples]\n",
    "        encodings_dict = rw_tokenizer(\n",
    "            sub_samples,\n",
    "            truncation=True,\n",
    "            max_length=config.train.seq_length,\n",
    "            padding=\"max_length\",\n",
    "            return_tensors=\"pt\",\n",
    "        )\n",
    "        input_ids = encodings_dict[\"input_ids\"].to(rw_device)\n",
    "        attn_masks = encodings_dict[\"attention_mask\"].to(rw_device)\n",
    "        input_ids = input_ids.repeat(2, 1)\n",
    "        attn_masks = attn_masks.repeat(2, 1)\n",
    "        with torch.no_grad():\n",
    "            sub_scores = rw_model(input_ids=input_ids, attention_mask=attn_masks)\n",
    "        scores_list.append(sub_scores[\"chosen_end_scores\"])\n",
    "    scores = torch.cat(scores_list, dim=0)\n",
    "    return scores\n",
    "\n",
    "def get_prompt_dataset(prompts, max_length):\n",
    "    \"\"\"\n",
    "    Get the prompt after T5 decoding to make sure dictionary\n",
    "    of prompts and summaries is consistent decode prompt from trlX pipeline\n",
    "    \"\"\"\n",
    "    formatted_prompts = []\n",
    "    for i in tqdm(range(len(prompts))):\n",
    "        tmp = tokenizer.decode(\n",
    "            tokenizer(\n",
    "                prompts[i].split(\"TL;DR:\")[0],\n",
    "                truncation=True,\n",
    "                max_length=max_length - 5,  # to make sure \"TL;DR\" dont get truncated\n",
    "                add_special_tokens=False,\n",
    "            )[\"input_ids\"],\n",
    "            skip_special_tokens=True,\n",
    "        ).strip()\n",
    "        tmp = tmp + \"\\nTL;DR:\"\n",
    "        tmp = tokenizer.decode(\n",
    "            tokenizer(tmp, truncation=True, max_length=max_length, add_special_tokens=False)[\"input_ids\"],\n",
    "            skip_special_tokens=True,\n",
    "        ).strip()\n",
    "        formatted_prompts.append(tmp)\n",
    "    return formatted_prompts\n",
    "\n",
    "def reward_fn(samples: List[str], **kwargs):\n",
    "    original_samples = [text.split(\"TL;DR:\")[0] + \"TL;DR: \" for text in samples]\n",
    "    original_samples = [text + post_summary_dict[text.strip()] for text in original_samples]\n",
    "    original_scores = get_scores(original_samples)\n",
    "    scores = get_scores(samples)\n",
    "    norms_scores = scores - original_scores\n",
    "    return norms_scores\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(config.tokenizer.tokenizer_path)\n",
    "tokenizer.pad_token = tokenizer.eos_token\n",
    "tokenizer.padding_side = \"left\"\n",
    "max_length_input = config.train.seq_length - config.method.gen_kwargs[\"max_new_tokens\"]\n",
    "\n",
    "dataset = load_dataset(\"CarperAI/openai_summarize_tldr\")\n",
    "\n",
    "# Store data into prompt and label pairs\n",
    "train_set = [(sample[\"prompt\"], sample[\"label\"]) for sample in dataset[\"train\"]]\n",
    "val_set = [(sample[\"prompt\"], sample[\"label\"]) for sample in dataset[\"valid\"]]\n",
    "\n",
    "# Split contents into summaries and labels\n",
    "train_posts, train_summaries = zip(*train_set)\n",
    "val_posts, val_summaries = zip(*val_set)\n",
    "\n",
    "# Get the OpenAI summaries\n",
    "post_summary_dict = {}\n",
    "train_prompts = get_prompt_dataset(train_posts, max_length_input)\n",
    "for i in range(len(train_prompts)):\n",
    "    post_summary_dict[train_prompts[i]] = train_summaries[i]\n",
    "val_prompts = get_prompt_dataset(val_posts, max_length_input)\n",
    "for i in range(len(val_prompts)):\n",
    "    post_summary_dict[val_prompts[i]] = val_summaries[i]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "35QMC3coxy-Y"
   },
   "outputs": [],
   "source": [
    "trainer = trlx.train(\n",
    "    reward_fn=reward_fn,\n",
    "    prompts=train_prompts,\n",
    "    eval_prompts=val_prompts[0:1000],  # sampling 1000 validation prompts for evaluation speed in training\n",
    "    config=config,\n",
    ")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "id": "xmg4StF22zrO"
   },
   "source": [
    "## 7. References \n",
    "- [Implementing RLHF: Learning to Summarize with trlX](https://wandb.ai/carperai/summarize_RLHF/reports/Implementing-RLHF-Learning-to-Summarize-with-trlX--VmlldzozMzAwODM2)\n",
    "\n",
    "- [General overview about RLHF](https://huggingface.co/blog/rlhf)\n",
    "- [Another end-to-end example with trlX](https://wandb.ai/carperai/summarize_RLHF/reports/Implementing-RLHF-Learning-to-Summarize-with-trlX--VmlldzozMzAwODM2)\n",
    "- [Similar human-in-the-loop annotation framework](https://github.com/CarperAI/cheese/tree/main/examples)\n",
    "- [Antropic harmless RLHF paper](https://arxiv.org/pdf/2204.05862.pdf) and [blog about CAI general principles](https://lifearchitect.ai/anthropic/)"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "provenance": []
  },
  "gpuClass": "standard",
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
