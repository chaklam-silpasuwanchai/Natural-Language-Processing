{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "91cb4a47-eb7c-4797-b6a0-01db2625ceba",
   "metadata": {},
   "source": [
    "# Hugging Face \n",
    "\n",
    "Your go-to tool for using any pretrained models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "0f122cc1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'4.36.2'"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import transformers\n",
    "transformers.__version__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "cf8e121b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'0.4.1'"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import evaluate #metrics\n",
    "evaluate.__version__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "88099160",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'2.16.0'"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import datasets\n",
    "datasets.__version__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "e7ff4d5e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'0.25.0'"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import accelerate\n",
    "accelerate.__version__"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "28998afe-e38c-4925-9c31-da2fc8d4b5bb",
   "metadata": {},
   "source": [
    "## 1. Pipeline \n",
    "\n",
    "The most basic thing in Huggingface; you insert the pretrained model, and just use it for inference."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "cf28fa84",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'label': 'NEGATIVE', 'score': 0.9867877960205078}]"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#sentiment analysis\n",
    "from transformers import pipeline\n",
    "\n",
    "clf = pipeline(\"sentiment-analysis\", model=\"distilbert-base-uncased-finetuned-sst-2-english\")\n",
    "clf(\"I do not love huggingface so much\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "41adab3d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'sequence': 'This is a NLP course on Huggingface',\n",
       " 'labels': ['tech', 'education', 'sports'],\n",
       " 'scores': [0.585610032081604, 0.39745786786079407, 0.016932116821408272]}"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "clf = pipeline(\"zero-shot-classification\", model=\"facebook/bart-large-mnli\")\n",
    "clf(\"This is a NLP course on Huggingface\", candidate_labels=[\"education\", \"tech\", \"sports\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "df1dd248",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[{'generated_text': 'AI is transforming our everyday lives into the most vibrant, safe and fulfilling life for our members.\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n'},\n",
       " {'generated_text': 'AI is transforming our everyday lives. Even the most modest, affluent, and socially-connected, people we know may feel a little bit of guilt about doing so.'}]"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gen = pipeline(\"text-generation\", model=\"distilgpt2\")\n",
    "gen(\"AI is transforming our everyday lives\", max_length=100, num_return_sequences=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "d0113a43",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at distilroberta-base were not used when initializing RobertaForMaskedLM: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "- This IS expected if you are initializing RobertaForMaskedLM from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing RobertaForMaskedLM from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[{'score': 0.15232914686203003,\n",
       "  'token': 2239,\n",
       "  'token_str': ' learning',\n",
       "  'sequence': 'Chaky loves to teach deep learning.'},\n",
       " {'score': 0.10399948805570602,\n",
       "  'token': 9589,\n",
       "  'token_str': ' breathing',\n",
       "  'sequence': 'Chaky loves to teach deep breathing.'},\n",
       " {'score': 0.07009342312812805,\n",
       "  'token': 30079,\n",
       "  'token_str': ' truths',\n",
       "  'sequence': 'Chaky loves to teach deep truths.'}]"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mlm = pipeline('fill-mask', model=\"distilroberta-base\")\n",
    "mlm(\"Chaky loves to teach deep <mask>.\", top_k=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "5728a0fa",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'score': 0.9159268140792847, 'start': 40, 'end': 43, 'answer': 'AIT'}"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "qa = pipeline(\"question-answering\", model=\"distilbert-base-cased-distilled-squad\")\n",
    "qa(question=\"Where to Chaky work?\", context=\"My name is Chaky and I love to teach at AIT.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "289e945b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at distilroberta-base were not used when initializing RobertaForMaskedLM: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "- This IS expected if you are initializing RobertaForMaskedLM from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing RobertaForMaskedLM from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[{'score': 0.10098593682050705,\n",
       "  'token': 35698,\n",
       "  'token_str': ' waitress',\n",
       "  'sequence': 'This woman works as a waitress.'},\n",
       " {'score': 0.08963349461555481,\n",
       "  'token': 28894,\n",
       "  'token_str': ' translator',\n",
       "  'sequence': 'This woman works as a translator.'},\n",
       " {'score': 0.07987944036722183,\n",
       "  'token': 9008,\n",
       "  'token_str': ' nurse',\n",
       "  'sequence': 'This woman works as a nurse.'},\n",
       " {'score': 0.06407161056995392,\n",
       "  'token': 33080,\n",
       "  'token_str': ' bartender',\n",
       "  'sequence': 'This woman works as a bartender.'},\n",
       " {'score': 0.04693792760372162,\n",
       "  'token': 8298,\n",
       "  'token_str': ' consultant',\n",
       "  'sequence': 'This woman works as a consultant.'}]"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#gender bias\n",
    "mlm = pipeline(\"fill-mask\", model=\"distilroberta-base\")\n",
    "result = mlm(\"This woman works as a <mask>.\")\n",
    "result"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4b9e6400-af47-4939-ba86-2ce0dfab6ffe",
   "metadata": {},
   "source": [
    "## 2. Tokenization\n",
    "\n",
    "The first component of the pipeline."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "67ba4827",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer\n",
    "\n",
    "checkpoint = \"distilbert-base-uncased-finetuned-sst-2-english\"\n",
    "tokenizer  = AutoTokenizer.from_pretrained(checkpoint)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "40cea17e",
   "metadata": {},
   "outputs": [],
   "source": [
    "raw_inputs = ['Chaky has been waiting in queue for sushi.',\n",
    "              \"Huggingface can do lots of stuffs so make sure you try everything.\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "92052cf1",
   "metadata": {},
   "outputs": [],
   "source": [
    "inputs = tokenizer(raw_inputs, padding=True, truncation=True, return_tensors='pt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "b752c358",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'input_ids': tensor([[  101, 15775,  4801,  2038,  2042,  3403,  1999, 24240,  2005, 10514,\n",
       "          6182,  1012,   102,     0,     0,     0,     0],\n",
       "        [  101, 17662, 12172,  2064,  2079,  7167,  1997,  4933,  2015,  2061,\n",
       "          2191,  2469,  2017,  3046,  2673,  1012,   102]]), 'attention_mask': tensor([[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0],\n",
       "        [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]])}"
      ]
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "inputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "c262870c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'[CLS] huggingface can do lots of stuffs so make sure you try everything. [SEP]'"
      ]
     },
     "execution_count": 64,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.decode([  101, 17662, 12172,  2064,  2079,  7167,  1997,  4933,  2015,  2061,\n",
    "          2191,  2469,  2017,  3046,  2673,  1012,   102])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0cc53105-c4a1-42dc-8ff7-34ff33a41e61",
   "metadata": {},
   "source": [
    "## 3. Model\n",
    "\n",
    "The second component of Pipeline (after Tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "b70d0a67",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoModel\n",
    "\n",
    "checkpoint = \"distilbert-base-uncased-finetuned-sst-2-english\"\n",
    "model      = AutoModel.from_pretrained(checkpoint)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "b393515a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'input_ids': tensor([[  101, 15775,  4801,  2038,  2042,  3403,  1999, 24240,  2005, 10514,\n",
       "          6182,  1012,   102,     0,     0,     0,     0],\n",
       "        [  101, 17662, 12172,  2064,  2079,  7167,  1997,  4933,  2015,  2061,\n",
       "          2191,  2469,  2017,  3046,  2673,  1012,   102]]), 'attention_mask': tensor([[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0],\n",
       "        [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]])}"
      ]
     },
     "execution_count": 66,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "inputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "37f22619",
   "metadata": {},
   "outputs": [],
   "source": [
    "outputs = model(**inputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "57c29556",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([2, 17, 768])"
      ]
     },
     "execution_count": 70,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "outputs.last_hidden_state.shape  #(batch size, seq length, hidden state of this model)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "83052a32-a6f9-48a1-952a-dc173d31c0fd",
   "metadata": {},
   "source": [
    "## 4. Postprocessing\n",
    "\n",
    "Last step of the Pipeline (after the Model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "50f8ab40",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoModelForSequenceClassification\n",
    "\n",
    "checkpoint = \"distilbert-base-uncased-finetuned-sst-2-english\"\n",
    "model      = AutoModelForSequenceClassification.from_pretrained(checkpoint)\n",
    "outputs    = model(**inputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "id": "1010c62c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 2.6290, -2.1602],\n",
       "        [-2.7711,  2.8129]], grad_fn=<AddmmBackward0>)"
      ]
     },
     "execution_count": 75,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "outputs.logits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "b8187381",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{0: 'NEGATIVE', 1: 'POSITIVE'}"
      ]
     },
     "execution_count": 73,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.config.id2label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "id": "d3f71e05",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "predictions = torch.nn.functional.softmax(outputs.logits, dim = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "id": "78f37c2c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0.9917, 0.0083],\n",
       "        [0.0037, 0.9963]], grad_fn=<SoftmaxBackward0>)"
      ]
     },
     "execution_count": 77,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predictions"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
