{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d2ad55e6-9d4c-43fd-a5ad-3dd6a9f9c8c7",
   "metadata": {},
   "source": [
    "# Token Classification (HuggingFace)\n",
    "\n",
    "- NER, POS Tagging, Chunking (which tokens belong to the same entity)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ec8aa15f-5a16-4a36-a232-df4f025e5a9a",
   "metadata": {},
   "source": [
    "## 1. Load the data\n",
    "\n",
    "CoNLL-2003 dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "c8d8341a-38a8-4f09-b022-68a028dfe8d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ['http_proxy']  = 'http://192.41.170.23:3128'\n",
    "os.environ['https_proxy'] = 'http://192.41.170.23:3128'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "120a0a04-add2-478d-b48a-f57713016bc3",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Found cached dataset conll2003 (/home/chaklams/.cache/huggingface/datasets/conll2003/conll2003/1.0.0/9a4d16a94f8674ba3466315300359b0acd891b68b6c8743ddf60b9c702adce98)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5edcbeeb75084c2e87018248e5d8701d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/3 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from datasets import load_dataset\n",
    "\n",
    "raw_datasets = load_dataset(\"conll2003\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "83adee20-7027-4b35-ad03-ab3f63a3d1aa",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train: Dataset({\n",
       "        features: ['id', 'tokens', 'pos_tags', 'chunk_tags', 'ner_tags'],\n",
       "        num_rows: 14041\n",
       "    })\n",
       "    validation: Dataset({\n",
       "        features: ['id', 'tokens', 'pos_tags', 'chunk_tags', 'ner_tags'],\n",
       "        num_rows: 3250\n",
       "    })\n",
       "    test: Dataset({\n",
       "        features: ['id', 'tokens', 'pos_tags', 'chunk_tags', 'ner_tags'],\n",
       "        num_rows: 3453\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "raw_datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "54723610-c042-435b-b0b1-2f10f7c03fef",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['EU', 'rejects', 'German', 'call', 'to', 'boycott', 'British', 'lamb', '.']"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "raw_datasets[\"train\"][0][\"tokens\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "9334a2b1-a3ac-4409-9307-45c5a80aa43b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[3, 0, 7, 0, 0, 0, 7, 0, 0]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "raw_datasets[\"train\"][0][\"ner_tags\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "cbe020ea-410f-4405-8585-8380bfb06c72",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[22, 42, 16, 21, 35, 37, 16, 21, 7]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "raw_datasets[\"train\"][0][\"pos_tags\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "89542e59-3ff0-439a-a66e-db4cf21a9983",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Sequence(feature=ClassLabel(names=['O', 'B-PER', 'I-PER', 'B-ORG', 'I-ORG', 'B-LOC', 'I-LOC', 'B-MISC', 'I-MISC'], id=None), length=-1, id=None)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "raw_datasets[\"train\"].features[\"ner_tags\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "01c7c4e0-3bea-4abe-b9c8-8053a0c81e63",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Sequence(feature=ClassLabel(names=['\"', \"''\", '#', '$', '(', ')', ',', '.', ':', '``', 'CC', 'CD', 'DT', 'EX', 'FW', 'IN', 'JJ', 'JJR', 'JJS', 'LS', 'MD', 'NN', 'NNP', 'NNPS', 'NNS', 'NN|SYM', 'PDT', 'POS', 'PRP', 'PRP$', 'RB', 'RBR', 'RBS', 'RP', 'SYM', 'TO', 'UH', 'VB', 'VBD', 'VBG', 'VBN', 'VBP', 'VBZ', 'WDT', 'WP', 'WP$', 'WRB'], id=None), length=-1, id=None)"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "raw_datasets[\"train\"].features[\"pos_tags\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "cdfe4444-6e73-46f0-8dd1-2ae20305dc58",
   "metadata": {},
   "outputs": [],
   "source": [
    "ner_features = raw_datasets[\"train\"].features[\"ner_tags\"]\n",
    "label_names  = ner_features.feature.names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "459fbb3d-e1f1-4b46-af47-249830e74b5d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['O', 'B-PER', 'I-PER', 'B-ORG', 'I-ORG', 'B-LOC', 'I-LOC', 'B-MISC', 'I-MISC']"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "label_names"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d7e74043-e1e4-46a7-940a-b1ddd759cc30",
   "metadata": {},
   "source": [
    "## 2. Preprocessing\n",
    "\n",
    "Tokenization (numericalization), aligning labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "d2e01488-6ce8-4f48-9533-733e20823401",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer\n",
    "\n",
    "checkpoint = \"bert-base-cased\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(checkpoint)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "bfa894ef-03bf-4a57-b5f3-dc14fffbd504",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'input_ids': [101, 24705, 3781, 7871, 1996, 3776, 102], 'token_type_ids': [0, 0, 0, 0, 0, 0, 0], 'attention_mask': [1, 1, 1, 1, 1, 1, 1]}"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer(\"Chaky loves deep learning\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "196841a9-fa21-48b4-9b3b-81ba5e0a97db",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'[CLS] Chaky loves deep learning [SEP]'"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.decode([101, 24705, 3781, 7871, 1996, 3776, 102])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "bcdbc4ae-f722-4cb6-85fe-23f75538986d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.is_fast #basically a internal Huggingface\n",
    "#optimization that makes its tokenizer very fast"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "68f8d0a0-d18c-4d89-93ee-cbfd38c36ea0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['EU', 'rejects', 'German', 'call', 'to', 'boycott', 'British', 'lamb', '.']"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokens = raw_datasets[\"train\"][0][\"tokens\"]\n",
    "tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "d620ea7b-386a-4d78-a63a-d490d2f13fbd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'input_ids': [101, 7270, 22961, 1528, 1840, 1106, 21423, 1418, 2495, 12913, 119, 102], 'token_type_ids': [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], 'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]}"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#we have to aware that our inputs are already\n",
    "#tokenized.....\n",
    "\n",
    "inputs = tokenizer(tokens, is_split_into_words=True)\n",
    "inputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "5e3030de-afc3-4522-b1db-d3e323d69776",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['[CLS]',\n",
       " 'EU',\n",
       " 'rejects',\n",
       " 'German',\n",
       " 'call',\n",
       " 'to',\n",
       " 'boycott',\n",
       " 'British',\n",
       " 'la',\n",
       " '##mb',\n",
       " '.',\n",
       " '[SEP]']"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "inputs.tokens()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "0f89630e-d72e-47de-8377-b46f3e6dcce0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[3, 0, 7, 0, 0, 0, 7, 0, 0]"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "raw_datasets[\"train\"][0][\"ner_tags\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "cf35bb01-b67a-4f24-a6c6-a9a30478412e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[None, 0, 1, 2, 3, 4, 5, 6, 7, 7, 8, None]"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "inputs.word_ids()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "c5299602-97d9-4c67-aac0-6e9ef31c1f2b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def align_labels_with_tokens(labels, word_ids):\n",
    "    new_labels = []\n",
    "    current_word = None\n",
    "    \n",
    "    for word_id in word_ids:\n",
    "        \n",
    "        if word_id != current_word:\n",
    "            #Start of a new word!\n",
    "            current_word = word_id\n",
    "            label = -100 if word_id is None else labels[word_id]\n",
    "            new_labels.append(label)\n",
    "        \n",
    "        elif word_id is None:\n",
    "            new_labels.append(-100) #-100 is a default index to ignore for huggingface\n",
    "            \n",
    "        else:\n",
    "            #same word as previous token\n",
    "            label = labels[word_id]\n",
    "            if label % 2 == 1:\n",
    "                label += 1\n",
    "            new_labels.append(label)\n",
    "            \n",
    "    return new_labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "d9f39ef6-c5f6-45fb-a5a4-893ab30962be",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[3, 0, 7, 0, 0, 0, 7, 0, 0]\n",
      "[-100, 3, 0, 7, 0, 0, 0, 7, 0, 0, 0, -100]\n"
     ]
    }
   ],
   "source": [
    "labels = raw_datasets[\"train\"][0][\"ner_tags\"]\n",
    "word_ids = inputs.word_ids()\n",
    "print(labels)\n",
    "print(align_labels_with_tokens(labels, word_ids))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "1f012355-798d-4a05-8841-923ada1326b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenize_and_align_labels(examples):\n",
    "    tokenized_inputs = tokenizer(examples[\"tokens\"], truncation=True, is_split_into_words=True)\n",
    "    \n",
    "    all_labels = examples[\"ner_tags\"]\n",
    "    new_labels = []\n",
    "    for i, labels in enumerate(all_labels):\n",
    "        word_ids = tokenized_inputs.word_ids(i)\n",
    "        new_labels.append(align_labels_with_tokens(labels, word_ids))\n",
    "    \n",
    "    tokenized_inputs[\"labels\"] = new_labels\n",
    "    return tokenized_inputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "d6e0e97e-d3a7-4556-9035-8149b62efb3e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8e294e56efda4702ae9efc4fd4871020",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/15 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading cached processed dataset at /home/chaklams/.cache/huggingface/datasets/conll2003/conll2003/1.0.0/9a4d16a94f8674ba3466315300359b0acd891b68b6c8743ddf60b9c702adce98/cache-b255492edbda1b2a.arrow\n",
      "Loading cached processed dataset at /home/chaklams/.cache/huggingface/datasets/conll2003/conll2003/1.0.0/9a4d16a94f8674ba3466315300359b0acd891b68b6c8743ddf60b9c702adce98/cache-68b2047c122dc838.arrow\n"
     ]
    }
   ],
   "source": [
    "tokenized_datasets = raw_datasets.map(\n",
    "    tokenize_and_align_labels, batched=True, remove_columns=raw_datasets[\"train\"].column_names,)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "c003a623-ddda-4998-a6c0-4425c83b8f81",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train: Dataset({\n",
       "        features: ['input_ids', 'token_type_ids', 'attention_mask', 'labels'],\n",
       "        num_rows: 14041\n",
       "    })\n",
       "    validation: Dataset({\n",
       "        features: ['input_ids', 'token_type_ids', 'attention_mask', 'labels'],\n",
       "        num_rows: 3250\n",
       "    })\n",
       "    test: Dataset({\n",
       "        features: ['input_ids', 'token_type_ids', 'attention_mask', 'labels'],\n",
       "        num_rows: 3453\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenized_datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "a9cf0feb-d1b6-49ba-bc3b-873b11920454",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[101, 7270, 22961, 1528, 1840, 1106, 21423, 1418, 2495, 12913, 119, 102]"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenized_datasets[\"train\"][0]['input_ids']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "a420e063-626a-4ab2-9914-adff0298a914",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'[CLS] EU rejects German call to boycott British lamb. [SEP]'"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.decode(tokenized_datasets[\"train\"][0]['input_ids'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "23d0a7d3-42db-4ac7-9ca7-97ec77c99135",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenized_datasets[\"train\"][0]['token_type_ids']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "d095a3d0-dc5a-4e76-98af-d5316f5040fe",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenized_datasets[\"train\"][0]['attention_mask']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "a16d0164-1ae4-4314-aacf-f6f833b2ff78",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[-100, 3, 0, 7, 0, 0, 0, 7, 0, 0, 0, -100]"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenized_datasets[\"train\"][0]['labels']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5bda154b-ea07-42ca-b18a-c3febf66ee73",
   "metadata": {},
   "source": [
    "## 3. Dataloader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "c92406ec-a1f4-4c69-ba74-e30baf7b725d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import DataCollatorForTokenClassification\n",
    "#huggingface is very kind to make a data collator for each pipeline\n",
    "\n",
    "data_collator = DataCollatorForTokenClassification(tokenizer=tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "006102aa-1122-4282-a4a6-2e7a635fe49f",
   "metadata": {},
   "outputs": [],
   "source": [
    "test = [tokenized_datasets[\"train\"][i] for i in range(2)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "c8829ffa-5c80-4645-b985-827bf368d783",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'input_ids': tensor([[  101,  7270, 22961,  1528,  1840,  1106, 21423,  1418,  2495, 12913,\n",
       "            119,   102],\n",
       "         [  101,  1943, 14428,   102,     0,     0,     0,     0,     0,     0,\n",
       "              0,     0]]),\n",
       " 'token_type_ids': tensor([[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
       "         [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]]),\n",
       " 'attention_mask': tensor([[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1],\n",
       "         [1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0]]),\n",
       " 'labels': tensor([[-100,    3,    0,    7,    0,    0,    0,    7,    0,    0,    0, -100],\n",
       "         [-100,    1,    2, -100, -100, -100, -100, -100, -100, -100, -100, -100]])}"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_collator(test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "b9adb3a6-eecf-41ca-aa41-5a02a494dea2",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import DataLoader\n",
    "\n",
    "train_loader = DataLoader(tokenized_datasets[\"train\"],      shuffle=True,\n",
    "                          collate_fn=data_collator, batch_size=8)\n",
    "val_loader   = DataLoader(tokenized_datasets[\"validation\"],\n",
    "                          collate_fn=data_collator, batch_size=8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "861fb952-b9c6-451f-a8dd-62a8d2d23d8a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# for batch in train_loader:\n",
    "#     print(batch)\n",
    "#     break"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "879bb709-fbea-4aa6-a8be-b8d82e03165a",
   "metadata": {},
   "source": [
    "## 4. Model\n",
    "\n",
    "The second part of the Pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5ed9b7d3-1242-401f-961b-55a551597830",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoModelForTokenClassification\n",
    "#basically, it imports a pretrained model, and add linear layers and only train that layers....\n",
    "\n",
    "model = AutoModelForTokenClassification(\n",
    "    model_checkpoint, id2label=id2label,label2id=label2id)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
