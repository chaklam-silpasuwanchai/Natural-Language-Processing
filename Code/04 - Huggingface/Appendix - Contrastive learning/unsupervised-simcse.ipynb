{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# [SimCSE: Simple Contrastive Learning of Sentence Embeddings](https://aclanthology.org/2021.emnlp-main.552.pdf)\n",
    "- unsupervised SimCSE simply predicts the input sentence itself with only dropout\n",
    "- obtain two different embeddings as “positive pairs”. Then we take other sentences in the same mini-batch as “negatives”\n",
    "- the model predicts the positive one among negatives\n",
    "\n",
    "<img src=\"./figures/simcse_architecture.png\" width=\"700\" height=\"300\">\n",
    "\n",
    "### Reference Code\n",
    "- https://github.com/bhuvanakundumani/SimCSE_unsupervised/tree/main\n",
    "- https://github.com/hppRC/simple-simcse/blob/main"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "device(type='cuda')"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os\n",
    "import math\n",
    "import re\n",
    "from random import *\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "from tqdm.auto import tqdm\n",
    "\n",
    "# Set GPU device\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"1\"\n",
    "\n",
    "os.environ['http_proxy']  = 'http://192.41.170.23:3128'\n",
    "os.environ['https_proxy'] = 'http://192.41.170.23:3128'\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "device"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Unsupervised SimCSE"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Preprocessing\n",
    "\n",
    "[Download wiki1m_for_simcse.txt](https://huggingface.co/datasets/princeton-nlp/datasets-for-simcse)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from datasets import load_dataset\n",
    "\n",
    "# dataset = load_dataset(\"princeton-nlp/datasets-for-simcse\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((846129, 2), (149318, 2))"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from datasets import DatasetDict, Dataset\n",
    "\n",
    "csv_path = './wiki1m_for_simcse.txt'\n",
    "# Assuming you have already read the CSV file and created the data list\n",
    "dataset_df = pd.read_csv(csv_path, header=None, names=['text'], sep='\\t')\n",
    "source_texts = dataset_df[\"text\"].values\n",
    "target_texts = dataset_df[\"text\"].values\n",
    "data = list(zip(source_texts, target_texts))\n",
    "\n",
    "# Split the data into training and validation sets\n",
    "train_data, val_data = train_test_split(data, test_size=0.15, random_state=42, shuffle=False)\n",
    "\n",
    "# Create datasets\n",
    "train_dataset = Dataset.from_pandas(\n",
    "    pd.DataFrame(train_data, columns=['source_text', 'target_text']))\n",
    "val_dataset = Dataset.from_pandas(\n",
    "    pd.DataFrame(val_data, columns=['source_text', 'target_text']))\n",
    "\n",
    "train_dataset.shape, val_dataset.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train: Dataset({\n",
       "        features: ['source_text', 'target_text'],\n",
       "        num_rows: 10000\n",
       "    })\n",
       "    validation: Dataset({\n",
       "        features: ['source_text', 'target_text'],\n",
       "        num_rows: 1000\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Create a DatasetDict\n",
    "raw_dataset = DatasetDict({\n",
    "    'train': train_dataset.select(list(range(10000))),\n",
    "    'validation': val_dataset.select(list(range(1000)))\n",
    "})\n",
    "\n",
    "raw_dataset\n",
    "# Now you can access your training and validation sets using dataset_dict['train'] and dataset_dict['validation']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'source_text': 'YMCA in South Australia',\n",
       " 'target_text': 'YMCA in South Australia'}"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "raw_dataset['train'][0]"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Model & Tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "BertModel(\n",
       "  (embeddings): BertEmbeddings(\n",
       "    (word_embeddings): Embedding(30522, 768, padding_idx=0)\n",
       "    (position_embeddings): Embedding(512, 768)\n",
       "    (token_type_embeddings): Embedding(2, 768)\n",
       "    (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "    (dropout): Dropout(p=0.1, inplace=False)\n",
       "  )\n",
       "  (encoder): BertEncoder(\n",
       "    (layer): ModuleList(\n",
       "      (0): BertLayer(\n",
       "        (attention): BertAttention(\n",
       "          (self): BertSelfAttention(\n",
       "            (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (output): BertSelfOutput(\n",
       "            (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (intermediate): BertIntermediate(\n",
       "          (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (intermediate_act_fn): GELUActivation()\n",
       "        )\n",
       "        (output): BertOutput(\n",
       "          (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "          (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (1): BertLayer(\n",
       "        (attention): BertAttention(\n",
       "          (self): BertSelfAttention(\n",
       "            (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (output): BertSelfOutput(\n",
       "            (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (intermediate): BertIntermediate(\n",
       "          (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (intermediate_act_fn): GELUActivation()\n",
       "        )\n",
       "        (output): BertOutput(\n",
       "          (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "          (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (2): BertLayer(\n",
       "        (attention): BertAttention(\n",
       "          (self): BertSelfAttention(\n",
       "            (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (output): BertSelfOutput(\n",
       "            (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (intermediate): BertIntermediate(\n",
       "          (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (intermediate_act_fn): GELUActivation()\n",
       "        )\n",
       "        (output): BertOutput(\n",
       "          (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "          (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (3): BertLayer(\n",
       "        (attention): BertAttention(\n",
       "          (self): BertSelfAttention(\n",
       "            (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (output): BertSelfOutput(\n",
       "            (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (intermediate): BertIntermediate(\n",
       "          (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (intermediate_act_fn): GELUActivation()\n",
       "        )\n",
       "        (output): BertOutput(\n",
       "          (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "          (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (4): BertLayer(\n",
       "        (attention): BertAttention(\n",
       "          (self): BertSelfAttention(\n",
       "            (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (output): BertSelfOutput(\n",
       "            (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (intermediate): BertIntermediate(\n",
       "          (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (intermediate_act_fn): GELUActivation()\n",
       "        )\n",
       "        (output): BertOutput(\n",
       "          (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "          (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (5): BertLayer(\n",
       "        (attention): BertAttention(\n",
       "          (self): BertSelfAttention(\n",
       "            (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (output): BertSelfOutput(\n",
       "            (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (intermediate): BertIntermediate(\n",
       "          (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (intermediate_act_fn): GELUActivation()\n",
       "        )\n",
       "        (output): BertOutput(\n",
       "          (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "          (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (6): BertLayer(\n",
       "        (attention): BertAttention(\n",
       "          (self): BertSelfAttention(\n",
       "            (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (output): BertSelfOutput(\n",
       "            (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (intermediate): BertIntermediate(\n",
       "          (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (intermediate_act_fn): GELUActivation()\n",
       "        )\n",
       "        (output): BertOutput(\n",
       "          (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "          (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (7): BertLayer(\n",
       "        (attention): BertAttention(\n",
       "          (self): BertSelfAttention(\n",
       "            (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (output): BertSelfOutput(\n",
       "            (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (intermediate): BertIntermediate(\n",
       "          (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (intermediate_act_fn): GELUActivation()\n",
       "        )\n",
       "        (output): BertOutput(\n",
       "          (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "          (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (8): BertLayer(\n",
       "        (attention): BertAttention(\n",
       "          (self): BertSelfAttention(\n",
       "            (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (output): BertSelfOutput(\n",
       "            (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (intermediate): BertIntermediate(\n",
       "          (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (intermediate_act_fn): GELUActivation()\n",
       "        )\n",
       "        (output): BertOutput(\n",
       "          (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "          (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (9): BertLayer(\n",
       "        (attention): BertAttention(\n",
       "          (self): BertSelfAttention(\n",
       "            (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (output): BertSelfOutput(\n",
       "            (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (intermediate): BertIntermediate(\n",
       "          (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (intermediate_act_fn): GELUActivation()\n",
       "        )\n",
       "        (output): BertOutput(\n",
       "          (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "          (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (10): BertLayer(\n",
       "        (attention): BertAttention(\n",
       "          (self): BertSelfAttention(\n",
       "            (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (output): BertSelfOutput(\n",
       "            (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (intermediate): BertIntermediate(\n",
       "          (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (intermediate_act_fn): GELUActivation()\n",
       "        )\n",
       "        (output): BertOutput(\n",
       "          (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "          (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (11): BertLayer(\n",
       "        (attention): BertAttention(\n",
       "          (self): BertSelfAttention(\n",
       "            (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (output): BertSelfOutput(\n",
       "            (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (intermediate): BertIntermediate(\n",
       "          (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (intermediate_act_fn): GELUActivation()\n",
       "        )\n",
       "        (output): BertOutput(\n",
       "          (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "          (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "  )\n",
       "  (pooler): BertPooler(\n",
       "    (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "    (activation): Tanh()\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from transformers import AutoConfig, AutoTokenizer, BertModel\n",
    "\n",
    "model_name_or_path = 'bert-base-uncased'\n",
    "\n",
    "# Loading tokenizer and config\n",
    "config = AutoConfig.from_pretrained(model_name_or_path)\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name_or_path)\n",
    "\n",
    "# start from a pretrained bert-base-uncased model\n",
    "model = BertModel.from_pretrained(model_name_or_path)\n",
    "model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "max_seq_length = 32\n",
    "batch_size = 32 \n",
    "\n",
    "def preprocess_function(examples):\n",
    "    source_texts = tokenizer(\n",
    "        examples['source_text'], max_length=max_seq_length, padding=\"max_length\", truncation=True)\n",
    "    target_texts = tokenizer(\n",
    "        examples['target_text'], max_length=max_seq_length, padding=\"max_length\", truncation=True)\n",
    "\n",
    "    return {\n",
    "        \"source_input_ids\": source_texts[\"input_ids\"],\n",
    "        \"source_attention_mask\": source_texts[\"attention_mask\"],\n",
    "        \"target_input_ids\": target_texts[\"input_ids\"],\n",
    "        \"target_attention_mask\": target_texts[\"attention_mask\"],\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "79373f2bda954a19a548914a83c7b88c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/10000 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "afb9adecc48d42d9ab4117d489ec5261",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/1000 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "tokenized_datasets = raw_dataset.map(\n",
    "    preprocess_function,\n",
    "    batched=True,\n",
    ")\n",
    "\n",
    "tokenized_datasets = tokenized_datasets.remove_columns(['source_text','target_text'])\n",
    "tokenized_datasets.set_format(\"torch\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train: Dataset({\n",
       "        features: ['source_input_ids', 'source_attention_mask', 'target_input_ids', 'target_attention_mask'],\n",
       "        num_rows: 10000\n",
       "    })\n",
       "    validation: Dataset({\n",
       "        features: ['source_input_ids', 'source_attention_mask', 'target_input_ids', 'target_attention_mask'],\n",
       "        num_rows: 1000\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenized_datasets"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Dataloader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import DataLoader\n",
    "from transformers import default_data_collator\n",
    "\n",
    "# Initialize the DataLoader with DataCollatorForLanguageModeling\n",
    "batch_size = 16\n",
    "\n",
    "train_dataloader = DataLoader(\n",
    "    tokenized_datasets['train'],\n",
    "    batch_size=batch_size,\n",
    "    shuffle=True,\n",
    "    collate_fn=default_data_collator,\n",
    ")\n",
    "\n",
    "eval_dataloader = DataLoader(\n",
    "    tokenized_datasets['validation'],\n",
    "    batch_size=batch_size,\n",
    "    collate_fn=default_data_collator,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "for batch in train_dataloader:\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_keys(['source_input_ids', 'source_attention_mask', 'target_input_ids', 'target_attention_mask'])"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "batch.keys()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Training"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Pooling\n",
    "SBERT adds a pooling operation to the output of BERT / RoBERTa to derive a fixed sized sentence embedding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define mean pooling function\n",
    "def mean_pool(token_embeds, attention_mask):\n",
    "    # reshape attention_mask to cover 768-dimension embeddings\n",
    "    in_mask = attention_mask.unsqueeze(-1).expand(\n",
    "        token_embeds.size()\n",
    "    ).float()\n",
    "    # perform mean-pooling but exclude padding tokens (specified by in_mask)\n",
    "    pool = torch.sum(token_embeds * in_mask, 1) / torch.clamp(\n",
    "        in_mask.sum(1), min=1e-9\n",
    "    )\n",
    "    return pool"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.optim import AdamW\n",
    "from transformers import get_linear_schedule_with_warmup\n",
    "\n",
    "num_train_epochs = 1\n",
    "gradient_accumulation_steps = 1\n",
    "learning_rate = 1e-5\n",
    "adam_epsilon = 1e-8\n",
    "warmup_proportion = 0.1\n",
    "weight_decay = 0.01\n",
    "\n",
    "num_train_optimization_steps = int(len(train_data) / batch_size / gradient_accumulation_steps) * num_train_epochs\n",
    "\n",
    "param_optimizer = list(model.named_parameters())\n",
    "no_decay = ['bias','LayerNorm.weight']\n",
    "optimizer_grouped_parameters = [\n",
    "    {'params': [p for n, p in param_optimizer if not any(nd in n for nd in no_decay)], 'weight_decay': weight_decay},\n",
    "    {'params': [p for n, p in param_optimizer if any(nd in n for nd in no_decay)], 'weight_decay': 0.0}\n",
    "    ]\n",
    "\n",
    "warmup_steps = int(warmup_proportion * num_train_optimization_steps)\n",
    "optimizer = AdamW(optimizer_grouped_parameters,lr=learning_rate, eps=adam_epsilon)\n",
    "scheduler = get_linear_schedule_with_warmup(\n",
    "    optimizer,\n",
    "    num_warmup_steps=warmup_steps,\n",
    "    num_training_steps=num_train_optimization_steps)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cosine Similarity \n",
    "\n",
    "$$\n",
    "\\text{sim}(h_1, h_2) = \\frac{{h_1 \\cdot h_2}}{{\\|h_1\\| \\cdot \\|h_2\\|}}\n",
    "$$\n",
    "\n",
    "## Contrastive Learning\n",
    "\n",
    "\n",
    "$$\n",
    "\\ell_i = -\\log \\frac{e^{\\text{sim}(\\mathbf{h}^{z_i}, \\mathbf{h}^{z'_i}) / \\tau}}{\\sum_{j=1}^{N} e^{\\text{sim}(\\mathbf{h}^{z_i}, \\mathbf{h}^{z'_j}) / \\tau}}\n",
    "$$\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn.functional as F\n",
    "\n",
    "def contrastive_loss(h_z, h_z_prime, temperature=1.0):\n",
    "    # Compute similarity scores for positive pair (i.e., h_z and h_z_prime)\n",
    "    sim_positive = F.cosine_similarity(h_z, h_z_prime, dim=-1) / temperature #Positive instance\n",
    "    # Compute similarity scores for negative pairs (i.e., h_z and other h_z')\n",
    "    sim_negative = F.cosine_similarity(h_z.unsqueeze(1), h_z_prime, dim=-1) / temperature #Negative instance\n",
    "    # Compute log probabilities\n",
    "    log_probs = F.log_softmax(torch.cat([sim_positive.unsqueeze(1), sim_negative], dim=1), dim=1)\n",
    "    # Negative log likelihood loss for the positive pair\n",
    "    loss = -log_probs[:, 0].mean()\n",
    "    return loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "#testing\n",
    "temperature = 1\n",
    "batch_size, embedding_dim = 16, 768\n",
    "h_z = torch.rand((batch_size, embedding_dim))\n",
    "h_z_prime = torch.rand((batch_size, embedding_dim))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([16, 768]), torch.Size([16, 768]))"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "h_z.shape, h_z_prime.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([0.7401, 0.7479, 0.7318, 0.7543, 0.7533, 0.7572, 0.7647, 0.7592, 0.7450,\n",
       "        0.7379, 0.7322, 0.7749, 0.7625, 0.7619, 0.7343, 0.7602])"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "F.cosine_similarity(h_z, h_z_prime, dim=-1) / temperature"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([16, 1, 768])"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "h_z.unsqueeze(1).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0.7401, 0.7551, 0.7343, 0.7484, 0.7522, 0.7621, 0.7389, 0.7435, 0.7422,\n",
       "         0.7484, 0.7330, 0.7532, 0.7458, 0.7675, 0.7463, 0.7539],\n",
       "        [0.7262, 0.7479, 0.7479, 0.7314, 0.7464, 0.7559, 0.7359, 0.7458, 0.7501,\n",
       "         0.7480, 0.7161, 0.7475, 0.7586, 0.7508, 0.7470, 0.7393],\n",
       "        [0.7446, 0.7332, 0.7318, 0.7468, 0.7445, 0.7429, 0.7447, 0.7427, 0.7430,\n",
       "         0.7331, 0.7407, 0.7499, 0.7416, 0.7418, 0.7334, 0.7383],\n",
       "        [0.7558, 0.7581, 0.7564, 0.7543, 0.7336, 0.7449, 0.7607, 0.7518, 0.7391,\n",
       "         0.7497, 0.7418, 0.7560, 0.7460, 0.7446, 0.7490, 0.7444],\n",
       "        [0.7542, 0.7483, 0.7345, 0.7429, 0.7533, 0.7488, 0.7483, 0.7523, 0.7333,\n",
       "         0.7582, 0.7431, 0.7406, 0.7584, 0.7435, 0.7573, 0.7576],\n",
       "        [0.7455, 0.7646, 0.7556, 0.7287, 0.7575, 0.7572, 0.7527, 0.7582, 0.7414,\n",
       "         0.7540, 0.7239, 0.7357, 0.7489, 0.7520, 0.7441, 0.7466],\n",
       "        [0.7478, 0.7592, 0.7608, 0.7549, 0.7878, 0.7397, 0.7647, 0.7597, 0.7397,\n",
       "         0.7574, 0.7474, 0.7619, 0.7766, 0.7445, 0.7429, 0.7410],\n",
       "        [0.7457, 0.7589, 0.7549, 0.7589, 0.7657, 0.7516, 0.7502, 0.7592, 0.7330,\n",
       "         0.7715, 0.7296, 0.7669, 0.7661, 0.7400, 0.7413, 0.7532],\n",
       "        [0.7630, 0.7555, 0.7447, 0.7558, 0.7515, 0.7423, 0.7631, 0.7601, 0.7450,\n",
       "         0.7512, 0.7294, 0.7498, 0.7236, 0.7383, 0.7458, 0.7480],\n",
       "        [0.7299, 0.7443, 0.7298, 0.7509, 0.7334, 0.7507, 0.7535, 0.7644, 0.7238,\n",
       "         0.7379, 0.7383, 0.7686, 0.7386, 0.7419, 0.7525, 0.7387],\n",
       "        [0.7415, 0.7628, 0.7394, 0.7410, 0.7479, 0.7399, 0.7524, 0.7491, 0.7457,\n",
       "         0.7528, 0.7322, 0.7494, 0.7546, 0.7456, 0.7373, 0.7524],\n",
       "        [0.7524, 0.7698, 0.7382, 0.7487, 0.7465, 0.7623, 0.7206, 0.7710, 0.7361,\n",
       "         0.7530, 0.7321, 0.7749, 0.7483, 0.7473, 0.7538, 0.7695],\n",
       "        [0.7316, 0.7437, 0.7412, 0.7416, 0.7305, 0.7257, 0.7422, 0.7337, 0.7042,\n",
       "         0.7305, 0.7190, 0.7142, 0.7625, 0.7374, 0.7373, 0.7285],\n",
       "        [0.7583, 0.7488, 0.7387, 0.7428, 0.7414, 0.7524, 0.7531, 0.7567, 0.7442,\n",
       "         0.7624, 0.7517, 0.7466, 0.7425, 0.7619, 0.7382, 0.7372],\n",
       "        [0.7483, 0.7596, 0.7535, 0.7512, 0.7666, 0.7450, 0.7497, 0.7326, 0.7421,\n",
       "         0.7515, 0.7443, 0.7569, 0.7451, 0.7223, 0.7343, 0.7444],\n",
       "        [0.7698, 0.7698, 0.7580, 0.7498, 0.7632, 0.7584, 0.7609, 0.7418, 0.7451,\n",
       "         0.7555, 0.7476, 0.7428, 0.7389, 0.7462, 0.7543, 0.7602]])"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "F.cosine_similarity(h_z.unsqueeze(1), h_z_prime, dim=-1) / temperature"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Contrastive Loss: 2.8297362327575684\n"
     ]
    }
   ],
   "source": [
    "loss = contrastive_loss(h_z, h_z_prime)\n",
    "print(f\"Contrastive Loss: {loss.item()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7b4a01a1de4d4605bc53adcc4b62595e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/625 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1 | loss = 2.033902\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "28f923f1caf14a02aae657b17256953c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/625 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 2 | loss = 2.023149\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "37e073d2316c488dbb57aca9c9216d12",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/625 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 3 | loss = 2.033118\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "eb964a20154e46288aeedd38fe2cf7ea",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/625 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 4 | loss = 2.020890\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "09d167bb81d147dcb067d0314192c804",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/625 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 5 | loss = 2.020044\n"
     ]
    }
   ],
   "source": [
    "from tqdm.auto import tqdm\n",
    "\n",
    "num_epoch = 5\n",
    "# 1 epoch should be enough, increase if wanted\n",
    "for epoch in range(num_epoch):\n",
    "    model.train()  \n",
    "    # initialize the dataloader loop with tqdm (tqdm == progress bar)\n",
    "    for step, batch in enumerate(tqdm(train_dataloader, leave=True)):\n",
    "        # zero all gradients on each new step\n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        # prepare batches and more all to the active device\n",
    "        inputs_ids_a = batch['source_input_ids'].to(device)\n",
    "        inputs_ids_b = batch['target_input_ids'].to(device)\n",
    "        attention_a = batch['source_attention_mask'].to(device)\n",
    "        attention_b = batch['target_attention_mask'].to(device)\n",
    "        \n",
    "        # extract token embeddings from BERT at last_hidden_state\n",
    "        u = model(inputs_ids_a, attention_mask=attention_a)  \n",
    "        v = model(inputs_ids_b, attention_mask=attention_b)  \n",
    "\n",
    "        u_last_hidden_state = u.last_hidden_state # all token embeddings A = batch_size, seq_len, hidden_dim\n",
    "        v_last_hidden_state = v.last_hidden_state # all token embeddings B = batch_size, seq_len, hidden_dim\n",
    "\n",
    "         # get the mean pooled vectors\n",
    "        u_mean_pool = mean_pool(u_last_hidden_state, attention_a) # batch_size, hidden_dim\n",
    "        v_mean_pool = mean_pool(v_last_hidden_state, attention_b) # batch_size, hidden_dim\n",
    "\n",
    "        loss = contrastive_loss(u_mean_pool, u_mean_pool)\n",
    "        # using loss, calculate gradients and then optimizerize\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        scheduler.step() # update learning rate scheduler\n",
    "        \n",
    "    print(f'Epoch: {epoch+1} | loss = {loss.item():.6f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "import torch\n",
    "\n",
    "class Similarity(nn.Module):\n",
    "    \"\"\" Dot product or cosine similarity \"\"\"\n",
    "    def __init__(self, temp):\n",
    "        super().__init__()\n",
    "        self.temp = temp\n",
    "        self.cos = nn.CosineSimilarity(dim=-1)\n",
    "\n",
    "    def forward(self, x, y):\n",
    "        return self.cos(x, y) / self.temp\n",
    "    \n",
    "cosine_similarity = Similarity(temp = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average Cosine Similarity: 0.9051\n"
     ]
    }
   ],
   "source": [
    "total_similarity = 0\n",
    "# from sklearn.metrics.pairwise import cosine_similarity\n",
    "\n",
    "with torch.no_grad():\n",
    "    for step, batch in enumerate(eval_dataloader):\n",
    "        # prepare batches and more all to the active device\n",
    "        inputs_ids_a = batch['source_input_ids'].to(device)\n",
    "        inputs_ids_b = batch['target_input_ids'].to(device)\n",
    "        attention_a = batch['source_attention_mask'].to(device)\n",
    "        attention_b = batch['target_attention_mask'].to(device)\n",
    "        \n",
    "        # extract token embeddings from BERT at last_hidden_state\n",
    "        u = model(inputs_ids_a, attention_mask=attention_a)  \n",
    "        v = model(inputs_ids_b, attention_mask=attention_b)  \n",
    "\n",
    "        u_last_hidden_state = u.last_hidden_state # all token embeddings A = batch_size, seq_len, hidden_dim\n",
    "        v_last_hidden_state = v.last_hidden_state # all token embeddings B = batch_size, seq_len, hidden_dim\n",
    "\n",
    "         # get the mean pooled vectors\n",
    "        u_mean_pool = mean_pool(u_last_hidden_state, attention_a) # batch_size, hidden_dim\n",
    "        v_mean_pool = mean_pool(v_last_hidden_state, attention_b) # batch_size, hidden_dim\n",
    "\n",
    "        similarity_score = cosine_similarity(u_mean_pool, v_mean_pool).mean()\n",
    "        total_similarity += similarity_score\n",
    "    \n",
    "average_similarity = total_similarity / len(eval_dataloader)\n",
    "print(f\"Average Cosine Similarity: {average_similarity:.4f}\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cosine Similarity: 0.5799\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "\n",
    "def calculate_similarity(model, tokenizer, sentence_a, sentence_b, device):\n",
    "    # Tokenize and convert sentences to input IDs and attention masks\n",
    "    inputs_a = tokenizer(sentence_a, return_tensors='pt', truncation=True, padding=True).to(device)\n",
    "    inputs_b = tokenizer(sentence_b, return_tensors='pt', truncation=True, padding=True).to(device)\n",
    "\n",
    "    # Move input IDs and attention masks to the active device\n",
    "    inputs_ids_a = inputs_a['input_ids']\n",
    "    attention_a = inputs_a['attention_mask']\n",
    "    inputs_ids_b = inputs_b['input_ids']\n",
    "    attention_b = inputs_b['attention_mask']\n",
    "\n",
    "    # Extract token embeddings from BERT\n",
    "    u = model(inputs_ids_a, attention_mask=attention_a)[0]  # all token embeddings A = batch_size, seq_len, hidden_dim\n",
    "    v = model(inputs_ids_b, attention_mask=attention_b)[0]  # all token embeddings B = batch_size, seq_len, hidden_dim\n",
    "\n",
    "    # Get the mean-pooled vectors\n",
    "    u = mean_pool(u, attention_a).detach().cpu().numpy().reshape(-1)  # batch_size, hidden_dim\n",
    "    v = mean_pool(v, attention_b).detach().cpu().numpy().reshape(-1)  # batch_size, hidden_dim\n",
    "\n",
    "    # Calculate cosine similarity\n",
    "    similarity_score = cosine_similarity(u.reshape(1, -1), v.reshape(1, -1))[0, 0]\n",
    "\n",
    "    return similarity_score\n",
    "\n",
    "# Example usage:\n",
    "sentence_a = 'Your contribution helped make it possible for us to provide our students with a quality education.'\n",
    "sentence_b = \"Your contributions were of no help with our students' education.\"\n",
    "similarity = calculate_similarity(model, tokenizer, sentence_a, sentence_b, device)\n",
    "print(f\"Cosine Similarity: {similarity:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cosine Similarity: 0.8270\n"
     ]
    }
   ],
   "source": [
    "# Example usage:\n",
    "sentence_a = 'Your contribution helped make it possible for us to provide our students with a quality education.'\n",
    "sentence_b = \"Your contributions were of help with our students' education.\"\n",
    "similarity = calculate_similarity(model, tokenizer, sentence_a, sentence_b, device)\n",
    "print(f\"Cosine Similarity: {similarity:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
