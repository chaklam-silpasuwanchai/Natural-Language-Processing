{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "5bb411cf-9b50-4d23-84cc-0f90f225bd08",
   "metadata": {},
   "source": [
    "# Soft Prompt\n",
    "\n",
    "Today, let's implement a simple soft prompt based on https://arxiv.org/abs/2104.08691v1 which allows us to only finetune the added weights while the model remains intact.\n",
    "\n",
    "<img src=\"img/soft_embedding.png\" width=\"200\">"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "945a566f-87dc-4f91-b4eb-009c15e6c671",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from transformers import GPT2LMHeadModel, GPT2TokenizerFast\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aa156625-8e85-413d-b935-462921947a7f",
   "metadata": {},
   "source": [
    "## 1. Load model\n",
    "\n",
    "Let's load the GPT2 language model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "b65bd308-ff5b-46bb-8f3d-501f42ac6cd0",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "#comment this if you are not using AIT proxy...\n",
    "import os\n",
    "os.environ['http_proxy']  = 'http://192.41.170.23:3128'\n",
    "os.environ['https_proxy'] = 'http://192.41.170.23:3128'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "8c449466-f947-47a5-a4d4-622b4bc84b87",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "tokenizer = GPT2TokenizerFast.from_pretrained(\"gpt2\")\n",
    "model = GPT2LMHeadModel.from_pretrained('gpt2')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eae5b5f0-0753-479e-8e92-af2757ecec8a",
   "metadata": {},
   "source": [
    "Let's examine the original embedding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "c6391eee-6e63-4fca-b948-c07328ee81d1",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Embedding(50257, 768)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.get_input_embeddings()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a1ae42e1-8728-4500-8ab6-150827a20625",
   "metadata": {},
   "source": [
    "## 2. Soft embeddings\n",
    "\n",
    "Let's define a soft embedding that will be trained while the pretrained model be frozen."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "1bccb792-58c9-4dad-a7e0-afac31ef87b5",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "class SoftEmbedding(nn.Module):\n",
    "    def __init__(self, \n",
    "                wte: nn.Embedding,  #original transformer word token embedding\n",
    "                n_tokens: int = 10, #number of tokens for each task\n",
    "                random_range: float = 0.5, #range to init embedding\n",
    "                initialize_from_vocab: bool = True):\n",
    "        super(SoftEmbedding, self).__init__()\n",
    "        self.wte = wte\n",
    "        self.n_tokens = n_tokens\n",
    "        self.learned_embedding = nn.parameter.Parameter(self.initialize_embedding(wte,\n",
    "                                                                               n_tokens, \n",
    "                                                                               random_range, \n",
    "                                                                               initialize_from_vocab))\n",
    "        #self.learned_embedding: (n_tokens, emb dim)\n",
    "        #self.wte.weight:        (vocab size, emb dim)\n",
    "        \n",
    "    def initialize_embedding(self, \n",
    "                             wte: nn.Embedding,\n",
    "                             n_tokens: int = 10, \n",
    "                             random_range: float = 0.5, \n",
    "                             initialize_from_vocab: bool = True):\n",
    "        if initialize_from_vocab:\n",
    "            return self.wte.weight[:n_tokens].clone().detach()\n",
    "        return torch.FloatTensor(n_tokens, wte.weight.size(1)).uniform_(-random_range, random_range)\n",
    "    \n",
    "    #define the forward process\n",
    "    def forward(self, tokens):\n",
    "        #first get the embedding of the input text, which is the n_tokens:\n",
    "        input_embedding = self.wte(tokens[:, self.n_tokens:])\n",
    "        #input_embedding: b, input_len, emb dim\n",
    "        \n",
    "        #repeat the learned embedding to all batch size\n",
    "        learned_embedding = self.learned_embedding.repeat(input_embedding.size(0), 1, 1)\n",
    "        #input_embedding: b, n_tokens, emb dim\n",
    "        \n",
    "        concat_embed = torch.cat([learned_embedding, input_embedding], 1)\n",
    "        #concat_embed: 1, input_len + n_tokens, emb dim\n",
    "        \n",
    "        return concat_embed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "46150056-d4be-4ae9-8137-32609e7c298a",
   "metadata": {},
   "outputs": [],
   "source": [
    "n_tokens = 20\n",
    "initialize_from_vocab = True\n",
    "\n",
    "s_wte = SoftEmbedding(model.get_input_embeddings(), \n",
    "                      n_tokens=n_tokens, \n",
    "                      initialize_from_vocab=initialize_from_vocab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "cac97607-48af-498f-b601-7e0c58c48629",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "SoftEmbedding(\n",
       "  (wte): Embedding(50257, 768)\n",
       ")"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "s_wte"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e019216a-cdbc-458f-ab73-c3f4b4fd72d4",
   "metadata": {},
   "source": [
    "Now we can replace the model input embeddings with ours."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "ebe59c69-3fea-4ac4-8b20-2750db80e4cf",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "model.set_input_embeddings(s_wte)  #note that set_input_embeddings take nn.Module object, NOT nn.Embedding..."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "60b08d75-6714-476d-8af3-7a6a0b852e70",
   "metadata": {
    "tags": []
   },
   "source": [
    "## 3. Testing\n",
    "\n",
    "Now let's see the forward pass in actionm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "c338ca6d-e8cd-4c68-9d18-2dbc6fdf0f49",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "inputs = tokenizer(\"Harry Potter is\", return_tensors=\"pt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "de189d2f-235c-40f2-a430-1f46daf1f3f1",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[18308, 14179,   318]])"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "inputs['input_ids']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "f4a91bb8-bf12-4a40-b9a0-f0b31e100a9a",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[1, 1, 1]])"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "inputs['attention_mask']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "29c709da-8d31-4d4a-9e5a-8efec522f4bb",
   "metadata": {},
   "source": [
    "Since we concat a soft embedding, we have to manually fill the `input_ids` variable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "117dd9a4-9bdf-432d-8bf2-2b777a5fcd89",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[    1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
       "             1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
       "         18308, 14179,   318]])"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# need to pad attention_mask and input_ids to be full seq_len + n_learned_tokens\n",
    "# it does not matter what input_ids you pad with, here we use 1\n",
    "inputs['input_ids'] = torch.cat([torch.full((1, n_tokens), 1), inputs['input_ids']], 1)\n",
    "inputs['input_ids']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "a6087cd3-0228-438a-a8c4-feb836248fac",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]])"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#pad attention mask as well\n",
    "inputs['attention_mask'] = torch.cat([torch.full((1,n_tokens), 1), inputs['attention_mask']], 1)\n",
    "inputs['attention_mask']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "1a9e91b4-07a9-4035-8406-5b5079f75016",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "outputs = model(**inputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "2b32931d-9de0-495b-802a-a98b05513b1f",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 23, 50257])"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "outputs.logits.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a892809c-b3f5-4189-adad-038f0af6c89f",
   "metadata": {},
   "source": [
    "## 4. Freezing\n",
    "\n",
    "Finally, we simply freeze all parameters except the embedding, then train as usual.  Yay!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "a423b8b5-c9b5-480d-a958-0604af0b47f9",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "transformer.wte.learned_embedding True\n",
      "transformer.wte.wte.weight True\n",
      "transformer.wpe.weight True\n",
      "transformer.h.0.ln_1.weight True\n",
      "transformer.h.0.ln_1.bias True\n"
     ]
    }
   ],
   "source": [
    "#check the list of named parameters\n",
    "i = 0\n",
    "for name, param in model.named_parameters():\n",
    "    print(name, param.requires_grad)\n",
    "    i = i + 1\n",
    "    if (i > 4): break  #lazy to print all...."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "97b95499-fb58-4ff3-8819-a1c60bfb17af",
   "metadata": {},
   "outputs": [],
   "source": [
    "parameters = list(model.parameters())\n",
    "for x in parameters[1:]:\n",
    "    x.requires_grad = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "d393ef1c-eff6-4c39-b7be-cbfe5ab607cf",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "transformer.wte.learned_embedding True\n",
      "transformer.wte.wte.weight False\n",
      "transformer.wpe.weight False\n",
      "transformer.h.0.ln_1.weight False\n",
      "transformer.h.0.ln_1.bias False\n",
      "transformer.h.0.attn.c_attn.weight False\n",
      "transformer.h.0.attn.c_attn.bias False\n",
      "transformer.h.0.attn.c_proj.weight False\n",
      "transformer.h.0.attn.c_proj.bias False\n",
      "transformer.h.0.ln_2.weight False\n",
      "transformer.h.0.ln_2.bias False\n",
      "transformer.h.0.mlp.c_fc.weight False\n",
      "transformer.h.0.mlp.c_fc.bias False\n",
      "transformer.h.0.mlp.c_proj.weight False\n",
      "transformer.h.0.mlp.c_proj.bias False\n",
      "transformer.h.1.ln_1.weight False\n",
      "transformer.h.1.ln_1.bias False\n",
      "transformer.h.1.attn.c_attn.weight False\n",
      "transformer.h.1.attn.c_attn.bias False\n",
      "transformer.h.1.attn.c_proj.weight False\n",
      "transformer.h.1.attn.c_proj.bias False\n",
      "transformer.h.1.ln_2.weight False\n",
      "transformer.h.1.ln_2.bias False\n",
      "transformer.h.1.mlp.c_fc.weight False\n",
      "transformer.h.1.mlp.c_fc.bias False\n",
      "transformer.h.1.mlp.c_proj.weight False\n",
      "transformer.h.1.mlp.c_proj.bias False\n",
      "transformer.h.2.ln_1.weight False\n",
      "transformer.h.2.ln_1.bias False\n",
      "transformer.h.2.attn.c_attn.weight False\n",
      "transformer.h.2.attn.c_attn.bias False\n",
      "transformer.h.2.attn.c_proj.weight False\n",
      "transformer.h.2.attn.c_proj.bias False\n",
      "transformer.h.2.ln_2.weight False\n",
      "transformer.h.2.ln_2.bias False\n",
      "transformer.h.2.mlp.c_fc.weight False\n",
      "transformer.h.2.mlp.c_fc.bias False\n",
      "transformer.h.2.mlp.c_proj.weight False\n",
      "transformer.h.2.mlp.c_proj.bias False\n",
      "transformer.h.3.ln_1.weight False\n",
      "transformer.h.3.ln_1.bias False\n",
      "transformer.h.3.attn.c_attn.weight False\n",
      "transformer.h.3.attn.c_attn.bias False\n",
      "transformer.h.3.attn.c_proj.weight False\n",
      "transformer.h.3.attn.c_proj.bias False\n",
      "transformer.h.3.ln_2.weight False\n",
      "transformer.h.3.ln_2.bias False\n",
      "transformer.h.3.mlp.c_fc.weight False\n",
      "transformer.h.3.mlp.c_fc.bias False\n",
      "transformer.h.3.mlp.c_proj.weight False\n",
      "transformer.h.3.mlp.c_proj.bias False\n",
      "transformer.h.4.ln_1.weight False\n",
      "transformer.h.4.ln_1.bias False\n",
      "transformer.h.4.attn.c_attn.weight False\n",
      "transformer.h.4.attn.c_attn.bias False\n",
      "transformer.h.4.attn.c_proj.weight False\n",
      "transformer.h.4.attn.c_proj.bias False\n",
      "transformer.h.4.ln_2.weight False\n",
      "transformer.h.4.ln_2.bias False\n",
      "transformer.h.4.mlp.c_fc.weight False\n",
      "transformer.h.4.mlp.c_fc.bias False\n",
      "transformer.h.4.mlp.c_proj.weight False\n",
      "transformer.h.4.mlp.c_proj.bias False\n",
      "transformer.h.5.ln_1.weight False\n",
      "transformer.h.5.ln_1.bias False\n",
      "transformer.h.5.attn.c_attn.weight False\n",
      "transformer.h.5.attn.c_attn.bias False\n",
      "transformer.h.5.attn.c_proj.weight False\n",
      "transformer.h.5.attn.c_proj.bias False\n",
      "transformer.h.5.ln_2.weight False\n",
      "transformer.h.5.ln_2.bias False\n",
      "transformer.h.5.mlp.c_fc.weight False\n",
      "transformer.h.5.mlp.c_fc.bias False\n",
      "transformer.h.5.mlp.c_proj.weight False\n",
      "transformer.h.5.mlp.c_proj.bias False\n",
      "transformer.h.6.ln_1.weight False\n",
      "transformer.h.6.ln_1.bias False\n",
      "transformer.h.6.attn.c_attn.weight False\n",
      "transformer.h.6.attn.c_attn.bias False\n",
      "transformer.h.6.attn.c_proj.weight False\n",
      "transformer.h.6.attn.c_proj.bias False\n",
      "transformer.h.6.ln_2.weight False\n",
      "transformer.h.6.ln_2.bias False\n",
      "transformer.h.6.mlp.c_fc.weight False\n",
      "transformer.h.6.mlp.c_fc.bias False\n",
      "transformer.h.6.mlp.c_proj.weight False\n",
      "transformer.h.6.mlp.c_proj.bias False\n",
      "transformer.h.7.ln_1.weight False\n",
      "transformer.h.7.ln_1.bias False\n",
      "transformer.h.7.attn.c_attn.weight False\n",
      "transformer.h.7.attn.c_attn.bias False\n",
      "transformer.h.7.attn.c_proj.weight False\n",
      "transformer.h.7.attn.c_proj.bias False\n",
      "transformer.h.7.ln_2.weight False\n",
      "transformer.h.7.ln_2.bias False\n",
      "transformer.h.7.mlp.c_fc.weight False\n",
      "transformer.h.7.mlp.c_fc.bias False\n",
      "transformer.h.7.mlp.c_proj.weight False\n",
      "transformer.h.7.mlp.c_proj.bias False\n",
      "transformer.h.8.ln_1.weight False\n",
      "transformer.h.8.ln_1.bias False\n",
      "transformer.h.8.attn.c_attn.weight False\n",
      "transformer.h.8.attn.c_attn.bias False\n",
      "transformer.h.8.attn.c_proj.weight False\n",
      "transformer.h.8.attn.c_proj.bias False\n",
      "transformer.h.8.ln_2.weight False\n",
      "transformer.h.8.ln_2.bias False\n",
      "transformer.h.8.mlp.c_fc.weight False\n",
      "transformer.h.8.mlp.c_fc.bias False\n",
      "transformer.h.8.mlp.c_proj.weight False\n",
      "transformer.h.8.mlp.c_proj.bias False\n",
      "transformer.h.9.ln_1.weight False\n",
      "transformer.h.9.ln_1.bias False\n",
      "transformer.h.9.attn.c_attn.weight False\n",
      "transformer.h.9.attn.c_attn.bias False\n",
      "transformer.h.9.attn.c_proj.weight False\n",
      "transformer.h.9.attn.c_proj.bias False\n",
      "transformer.h.9.ln_2.weight False\n",
      "transformer.h.9.ln_2.bias False\n",
      "transformer.h.9.mlp.c_fc.weight False\n",
      "transformer.h.9.mlp.c_fc.bias False\n",
      "transformer.h.9.mlp.c_proj.weight False\n",
      "transformer.h.9.mlp.c_proj.bias False\n",
      "transformer.h.10.ln_1.weight False\n",
      "transformer.h.10.ln_1.bias False\n",
      "transformer.h.10.attn.c_attn.weight False\n",
      "transformer.h.10.attn.c_attn.bias False\n",
      "transformer.h.10.attn.c_proj.weight False\n",
      "transformer.h.10.attn.c_proj.bias False\n",
      "transformer.h.10.ln_2.weight False\n",
      "transformer.h.10.ln_2.bias False\n",
      "transformer.h.10.mlp.c_fc.weight False\n",
      "transformer.h.10.mlp.c_fc.bias False\n",
      "transformer.h.10.mlp.c_proj.weight False\n",
      "transformer.h.10.mlp.c_proj.bias False\n",
      "transformer.h.11.ln_1.weight False\n",
      "transformer.h.11.ln_1.bias False\n",
      "transformer.h.11.attn.c_attn.weight False\n",
      "transformer.h.11.attn.c_attn.bias False\n",
      "transformer.h.11.attn.c_proj.weight False\n",
      "transformer.h.11.attn.c_proj.bias False\n",
      "transformer.h.11.ln_2.weight False\n",
      "transformer.h.11.ln_2.bias False\n",
      "transformer.h.11.mlp.c_fc.weight False\n",
      "transformer.h.11.mlp.c_fc.bias False\n",
      "transformer.h.11.mlp.c_proj.weight False\n",
      "transformer.h.11.mlp.c_proj.bias False\n",
      "transformer.ln_f.weight False\n",
      "transformer.ln_f.bias False\n"
     ]
    }
   ],
   "source": [
    "#make sure things are frozen accordingly....\n",
    "for name, param in model.named_parameters():\n",
    "     print(name, param.requires_grad)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
