{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# [CoCa: Contrastive Captioners are Image-Text Foundation Models](https://arxiv.org/pdf/2205.01917.pdf)\n",
    "\n",
    "<img src=\"./figures/coca.png\" title=\"CoCa\" />"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import einsum, nn\n",
    "import torch.nn.functional as F\n",
    "from torch.autograd import Function\n",
    "import torch.distributed as dist\n",
    "\n",
    "from einops import rearrange, repeat\n",
    "\n",
    "# helper functions\n",
    "\n",
    "def exists(val):\n",
    "    return val is not None\n",
    "\n",
    "def default(val, d):\n",
    "    return val if exists(val) else d"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# distributed\n",
    "\n",
    "def pad_dim_to(t, length, dim = 0):\n",
    "    pad_length = length - t.shape[dim]\n",
    "    zero_pairs = (-dim - 1) if dim < 0 else (t.ndim - dim - 1)\n",
    "    return F.pad(t, (*((0, 0) * zero_pairs), 0, pad_length))\n",
    "\n",
    "def all_gather_variable_batch(t):\n",
    "    device, rank, world_size = t.device, dist.get_rank(), dist.get_world_size()\n",
    "\n",
    "    size = torch.tensor(t.shape[0], device = device, dtype = torch.long)\n",
    "    sizes = [torch.empty_like(size, device = device, dtype = torch.long) for i in range(world_size)]\n",
    "    dist.all_gather(sizes, size)\n",
    "\n",
    "    sizes = torch.stack(sizes)\n",
    "    max_size = sizes.amax().item()\n",
    "\n",
    "    padded_t = pad_dim_to(t, max_size, dim = 0)\n",
    "    gathered_tensors = [torch.empty_like(padded_t, device = device, dtype = padded_t.dtype) for i in range(world_size)]\n",
    "    dist.all_gather(gathered_tensors, padded_t)\n",
    "\n",
    "    gathered_tensor = torch.cat(gathered_tensors)\n",
    "    seq = torch.arange(max_size, device = device)\n",
    "\n",
    "    mask = rearrange(seq, 'j -> 1 j') < rearrange(sizes, 'i -> i 1')\n",
    "    mask = rearrange(mask, 'i j -> (i j)')\n",
    "\n",
    "    gathered_tensor = gathered_tensor[mask]\n",
    "    sizes = sizes.tolist()\n",
    "\n",
    "    return gathered_tensor, sizes\n",
    "\n",
    "class AllGather(Function):\n",
    "    @staticmethod\n",
    "    def forward(ctx, x):\n",
    "        assert dist.is_initialized() and dist.get_world_size() > 1\n",
    "        x, batch_sizes = all_gather_variable_batch(x)\n",
    "        ctx.batch_sizes = batch_sizes\n",
    "        return x\n",
    "\n",
    "    @staticmethod\n",
    "    def backward(ctx, grads):\n",
    "        batch_sizes, rank = ctx.batch_sizes, dist.get_rank()\n",
    "        grads_by_rank = grads.split(batch_sizes, dim = 0)\n",
    "        return grads_by_rank[rank]\n",
    "\n",
    "all_gather = AllGather.apply"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# normalization\n",
    "# they use layernorm without bias, something that pytorch does not offer\n",
    "class LayerNorm(nn.Module):\n",
    "    def __init__(self, dim):\n",
    "        super().__init__()\n",
    "        self.gamma = nn.Parameter(torch.ones(dim))\n",
    "        self.register_buffer(\"beta\", torch.zeros(dim))\n",
    "\n",
    "    def forward(self, x):\n",
    "        return F.layer_norm(x, x.shape[-1:], self.gamma, self.beta)\n",
    "\n",
    "# residual\n",
    "class Residual(nn.Module):\n",
    "    def __init__(self, fn):\n",
    "        super().__init__()\n",
    "        self.fn = fn\n",
    "\n",
    "    def forward(self, x, *args, **kwargs):\n",
    "        return self.fn(x, *args, **kwargs) + x\n",
    "\n",
    "# to latents\n",
    "class EmbedToLatents(nn.Module):\n",
    "    def __init__(self, dim, dim_latents):\n",
    "        super().__init__()\n",
    "        self.to_latents = nn.Linear(dim, dim_latents, bias=False)\n",
    "\n",
    "    def forward(self, x):\n",
    "        latents = self.to_latents(x)\n",
    "        return F.normalize(latents, dim=-1)\n",
    "\n",
    "# rotary positional embedding\n",
    "# https://arxiv.org/abs/2104.09864\n",
    "class RotaryEmbedding(nn.Module):\n",
    "    def __init__(self, dim):\n",
    "        super().__init__()\n",
    "        inv_freq = 1.0 / (10000 ** (torch.arange(0, dim, 2).float() / dim))\n",
    "        self.register_buffer(\"inv_freq\", inv_freq)\n",
    "\n",
    "    def forward(self, max_seq_len, *, device):\n",
    "        seq = torch.arange(max_seq_len, device=device, dtype=self.inv_freq.dtype)\n",
    "        freqs = einsum(\"i , j -> i j\", seq, self.inv_freq)\n",
    "        return torch.cat((freqs, freqs), dim=-1)\n",
    "\n",
    "\n",
    "def rotate_half(x):\n",
    "    x = rearrange(x, \"... (j d) -> ... j d\", j=2)\n",
    "    x1, x2 = x.unbind(dim=-2)\n",
    "    return torch.cat((-x2, x1), dim=-1)\n",
    "\n",
    "\n",
    "def apply_rotary_pos_emb(pos, t):\n",
    "    return (t * pos.cos()) + (rotate_half(t) * pos.sin())\n",
    "\n",
    "\n",
    "# classic Noam Shazeer paper, except here they use SwiGLU instead of the more popular GEGLU for gating the feedforward\n",
    "# https://arxiv.org/abs/2002.05202\n",
    "class SwiGLU(nn.Module):\n",
    "    def forward(self, x):\n",
    "        x, gate = x.chunk(2, dim=-1)\n",
    "        return F.silu(gate) * x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# parallel attention and feedforward with residual\n",
    "# discovered by Wang et al + EleutherAI from GPT-J fame\n",
    "class ParallelTransformerBlock(nn.Module):\n",
    "    def __init__(self, dim, dim_head=64, heads=8, ff_mult=4):\n",
    "        super().__init__()\n",
    "        self.norm = LayerNorm(dim)\n",
    "\n",
    "        attn_inner_dim = dim_head * heads\n",
    "        ff_inner_dim = dim * ff_mult\n",
    "        self.fused_dims = (attn_inner_dim, dim_head, dim_head, (ff_inner_dim * 2))\n",
    "\n",
    "        self.heads = heads\n",
    "        self.scale = dim_head**-0.5\n",
    "        self.rotary_emb = RotaryEmbedding(dim_head)\n",
    "\n",
    "        self.fused_attn_ff_proj = nn.Linear(dim, sum(self.fused_dims), bias=False)\n",
    "        self.attn_out = nn.Linear(attn_inner_dim, dim, bias=False)\n",
    "\n",
    "        self.ff_out = nn.Sequential(\n",
    "            SwiGLU(),\n",
    "            nn.Linear(ff_inner_dim, dim, bias=False)\n",
    "        )\n",
    "\n",
    "        # for caching causal mask and rotary embeddings\n",
    "\n",
    "        self.mask = None\n",
    "        self.pos_emb = None\n",
    "\n",
    "    def get_mask(self, n, device):\n",
    "        if self.mask is not None and self.mask.shape[-1] >= n:\n",
    "            return self.mask[:n, :n].to(device)\n",
    "\n",
    "        mask = torch.ones((n, n), device=device, dtype=torch.bool).triu(1)\n",
    "        self.mask = mask\n",
    "        return mask\n",
    "\n",
    "    def get_rotary_embedding(self, n, device):\n",
    "        if self.pos_emb is not None and self.pos_emb.shape[-2] >= n:\n",
    "            return self.pos_emb[:n].to(device)\n",
    "\n",
    "        pos_emb = self.rotary_emb(n, device=device)\n",
    "        self.pos_emb = pos_emb\n",
    "        return pos_emb\n",
    "\n",
    "    def forward(self, x, attn_mask=None):\n",
    "        \"\"\"\n",
    "        einstein notation\n",
    "        b - batch\n",
    "        h - heads\n",
    "        n, i, j - sequence length (base sequence length, source, target)\n",
    "        d - feature dimension\n",
    "        \"\"\"\n",
    "\n",
    "        n, device, h = x.shape[1], x.device, self.heads\n",
    "\n",
    "        # pre layernorm\n",
    "\n",
    "        x = self.norm(x)\n",
    "\n",
    "        # attention queries, keys, values, and feedforward inner\n",
    "\n",
    "        q, k, v, ff = self.fused_attn_ff_proj(x).split(self.fused_dims, dim=-1)\n",
    "\n",
    "        # split heads\n",
    "        # they use multi-query single-key-value attention, yet another Noam Shazeer paper\n",
    "        # they found no performance loss past a certain scale, and more efficient decoding obviously\n",
    "        # https://arxiv.org/abs/1911.02150\n",
    "\n",
    "        q = rearrange(q, \"b n (h d) -> b h n d\", h=h)\n",
    "\n",
    "        # rotary embeddings\n",
    "\n",
    "        positions = self.get_rotary_embedding(n, device)\n",
    "        q, k = map(lambda t: apply_rotary_pos_emb(positions, t), (q, k))\n",
    "\n",
    "        # scale\n",
    "\n",
    "        q = q * self.scale\n",
    "\n",
    "        # similarity\n",
    "\n",
    "        sim = einsum(\"b h i d, b j d -> b h i j\", q, k)\n",
    "\n",
    "        # causal mask\n",
    "\n",
    "        causal_mask = self.get_mask(n, device)\n",
    "        sim = sim.masked_fill(causal_mask, -torch.finfo(sim.dtype).max)\n",
    "\n",
    "        # extra attention mask - for masking out attention from text CLS token to padding\n",
    "\n",
    "        if exists(attn_mask):\n",
    "            attn_mask = rearrange(attn_mask, 'b i j -> b 1 i j')\n",
    "            sim = sim.masked_fill(~attn_mask, -torch.finfo(sim.dtype).max)\n",
    "\n",
    "        # attention\n",
    "\n",
    "        sim = sim - sim.amax(dim=-1, keepdim=True).detach()\n",
    "        attn = sim.softmax(dim=-1)\n",
    "\n",
    "        # aggregate values\n",
    "\n",
    "        out = einsum(\"b h i j, b j d -> b h i d\", attn, v)\n",
    "\n",
    "        # merge heads\n",
    "\n",
    "        out = rearrange(out, \"b h n d -> b n (h d)\")\n",
    "        return self.attn_out(out) + self.ff_out(ff)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# cross attention - using multi-query + one-headed key / values as in PaLM w/ optional parallel feedforward\n",
    "class CrossAttention(nn.Module):\n",
    "    def __init__(\n",
    "        self,\n",
    "        dim,\n",
    "        *,\n",
    "        context_dim=None,\n",
    "        dim_head=64,\n",
    "        heads=8,\n",
    "        parallel_ff=False,\n",
    "        ff_mult=4,\n",
    "        norm_context=False\n",
    "    ):\n",
    "        super().__init__()\n",
    "        self.heads = heads\n",
    "        self.scale = dim_head ** -0.5\n",
    "        inner_dim = heads * dim_head\n",
    "        context_dim = default(context_dim, dim)\n",
    "\n",
    "        self.norm = LayerNorm(dim)\n",
    "        self.context_norm = LayerNorm(context_dim) if norm_context else nn.Identity()\n",
    "\n",
    "        self.to_q = nn.Linear(dim, inner_dim, bias=False)\n",
    "        self.to_kv = nn.Linear(context_dim, dim_head * 2, bias=False)\n",
    "        self.to_out = nn.Linear(inner_dim, dim, bias=False)\n",
    "\n",
    "        # whether to have parallel feedforward\n",
    "\n",
    "        ff_inner_dim = ff_mult * dim\n",
    "\n",
    "        self.ff = nn.Sequential(\n",
    "            nn.Linear(dim, ff_inner_dim * 2, bias=False),\n",
    "            SwiGLU(),\n",
    "            nn.Linear(ff_inner_dim, dim, bias=False)\n",
    "        ) if parallel_ff else None\n",
    "\n",
    "    def forward(self, x, context):\n",
    "        \"\"\"\n",
    "        einstein notation\n",
    "        b - batch\n",
    "        h - heads\n",
    "        n, i, j - sequence length (base sequence length, source, target)\n",
    "        d - feature dimension\n",
    "        \"\"\"\n",
    "\n",
    "        # pre-layernorm, for queries and context\n",
    "\n",
    "        x = self.norm(x)\n",
    "        context = self.context_norm(context)\n",
    "\n",
    "        # get queries\n",
    "\n",
    "        q = self.to_q(x)\n",
    "        q = rearrange(q, 'b n (h d) -> b h n d', h = self.heads)\n",
    "\n",
    "        # scale\n",
    "\n",
    "        q = q * self.scale\n",
    "\n",
    "        # get key / values\n",
    "\n",
    "        k, v = self.to_kv(context).chunk(2, dim=-1)\n",
    "\n",
    "        # query / key similarity\n",
    "\n",
    "        sim = einsum('b h i d, b j d -> b h i j', q, k)\n",
    "\n",
    "        # attention\n",
    "\n",
    "        sim = sim - sim.amax(dim=-1, keepdim=True)\n",
    "        attn = sim.softmax(dim=-1)\n",
    "\n",
    "        # aggregate\n",
    "\n",
    "        out = einsum('b h i j, b j d -> b h i d', attn, v)\n",
    "\n",
    "        # merge and combine heads\n",
    "\n",
    "        out = rearrange(out, 'b h n d -> b n (h d)')\n",
    "        out = self.to_out(out)\n",
    "\n",
    "        # add parallel feedforward (for multimodal layers)\n",
    "\n",
    "        if exists(self.ff):\n",
    "            out = out + self.ff(x)\n",
    "\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# transformer\n",
    "class CoCa(nn.Module):\n",
    "    def __init__(\n",
    "        self,\n",
    "        *,\n",
    "        dim,\n",
    "        num_tokens,\n",
    "        unimodal_depth,\n",
    "        multimodal_depth,\n",
    "        dim_latents = None,\n",
    "        image_dim = None,\n",
    "        num_img_queries=256,\n",
    "        dim_head=64,\n",
    "        heads=8,\n",
    "        ff_mult=4,\n",
    "        img_encoder=None,\n",
    "        caption_loss_weight=1.,\n",
    "        contrastive_loss_weight=1.,\n",
    "        pad_id=0\n",
    "    ):\n",
    "        super().__init__()\n",
    "        self.dim = dim\n",
    "\n",
    "        self.pad_id = pad_id\n",
    "        self.caption_loss_weight = caption_loss_weight\n",
    "        self.contrastive_loss_weight = contrastive_loss_weight\n",
    "\n",
    "        # token embeddings\n",
    "        self.token_emb = nn.Embedding(num_tokens, dim)\n",
    "        self.text_cls_token = nn.Parameter(torch.randn(dim))\n",
    "\n",
    "        # image encoder\n",
    "        self.img_encoder = img_encoder\n",
    "\n",
    "        # attention pooling for image tokens\n",
    "        self.img_queries = nn.Parameter(torch.randn(num_img_queries + 1, dim)) # num image queries for multimodal, but 1 extra CLS for contrastive learning\n",
    "        self.img_attn_pool = CrossAttention(dim=dim, context_dim=image_dim, dim_head=dim_head, heads=heads, norm_context=True)\n",
    "\n",
    "        self.img_attn_pool_norm = LayerNorm(dim)\n",
    "        self.text_cls_norm = LayerNorm(dim)\n",
    "\n",
    "        # to latents\n",
    "        dim_latents = default(dim_latents, dim)\n",
    "        self.img_to_latents = EmbedToLatents(dim, dim_latents)\n",
    "        self.text_to_latents = EmbedToLatents(dim, dim_latents)\n",
    "\n",
    "        # contrastive learning temperature\n",
    "        self.temperature = nn.Parameter(torch.Tensor([1.]))\n",
    "\n",
    "        # unimodal layers\n",
    "        self.unimodal_layers = nn.ModuleList([])\n",
    "        for ind in range(unimodal_depth):\n",
    "            self.unimodal_layers.append(\n",
    "                Residual(ParallelTransformerBlock(dim=dim, dim_head=dim_head, heads=heads, ff_mult=ff_mult)),\n",
    "            )\n",
    "\n",
    "        # multimodal layers\n",
    "        self.multimodal_layers = nn.ModuleList([])\n",
    "        for ind in range(multimodal_depth):\n",
    "            self.multimodal_layers.append(nn.ModuleList([\n",
    "                Residual(ParallelTransformerBlock(dim=dim, dim_head=dim_head, heads=heads, ff_mult=ff_mult)),\n",
    "                Residual(CrossAttention(dim=dim, dim_head=dim_head, heads=heads, parallel_ff=True, ff_mult=ff_mult))\n",
    "            ]))\n",
    "\n",
    "        # to logits\n",
    "        self.to_logits = nn.Sequential(\n",
    "            LayerNorm(dim),\n",
    "            nn.Linear(dim, num_tokens, bias=False)\n",
    "        )\n",
    "\n",
    "        # they used embedding weight tied projection out to logits, not common, but works\n",
    "        self.to_logits[-1].weight = self.token_emb.weight\n",
    "        nn.init.normal_(self.token_emb.weight, std=0.02)\n",
    "\n",
    "        # whether in data parallel setting\n",
    "        self.is_distributed = dist.is_initialized() and dist.get_world_size() > 1\n",
    "\n",
    "    def embed_text(self, text):\n",
    "        batch, device = text.shape[0], text.device\n",
    "\n",
    "        seq = text.shape[1]\n",
    "\n",
    "        text_tokens = self.token_emb(text)\n",
    "\n",
    "        # append text cls tokens\n",
    "        text_cls_tokens = repeat(self.text_cls_token, 'd -> b 1 d', b=batch)\n",
    "        text_tokens = torch.cat((text_tokens, text_cls_tokens), dim=-2)\n",
    "\n",
    "        # create specific mask for text cls token at the end\n",
    "        # to prevent it from attending to padding\n",
    "        cls_mask = rearrange(text!=self.pad_id, 'b j -> b 1 j')\n",
    "        attn_mask = F.pad(cls_mask, (0, 1, seq, 0), value=True)\n",
    "\n",
    "        # go through unimodal layers\n",
    "        for attn_ff in self.unimodal_layers:\n",
    "            text_tokens = attn_ff(text_tokens, attn_mask=attn_mask)\n",
    "\n",
    "        # get text cls token\n",
    "        text_tokens, text_cls_tokens = text_tokens[:, :-1], text_tokens[:, -1]\n",
    "        text_embeds = self.text_cls_norm(text_cls_tokens)\n",
    "        return text_embeds, text_tokens\n",
    "\n",
    "    def embed_image(self, images=None, image_tokens=None):\n",
    "        # encode images into embeddings\n",
    "        # with the img_encoder passed in at init\n",
    "        # it can also accept precomputed image tokens\n",
    "\n",
    "        assert not (exists(images) and exists(image_tokens))\n",
    "\n",
    "        if exists(images):\n",
    "            assert exists(self.img_encoder), 'img_encoder must be passed in for automatic image encoding'\n",
    "            image_tokens = self.img_encoder(images)\n",
    "\n",
    "        # attention pool image tokens\n",
    "\n",
    "        img_queries = repeat(self.img_queries, 'n d -> b n d', b=image_tokens.shape[0])\n",
    "        img_queries = self.img_attn_pool(img_queries, image_tokens)\n",
    "        img_queries = self.img_attn_pool_norm(img_queries)\n",
    "\n",
    "        return img_queries[:, 0], img_queries[:, 1:]\n",
    "\n",
    "    def forward(\n",
    "        self,\n",
    "        text,\n",
    "        images=None,\n",
    "        image_tokens=None,\n",
    "        labels=None,\n",
    "        return_loss=False,\n",
    "        return_embeddings=False\n",
    "    ):\n",
    "        batch, device = text.shape[0], text.device\n",
    "\n",
    "        if return_loss and not exists(labels):\n",
    "            text, labels = text[:, :-1], text[:, 1:]\n",
    "\n",
    "        text_embeds, text_tokens = self.embed_text(text)\n",
    "\n",
    "        image_embeds, image_tokens = self.embed_image(images=images, image_tokens=image_tokens)\n",
    "\n",
    "        # return embeddings if that is what the researcher wants\n",
    "        if return_embeddings:\n",
    "            return text_embeds, image_embeds\n",
    "\n",
    "        # go through multimodal layers\n",
    "        for attn_ff, cross_attn in self.multimodal_layers:\n",
    "            text_tokens = attn_ff(text_tokens)\n",
    "            text_tokens = cross_attn(text_tokens, image_tokens)\n",
    "\n",
    "        logits = self.to_logits(text_tokens)\n",
    "\n",
    "        if not return_loss:\n",
    "            return logits\n",
    "\n",
    "        # shorthand\n",
    "        ce = F.cross_entropy\n",
    "\n",
    "        # calculate caption loss (cross entropy loss)\n",
    "        logits = rearrange(logits, 'b n c -> b c n')\n",
    "        caption_loss = ce(logits, labels, ignore_index=self.pad_id)\n",
    "        caption_loss = caption_loss * self.caption_loss_weight\n",
    "\n",
    "        # embedding to latents\n",
    "        text_latents = self.text_to_latents(text_embeds)\n",
    "        image_latents = self.img_to_latents(image_embeds)\n",
    "\n",
    "        # maybe distributed all gather\n",
    "        if self.is_distributed:\n",
    "            latents = torch.stack((text_latents, image_latents), dim = 1)\n",
    "            latents = all_gather(latents)\n",
    "            text_latents, image_latents = latents.unbind(dim = 1)\n",
    "\n",
    "        # calculate contrastive loss\n",
    "        sim = einsum('i d, j d -> i j', text_latents, image_latents)\n",
    "        sim = sim * self.temperature.exp()\n",
    "        contrastive_labels = torch.arange(batch, device=device)\n",
    "\n",
    "        contrastive_loss = (ce(sim, contrastive_labels) + ce(sim.t(), contrastive_labels)) * 0.5\n",
    "        contrastive_loss = contrastive_loss * self.contrastive_loss_weight\n",
    "\n",
    "        return caption_loss + contrastive_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device cpu\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "device = torch.device(\"cuda:1\" if torch.cuda.is_available() else \"cpu\")\n",
    "print('Using device', device)\n",
    "\n",
    "# import vision transformer\n",
    "# from vit_pytorch.simple_vit_with_patch_dropout import SimpleViT\n",
    "from vit_pytorch import ViT\n",
    "from vit_pytorch.extractor import Extractor\n",
    "\n",
    "vit = ViT(\n",
    "    image_size = 256,\n",
    "    patch_size = 32,\n",
    "    num_classes = 1000,\n",
    "    dim = 1024,\n",
    "    depth = 6,\n",
    "    heads = 16,\n",
    "    mlp_dim = 2048,\n",
    "    # patch_dropout = 0.5  # https://arxiv.org/abs/2212.00794\n",
    ")\n",
    "\n",
    "vit = Extractor(vit, return_embeddings_only = True, detach = False)\n",
    "# vit\n",
    "# extractor will enable it so the vision transformer returns its embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "CoCa(\n",
       "  (token_emb): Embedding(20000, 512)\n",
       "  (img_encoder): Extractor(\n",
       "    (vit): ViT(\n",
       "      (to_patch_embedding): Sequential(\n",
       "        (0): Rearrange('b c (h p1) (w p2) -> b (h w) (p1 p2 c)', p1=32, p2=32)\n",
       "        (1): LayerNorm((3072,), eps=1e-05, elementwise_affine=True)\n",
       "        (2): Linear(in_features=3072, out_features=1024, bias=True)\n",
       "        (3): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "      )\n",
       "      (dropout): Dropout(p=0.0, inplace=False)\n",
       "      (transformer): Transformer(\n",
       "        (norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "        (layers): ModuleList(\n",
       "          (0-5): 6 x ModuleList(\n",
       "            (0): Attention(\n",
       "              (norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "              (attend): Softmax(dim=-1)\n",
       "              (dropout): Dropout(p=0.0, inplace=False)\n",
       "              (to_qkv): Linear(in_features=1024, out_features=3072, bias=False)\n",
       "              (to_out): Sequential(\n",
       "                (0): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "                (1): Dropout(p=0.0, inplace=False)\n",
       "              )\n",
       "            )\n",
       "            (1): FeedForward(\n",
       "              (net): Sequential(\n",
       "                (0): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "                (1): Linear(in_features=1024, out_features=2048, bias=True)\n",
       "                (2): GELU(approximate='none')\n",
       "                (3): Dropout(p=0.0, inplace=False)\n",
       "                (4): Linear(in_features=2048, out_features=1024, bias=True)\n",
       "                (5): Dropout(p=0.0, inplace=False)\n",
       "              )\n",
       "            )\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "      (to_latent): Identity()\n",
       "      (mlp_head): Linear(in_features=1024, out_features=1000, bias=True)\n",
       "    )\n",
       "  )\n",
       "  (img_attn_pool): CrossAttention(\n",
       "    (norm): LayerNorm()\n",
       "    (context_norm): LayerNorm()\n",
       "    (to_q): Linear(in_features=512, out_features=512, bias=False)\n",
       "    (to_kv): Linear(in_features=1024, out_features=128, bias=False)\n",
       "    (to_out): Linear(in_features=512, out_features=512, bias=False)\n",
       "  )\n",
       "  (img_attn_pool_norm): LayerNorm()\n",
       "  (text_cls_norm): LayerNorm()\n",
       "  (img_to_latents): EmbedToLatents(\n",
       "    (to_latents): Linear(in_features=512, out_features=512, bias=False)\n",
       "  )\n",
       "  (text_to_latents): EmbedToLatents(\n",
       "    (to_latents): Linear(in_features=512, out_features=512, bias=False)\n",
       "  )\n",
       "  (unimodal_layers): ModuleList(\n",
       "    (0-5): 6 x Residual(\n",
       "      (fn): ParallelTransformerBlock(\n",
       "        (norm): LayerNorm()\n",
       "        (rotary_emb): RotaryEmbedding()\n",
       "        (fused_attn_ff_proj): Linear(in_features=512, out_features=4736, bias=False)\n",
       "        (attn_out): Linear(in_features=512, out_features=512, bias=False)\n",
       "        (ff_out): Sequential(\n",
       "          (0): SwiGLU()\n",
       "          (1): Linear(in_features=2048, out_features=512, bias=False)\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "  )\n",
       "  (multimodal_layers): ModuleList(\n",
       "    (0-5): 6 x ModuleList(\n",
       "      (0): Residual(\n",
       "        (fn): ParallelTransformerBlock(\n",
       "          (norm): LayerNorm()\n",
       "          (rotary_emb): RotaryEmbedding()\n",
       "          (fused_attn_ff_proj): Linear(in_features=512, out_features=4736, bias=False)\n",
       "          (attn_out): Linear(in_features=512, out_features=512, bias=False)\n",
       "          (ff_out): Sequential(\n",
       "            (0): SwiGLU()\n",
       "            (1): Linear(in_features=2048, out_features=512, bias=False)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "      (1): Residual(\n",
       "        (fn): CrossAttention(\n",
       "          (norm): LayerNorm()\n",
       "          (context_norm): Identity()\n",
       "          (to_q): Linear(in_features=512, out_features=512, bias=False)\n",
       "          (to_kv): Linear(in_features=512, out_features=128, bias=False)\n",
       "          (to_out): Linear(in_features=512, out_features=512, bias=False)\n",
       "          (ff): Sequential(\n",
       "            (0): Linear(in_features=512, out_features=4096, bias=False)\n",
       "            (1): SwiGLU()\n",
       "            (2): Linear(in_features=2048, out_features=512, bias=False)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "  )\n",
       "  (to_logits): Sequential(\n",
       "    (0): LayerNorm()\n",
       "    (1): Linear(in_features=512, out_features=20000, bias=False)\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# import CoCa and instantiate it\n",
    "coca = CoCa(\n",
    "    dim = 512,                     # model dimension\n",
    "    img_encoder = vit,             # vision transformer - image encoder, returning image embeddings as (batch, seq, dim)\n",
    "    image_dim = 1024,              # image embedding dimension, if not the same as model dimensions\n",
    "    num_tokens = 20000,            # number of text tokens\n",
    "    unimodal_depth = 6,            # depth of the unimodal transformer\n",
    "    multimodal_depth = 6,          # depth of the multimodal transformer\n",
    "    dim_head = 64,                 # dimension per attention head\n",
    "    heads = 8,                     # number of attention heads\n",
    "    caption_loss_weight = 1.,      # weight on the autoregressive caption loss\n",
    "    contrastive_loss_weight = 1.,  # weight on the contrastive loss between image and text CLS embeddings\n",
    ").to(device)\n",
    "\n",
    "coca"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# mock text and images\n",
    "\n",
    "text = torch.randint(0, 20000, (4, 512)).to(device)\n",
    "images = torch.randn(4, 3, 256, 256).to(device)\n",
    "\n",
    "# train by giving CoCa your text and images with `return_loss = True`\n",
    "\n",
    "loss = coca(\n",
    "    text = text,\n",
    "    images = images,\n",
    "    return_loss = True  # set this to True to get the full caption + contrastive loss\n",
    ")\n",
    "\n",
    "loss.backward()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([4, 512, 20000])"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# do the above for as much text and images...\n",
    "# then you can get the caption logits as so\n",
    "logits = coca(\n",
    "    text = text,\n",
    "    images = images\n",
    ") # (4, 512, 20000)\n",
    "logits.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([4, 512]), torch.Size([4, 512]))"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# and the CLIP-like text and image embeddings as\n",
    "\n",
    "text_embeds, image_embeds = coca(\n",
    "    text = text,\n",
    "    images = images,\n",
    "    return_embeddings = True\n",
    ") # (4, 512), (4, 512)\n",
    "text_embeds.shape, image_embeds.shape"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
