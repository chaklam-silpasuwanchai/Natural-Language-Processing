{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# [CoCa: Contrastive Captioners are Image-Text Foundation Models](https://arxiv.org/pdf/2205.01917.pdf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "https://github.com/jiaowoguanren0615/CoCa-Pytorch/tree/main/CoCa"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip3 install coca-pytorch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device cpu\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Extractor(\n",
       "  (vit): ViT(\n",
       "    (to_patch_embedding): Sequential(\n",
       "      (0): Rearrange('b c (h p1) (w p2) -> b (h w) (p1 p2 c)', p1=32, p2=32)\n",
       "      (1): LayerNorm((3072,), eps=1e-05, elementwise_affine=True)\n",
       "      (2): Linear(in_features=3072, out_features=1024, bias=True)\n",
       "      (3): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "    )\n",
       "    (dropout): Dropout(p=0.0, inplace=False)\n",
       "    (transformer): Transformer(\n",
       "      (norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "      (layers): ModuleList(\n",
       "        (0-5): 6 x ModuleList(\n",
       "          (0): Attention(\n",
       "            (norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "            (attend): Softmax(dim=-1)\n",
       "            (dropout): Dropout(p=0.0, inplace=False)\n",
       "            (to_qkv): Linear(in_features=1024, out_features=3072, bias=False)\n",
       "            (to_out): Sequential(\n",
       "              (0): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (1): Dropout(p=0.0, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (1): FeedForward(\n",
       "            (net): Sequential(\n",
       "              (0): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "              (1): Linear(in_features=1024, out_features=2048, bias=True)\n",
       "              (2): GELU(approximate='none')\n",
       "              (3): Dropout(p=0.0, inplace=False)\n",
       "              (4): Linear(in_features=2048, out_features=1024, bias=True)\n",
       "              (5): Dropout(p=0.0, inplace=False)\n",
       "            )\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (to_latent): Identity()\n",
       "    (mlp_head): Linear(in_features=1024, out_features=1000, bias=True)\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "device = torch.device(\"cuda:1\" if torch.cuda.is_available() else \"cpu\")\n",
    "print('Using device', device)\n",
    "\n",
    "# import vision transformer\n",
    "# from vit_pytorch.simple_vit_with_patch_dropout import SimpleViT\n",
    "from vit_pytorch import ViT\n",
    "from vit_pytorch.extractor import Extractor\n",
    "\n",
    "vit = ViT(\n",
    "    image_size = 256,\n",
    "    patch_size = 32,\n",
    "    num_classes = 1000,\n",
    "    dim = 1024,\n",
    "    depth = 6,\n",
    "    heads = 16,\n",
    "    mlp_dim = 2048,\n",
    "    # patch_dropout = 0.5  # https://arxiv.org/abs/2212.00794\n",
    ")\n",
    "\n",
    "vit = Extractor(vit, return_embeddings_only = True, detach = False)\n",
    "vit\n",
    "# extractor will enable it so the vision transformer returns its embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "CoCa(\n",
       "  (token_emb): Embedding(20000, 512)\n",
       "  (img_encoder): Extractor(\n",
       "    (vit): ViT(\n",
       "      (to_patch_embedding): Sequential(\n",
       "        (0): Rearrange('b c (h p1) (w p2) -> b (h w) (p1 p2 c)', p1=32, p2=32)\n",
       "        (1): LayerNorm((3072,), eps=1e-05, elementwise_affine=True)\n",
       "        (2): Linear(in_features=3072, out_features=1024, bias=True)\n",
       "        (3): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "      )\n",
       "      (dropout): Dropout(p=0.0, inplace=False)\n",
       "      (transformer): Transformer(\n",
       "        (norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "        (layers): ModuleList(\n",
       "          (0-5): 6 x ModuleList(\n",
       "            (0): Attention(\n",
       "              (norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "              (attend): Softmax(dim=-1)\n",
       "              (dropout): Dropout(p=0.0, inplace=False)\n",
       "              (to_qkv): Linear(in_features=1024, out_features=3072, bias=False)\n",
       "              (to_out): Sequential(\n",
       "                (0): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "                (1): Dropout(p=0.0, inplace=False)\n",
       "              )\n",
       "            )\n",
       "            (1): FeedForward(\n",
       "              (net): Sequential(\n",
       "                (0): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "                (1): Linear(in_features=1024, out_features=2048, bias=True)\n",
       "                (2): GELU(approximate='none')\n",
       "                (3): Dropout(p=0.0, inplace=False)\n",
       "                (4): Linear(in_features=2048, out_features=1024, bias=True)\n",
       "                (5): Dropout(p=0.0, inplace=False)\n",
       "              )\n",
       "            )\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "      (to_latent): Identity()\n",
       "      (mlp_head): Linear(in_features=1024, out_features=1000, bias=True)\n",
       "    )\n",
       "  )\n",
       "  (img_attn_pool): CrossAttention(\n",
       "    (norm): LayerNorm()\n",
       "    (context_norm): LayerNorm()\n",
       "    (to_q): Linear(in_features=512, out_features=512, bias=False)\n",
       "    (to_kv): Linear(in_features=1024, out_features=128, bias=False)\n",
       "    (to_out): Linear(in_features=512, out_features=512, bias=False)\n",
       "  )\n",
       "  (img_attn_pool_norm): LayerNorm()\n",
       "  (text_cls_norm): LayerNorm()\n",
       "  (img_to_latents): EmbedToLatents(\n",
       "    (to_latents): Linear(in_features=512, out_features=512, bias=False)\n",
       "  )\n",
       "  (text_to_latents): EmbedToLatents(\n",
       "    (to_latents): Linear(in_features=512, out_features=512, bias=False)\n",
       "  )\n",
       "  (unimodal_layers): ModuleList(\n",
       "    (0-5): 6 x Residual(\n",
       "      (fn): ParallelTransformerBlock(\n",
       "        (norm): LayerNorm()\n",
       "        (rotary_emb): RotaryEmbedding()\n",
       "        (fused_attn_ff_proj): Linear(in_features=512, out_features=4736, bias=False)\n",
       "        (attn_out): Linear(in_features=512, out_features=512, bias=False)\n",
       "        (ff_out): Sequential(\n",
       "          (0): SwiGLU()\n",
       "          (1): Linear(in_features=2048, out_features=512, bias=False)\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "  )\n",
       "  (multimodal_layers): ModuleList(\n",
       "    (0-5): 6 x ModuleList(\n",
       "      (0): Residual(\n",
       "        (fn): ParallelTransformerBlock(\n",
       "          (norm): LayerNorm()\n",
       "          (rotary_emb): RotaryEmbedding()\n",
       "          (fused_attn_ff_proj): Linear(in_features=512, out_features=4736, bias=False)\n",
       "          (attn_out): Linear(in_features=512, out_features=512, bias=False)\n",
       "          (ff_out): Sequential(\n",
       "            (0): SwiGLU()\n",
       "            (1): Linear(in_features=2048, out_features=512, bias=False)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "      (1): Residual(\n",
       "        (fn): CrossAttention(\n",
       "          (norm): LayerNorm()\n",
       "          (context_norm): Identity()\n",
       "          (to_q): Linear(in_features=512, out_features=512, bias=False)\n",
       "          (to_kv): Linear(in_features=512, out_features=128, bias=False)\n",
       "          (to_out): Linear(in_features=512, out_features=512, bias=False)\n",
       "          (ff): Sequential(\n",
       "            (0): Linear(in_features=512, out_features=4096, bias=False)\n",
       "            (1): SwiGLU()\n",
       "            (2): Linear(in_features=2048, out_features=512, bias=False)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "  )\n",
       "  (to_logits): Sequential(\n",
       "    (0): LayerNorm()\n",
       "    (1): Linear(in_features=512, out_features=20000, bias=False)\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# import CoCa and instantiate it\n",
    "from coca_pytorch.coca_pytorch import CoCa\n",
    "\n",
    "coca = CoCa(\n",
    "    dim = 512,                     # model dimension\n",
    "    img_encoder = vit,             # vision transformer - image encoder, returning image embeddings as (batch, seq, dim)\n",
    "    image_dim = 1024,              # image embedding dimension, if not the same as model dimensions\n",
    "    num_tokens = 20000,            # number of text tokens\n",
    "    unimodal_depth = 6,            # depth of the unimodal transformer\n",
    "    multimodal_depth = 6,          # depth of the multimodal transformer\n",
    "    dim_head = 64,                 # dimension per attention head\n",
    "    heads = 8,                     # number of attention heads\n",
    "    caption_loss_weight = 1.,      # weight on the autoregressive caption loss\n",
    "    contrastive_loss_weight = 1.,  # weight on the contrastive loss between image and text CLS embeddings\n",
    ").to(device)\n",
    "coca"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# mock text and images\n",
    "\n",
    "text = torch.randint(0, 20000, (4, 512)).to(device)\n",
    "images = torch.randn(4, 3, 256, 256).to(device)\n",
    "\n",
    "# train by giving CoCa your text and images with `return_loss = True`\n",
    "\n",
    "loss = coca(\n",
    "    text = text,\n",
    "    images = images,\n",
    "    return_loss = True  # set this to True to get the full caption + contrastive loss\n",
    ")\n",
    "\n",
    "loss.backward()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([4, 512, 20000])"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# do the above for as much text and images...\n",
    "# then you can get the caption logits as so\n",
    "logits = coca(\n",
    "    text = text,\n",
    "    images = images\n",
    ") # (4, 512, 20000)\n",
    "logits.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([4, 512]), torch.Size([4, 512]))"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# and the CLIP-like text and image embeddings as\n",
    "\n",
    "text_embeds, image_embeds = coca(\n",
    "    text = text,\n",
    "    images = images,\n",
    "    return_embeddings = True\n",
    ") # (4, 512), (4, 512)\n",
    "text_embeds.shape, image_embeds.shape"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
