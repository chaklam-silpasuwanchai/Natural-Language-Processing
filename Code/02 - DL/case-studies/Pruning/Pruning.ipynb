{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Pruning\n",
    "\n",
    "This tutorial will be based on *Torchtext + Padded + BiLSTM* under *Classification* folder.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cpu\n"
     ]
    }
   ],
   "source": [
    "import torch, torchdata, torchtext\n",
    "from torch import nn\n",
    "\n",
    "import time\n",
    "\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(device)\n",
    "\n",
    "#make our work comparable if restarted the kernel\n",
    "SEED = 1234\n",
    "torch.manual_seed(SEED)\n",
    "torch.backends.cudnn.deterministic = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# torch.cuda.get_device_name(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'2.1.2'"
      ]
     },
     "execution_count": 106,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.__version__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'0.16.2'"
      ]
     },
     "execution_count": 107,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torchtext.__version__"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "## 1. ETL: Loading the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [],
   "source": [
    "#uncomment this if you are not using our department puffer\n",
    "# import os\n",
    "# os.environ['http_proxy']  = 'http://192.41.170.23:3128'\n",
    "# os.environ['https_proxy'] = 'http://192.41.170.23:3128'\n",
    "\n",
    "from torchtext.datasets import AG_NEWS\n",
    "train, test = AG_NEWS()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_size = len(list(iter(train)))\n",
    "too_much, train, valid = train.random_split(total_length=train_size, weights = {\"too_much\": 0.7, \"smaller_train\": 0.2, \"valid\": 0.1}, seed=999)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "## 2. Preprocessing \n",
    "\n",
    "### Tokenizing\n",
    "\n",
    "The first step is to decide which tokenizer we want to use, which depicts how we split our sentences."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['We', 'are', 'learning', 'torchtext', 'in', 'AIT', '!']"
      ]
     },
     "execution_count": 110,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#pip install spacy\n",
    "#python3 -m spacy download en_core_web_sm\n",
    "from torchtext.data.utils import get_tokenizer\n",
    "tokenizer = get_tokenizer('spacy', language='en_core_web_sm')\n",
    "tokens = tokenizer(\"We are learning torchtext in AIT!\")  #some test\n",
    "tokens"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Text to integers (numeral)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next we gonna create function (torchtext called vocabs) that turn these tokens into integers.  Here we use built in factory function <code>build_vocab_from_iterator</code> which accepts iterator that yield list or iterator of tokens."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torchtext.vocab import build_vocab_from_iterator\n",
    "def yield_tokens(data_iter):\n",
    "    for _, text in data_iter:\n",
    "        yield tokenizer(text)\n",
    "\n",
    "vocab = build_vocab_from_iterator(yield_tokens(train), specials=['<unk>', '<pad>', '<bos>', '<eos>'])\n",
    "vocab.set_default_index(vocab[\"<unk>\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "## 3. FastText Embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torchtext.vocab import FastText\n",
    "fast_vectors = FastText(language='simple') #small for easy training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [],
   "source": [
    "fast_embedding = fast_vectors.get_vecs_by_tokens(vocab.get_itos()).to(device)\n",
    "# vocab.get_itos() returns a list of strings (tokens), where the token at the i'th position is what you get from doing vocab[token]\n",
    "# get_vecs_by_tokens gets the pre-trained vector for each string when given a list of strings\n",
    "# therefore pretrained_embedding is a fully \"aligned\" embedding matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([52828, 300])"
      ]
     },
     "execution_count": 114,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "fast_embedding.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "## 4. Preparing the dataloader"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In torchtext, first thing before the batch iterator is to define how you want to process your text and label.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [],
   "source": [
    "text_pipeline  = lambda x: vocab(tokenizer(x))\n",
    "label_pipeline = lambda x: int(x) - 1 #turn {1, 2, 3, 4} to {0, 1, 2, 3} for pytorch training "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, let's make the batch iterator.  Here we create a function <code>collate_fn</code> that define how we want to create our batch.  **We gonna add length of the sequence since packed padded sequences require this.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data   import DataLoader\n",
    "from torch.nn.utils.rnn import pad_sequence\n",
    "\n",
    "pad_idx = vocab['<pad>'] #++<----making sure our embedding layer ignores pad\n",
    "\n",
    "def collate_batch(batch):\n",
    "    label_list, text_list, length_list = [], [], []\n",
    "    for (_label, _text) in batch:\n",
    "        label_list.append(label_pipeline(_label))\n",
    "        processed_text = torch.tensor(text_pipeline(_text), dtype=torch.int64)\n",
    "        text_list.append(processed_text)\n",
    "        length_list.append(processed_text.size(0))  #++<-----packed padded sequences require \n",
    "    #criterion expects float labels\n",
    "    return torch.tensor(label_list, dtype=torch.int64), pad_sequence(text_list,  padding_value=pad_idx, batch_first=True), torch.tensor(length_list, dtype=torch.int64)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create train, val, and test dataloaders"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 64\n",
    "\n",
    "train_loader = DataLoader(train, batch_size=batch_size,\n",
    "                              shuffle=True, collate_fn=collate_batch)\n",
    "valid_loader = DataLoader(valid, batch_size=batch_size,\n",
    "                              shuffle=False, collate_fn=collate_batch)\n",
    "test_loader  = DataLoader(test, batch_size=batch_size,\n",
    "                             shuffle=False, collate_fn=collate_batch)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "## 5. Model and Evaluate\n",
    "\n",
    "Here we will simply evaluate the model that we have saved before.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "\n",
    "class LSTM(nn.Module):\n",
    "    def __init__(self, input_dim, emb_dim, hid_dim, output_dim, num_layers, bidirectional, dropout):\n",
    "        super().__init__()\n",
    "        self.embedding = nn.Embedding(input_dim, emb_dim, padding_idx=pad_idx)\n",
    "        self.lstm = nn.LSTM(emb_dim, \n",
    "                           hid_dim, \n",
    "                           num_layers=num_layers, \n",
    "                           bidirectional=bidirectional, \n",
    "                           dropout=dropout,\n",
    "                           batch_first=True)\n",
    "        self.fc = nn.Linear(hid_dim * 2, output_dim)\n",
    "        \n",
    "    def forward(self, text, text_lengths):\n",
    "        embedded = self.embedding(text)\n",
    "        packed_embedded = nn.utils.rnn.pack_padded_sequence(embedded, text_lengths.to('cpu'), enforce_sorted=False, batch_first=True)\n",
    "        packed_output, (hn, cn) = self.lstm(packed_embedded)  #if no h0, all zeroes\n",
    "        output, output_lengths = nn.utils.rnn.pad_packed_sequence(packed_output, batch_first=True)\n",
    "        hn = torch.cat((hn[-2,:,:], hn[-1,:,:]), dim = 1)\n",
    "        \n",
    "        return self.fc(hn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_dim  = len(vocab)\n",
    "hid_dim    = 256\n",
    "emb_dim    = 300         #**<----change to 300\n",
    "output_dim = 4 #four classes\n",
    "\n",
    "#for biLSTM\n",
    "num_layers = 2\n",
    "bidirectional = True\n",
    "dropout = 0.5\n",
    "\n",
    "model = LSTM(input_dim, emb_dim, hid_dim, output_dim, num_layers, bidirectional, dropout).to(device)\n",
    "model.embedding.weight.data = fast_embedding #**<------applied the fast text embedding as the initial weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {},
   "outputs": [],
   "source": [
    "criterion = nn.CrossEntropyLoss() #combine softmax with cross entropy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {},
   "outputs": [],
   "source": [
    "def accuracy(preds, y):\n",
    "    \n",
    "    predicted = torch.max(preds.data, 1)[1]\n",
    "    batch_corr = (predicted == y).sum()\n",
    "    acc = batch_corr / len(y)\n",
    "    \n",
    "    return acc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate(model, loader, criterion, loader_length):\n",
    "    epoch_loss = 0\n",
    "    epoch_acc = 0\n",
    "    model.eval()\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for i, (label, text, text_length) in enumerate(loader): \n",
    "            label = label.to(device) #(batch_size, )\n",
    "            text  = text.to(device)  #(seq len, batch_size)\n",
    "\n",
    "            predictions = model(text, text_length).squeeze(1) \n",
    "            \n",
    "            loss = criterion(predictions, label)\n",
    "            acc  = accuracy(predictions, label)\n",
    "\n",
    "            epoch_loss += loss.item()\n",
    "            epoch_acc += acc.item()\n",
    "        \n",
    "    return epoch_loss / loader_length, epoch_acc / loader_length"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_loader_length  = len(list(iter(test_loader)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Loss: 0.251 | Test Acc: 91.54%\n"
     ]
    }
   ],
   "source": [
    "save_path = f'models/{model.__class__.__name__}.pt'\n",
    "model.load_state_dict(torch.load(save_path, map_location=torch.device('cpu')))\n",
    "test_loss, test_acc = evaluate(model, test_loader, criterion, test_loader_length)\n",
    "\n",
    "print(f'Test Loss: {test_loss:.3f} | Test Acc: {test_acc*100:.2f}%')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Pruning\n",
    "\n",
    "Let's try prune and see the effect on the accuracy.  We will use `torch.nn.utils.prune` to prune our neural networks, and also learn how to extend it to implement our own custom pruning technique."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import torch.nn.utils.prune as prune"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's inspect a layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "fc = model.fc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Linear(in_features=512, out_features=4, bias=True)"
      ]
     },
     "execution_count": 127,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "fc #512 * 4 = 2048 parameters"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's see the weights.  Notice `weight` and `bias`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('weight', Parameter containing:\n",
      "tensor([[-0.1492, -0.0344,  0.0106,  ..., -0.0669, -0.0046, -0.0156],\n",
      "        [ 0.0277,  0.0525, -0.0747,  ...,  0.0243,  0.1342, -0.1535],\n",
      "        [ 0.0019,  0.1591, -0.0453,  ..., -0.0197,  0.0368, -0.0796],\n",
      "        [-0.0286, -0.1525, -0.0484,  ..., -0.0525,  0.0902,  0.0980]],\n",
      "       requires_grad=True)), ('bias', Parameter containing:\n",
      "tensor([ 0.0069, -0.0114,  0.0139, -0.0159], requires_grad=True))]\n"
     ]
    }
   ],
   "source": [
    "print(list(fc.named_parameters()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "weight : (4, 512)\n",
      "bias : (4,)\n"
     ]
    }
   ],
   "source": [
    "for l in list(fc.named_parameters()):\n",
    "    print(l[0], ':', l[1].detach().numpy().shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's also see the `buffers`.  Buffer is simply a tensor but does not involved in gradient update.  Buffer is like hidden variable and is useful to store different things (is like hidden in html)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[]\n"
     ]
    }
   ],
   "source": [
    "#nothing yet\n",
    "print(list(fc.named_buffers()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To prune a module, there are many pruning techniques available in \n",
    "`torch.nn.utils.prune` (or [implement](#extending-torch-nn-utils-pruning-with-custom-pruning-functions)\n",
    "your own by subclassing `BasePruningMethod`). \n",
    "\n",
    "### 6.1 Random Pruning\n",
    "\n",
    "In this example, we will prune at random 5% of the connections in \n",
    "the parameter named `weight` in the `fc` layer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Linear(in_features=512, out_features=4, bias=True)"
      ]
     },
     "execution_count": 131,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "prune.random_unstructured(fc, name=\"weight\", amount=0.95)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's check the `fc` layer.  Notice the weight is now replaced with `weight_orig`, which is basically the original one."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('bias', Parameter containing:\n",
      "tensor([ 0.0069, -0.0114,  0.0139, -0.0159], requires_grad=True)), ('weight_orig', Parameter containing:\n",
      "tensor([[-0.1492, -0.0344,  0.0106,  ..., -0.0669, -0.0046, -0.0156],\n",
      "        [ 0.0277,  0.0525, -0.0747,  ...,  0.0243,  0.1342, -0.1535],\n",
      "        [ 0.0019,  0.1591, -0.0453,  ..., -0.0197,  0.0368, -0.0796],\n",
      "        [-0.0286, -0.1525, -0.0484,  ..., -0.0525,  0.0902,  0.0980]],\n",
      "       requires_grad=True))]\n"
     ]
    }
   ],
   "source": [
    "print(list(fc.named_parameters()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now you may wonder how the pruning happens.  When forwarding, pytorch will apply the `weight_mask` stored in `named_buffers` to `weight_orig`.   Notice how the `named_buffers` has a `weight_mask` with bunch of 0 and 1, where this mask will be multipled with `weight_orig` resulting in some weight pruned."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('weight_mask', tensor([[0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 1.,  ..., 0., 0., 0.]]))]\n"
     ]
    }
   ],
   "source": [
    "print(list(fc.named_buffers()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "The pruning techniques implemented in `torch.nn.utils.prune` compute the pruned version of the weight (by \n",
    "combining the mask with the original parameter) and store them in the attribute `weight`. Note, this is no longer a parameter of the `fc`, it is now simply an attribute."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[-0.0000, -0.0000,  0.0000,  ..., -0.0000, -0.0000, -0.0000],\n",
      "        [ 0.0000,  0.0000, -0.0000,  ...,  0.0000,  0.0000, -0.0000],\n",
      "        [ 0.0000,  0.0000, -0.0000,  ..., -0.0000,  0.0000, -0.0000],\n",
      "        [-0.0000, -0.0000, -0.0484,  ..., -0.0000,  0.0000,  0.0000]],\n",
      "       grad_fn=<MulBackward0>)\n"
     ]
    }
   ],
   "source": [
    "print(fc.weight)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Last thing, you may wonder how you can be sure PyTorch prune the network really during forward.   Actually, pruning is applied prior to each forward pass using PyTorch's `forward_pre_hooks`. Specifically, when the `fc` is pruned, it will acquire a `forward_pre_hook` for each parameter associated with it that gets pruned. In this case, since we have so far only pruned the original parameter named `fc`, only one hook will be present. **Please note that you don't have to do anything.  I am just trying to say the internal mechnanism how PyTorch does stuff**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "OrderedDict([(11, <torch.nn.utils.prune.RandomUnstructured object at 0x176bab290>)])\n"
     ]
    }
   ],
   "source": [
    "print(fc._forward_pre_hooks)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Lastly, let's try classification using our randomly pruned version."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Loss: 1.164 | Test Acc: 87.39%\n"
     ]
    }
   ],
   "source": [
    "test_loss, test_acc = evaluate(model, test_loader, criterion, test_loader_length)\n",
    "print(f'Test Loss: {test_loss:.3f} | Test Acc: {test_acc*100:.2f}%')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "Wow, pruning 95% still get 88% accuracy!  In case you want to make your model permanently pruned, just use the command `remove`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('bias', Parameter containing:\n",
      "tensor([ 0.0069, -0.0114,  0.0139, -0.0159], requires_grad=True)), ('weight', Parameter containing:\n",
      "tensor([[-0.0000, -0.0000,  0.0000,  ..., -0.0000, -0.0000, -0.0000],\n",
      "        [ 0.0000,  0.0000, -0.0000,  ...,  0.0000,  0.0000, -0.0000],\n",
      "        [ 0.0000,  0.0000, -0.0000,  ..., -0.0000,  0.0000, -0.0000],\n",
      "        [-0.0000, -0.0000, -0.0484,  ..., -0.0000,  0.0000,  0.0000]],\n",
      "       requires_grad=True))]\n"
     ]
    }
   ],
   "source": [
    "#make it permanent....\n",
    "prune.remove(fc, 'weight')\n",
    "print(list(fc.named_parameters()))  #notice now the weight is the pruned version and weight_orig is gone."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "### 6.2 Magnitude Pruning\n",
    "\n",
    "We can also prune less randomly, but based on the lowest magnitude, which indicates that those parameters has less importance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "#reset the pruning\n",
    "def reset():\n",
    "    model = LSTM(input_dim, emb_dim, hid_dim, output_dim, num_layers, bidirectional, dropout).to(device)\n",
    "    model.load_state_dict(torch.load(save_path, map_location=torch.device('cpu')))\n",
    "    return model\n",
    "    \n",
    "model = reset()\n",
    "fc = model.fc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[-0.1492, -0.0000,  0.0000,  ..., -0.0000, -0.0000, -0.0000],\n",
      "        [ 0.0000,  0.0000, -0.0000,  ...,  0.0000,  0.0000, -0.1535],\n",
      "        [ 0.0000,  0.1591, -0.0000,  ..., -0.0000,  0.0000, -0.0000],\n",
      "        [-0.0000, -0.1525, -0.0000,  ..., -0.0000,  0.0000,  0.0000]],\n",
      "       grad_fn=<MulBackward0>)\n"
     ]
    }
   ],
   "source": [
    "#prune based on the lowest magnitude based on l1 norm\n",
    "prune.l1_unstructured(fc, name='weight', amount=0.95)\n",
    "print(fc.weight)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In case you want to prune based on L2 norm on specific dim, use `ln_structured`, where structured means on particular dimension, while `n` refers to the type of norm.  I have commented in case you wanna try."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# prune.ln_structured(fc, name=\"weight\", amount=0.5, n=2, dim=0)\n",
    "# print(fc.weight)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Loss: 0.542 | Test Acc: 91.22%\n"
     ]
    }
   ],
   "source": [
    "test_loss, test_acc = evaluate(model, test_loader, criterion, test_loader_length)\n",
    "print(f'Test Loss: {test_loss:.3f} | Test Acc: {test_acc*100:.2f}%')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6.3 Pruning multiple parameters in a model \n",
    "\n",
    "By specifying the desired pruning technique and parameters, we can easily prune multiple tensors in a network."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dict_keys(['embedding.weight_mask', 'fc.weight_mask'])\n"
     ]
    }
   ],
   "source": [
    "model = reset()\n",
    "\n",
    "for name, module in model.named_modules():\n",
    "    # prune 99% of connections in all embedding layers \n",
    "    if isinstance(module, torch.nn.Embedding):\n",
    "        prune.l1_unstructured(module, name='weight', amount=0.99)  #forward weight (you can check the name in named_parameters)\n",
    "    # prune 50% of connections in all linear layers \n",
    "    elif isinstance(module, torch.nn.Linear):\n",
    "        prune.l1_unstructured(module, name='weight', amount=0.5)\n",
    "\n",
    "print(dict(model.named_buffers()).keys())  # to verify that all masks exist"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Loss: 0.820 | Test Acc: 69.60%\n"
     ]
    }
   ],
   "source": [
    "test_loss, test_acc = evaluate(model, test_loader, criterion, test_loader_length)\n",
    "print(f'Test Loss: {test_loss:.3f} | Test Acc: {test_acc*100:.2f}%')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6.4 Global pruning\n",
    "\n",
    "So far, we only looked at what is usually referred to as \"local\" pruning, i.e. the practice of pruning tensors in a model one by one, by  comparing the statistics (weight magnitude, activation, gradient, etc.) of  each entry exclusively to the other entries in that tensor. However, a  common and perhaps more powerful technique is to prune the model all at  once, by removing (for example) the lowest 20% of connections across the  whole model, instead of removing the lowest 20% of connections in each  layer. This is likely to result in different pruning percentages per layer.\n",
    "\n",
    "Let's see how to do that using `global_unstructured` from `torch.nn.utils.prune`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "model = reset()\n",
    "\n",
    "parameters_to_prune = (\n",
    "    (model.embedding, 'weight'),\n",
    "    (model.lstm, 'weight_ih_l0'),\n",
    "    (model.lstm, 'weight_hh_l0'),\n",
    "    (model.fc, 'weight'),\n",
    "    (model.fc, 'bias'),\n",
    ")\n",
    "\n",
    "prune.global_unstructured(\n",
    "    parameters_to_prune,\n",
    "    pruning_method=prune.L1Unstructured,\n",
    "    amount=0.7,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sparsity in embedding.weight: 69.48%\n",
      "Sparsity in lstm.weight_ih_l0: 78.33%\n",
      "Sparsity in lstm.weight_hh_l0: 91.77%\n",
      "Sparsity in fc.weight: 65.43%\n",
      "Sparsity in fc.bias: 100.00%\n",
      "Global sparsity: 70.00%\n"
     ]
    }
   ],
   "source": [
    "print(\n",
    "    \"Sparsity in embedding.weight: {:.2f}%\".format(\n",
    "        100. * float(torch.sum(model.embedding.weight == 0))\n",
    "        / float(model.embedding.weight.nelement())\n",
    "    )\n",
    ")\n",
    "print(\n",
    "    \"Sparsity in lstm.weight_ih_l0: {:.2f}%\".format(\n",
    "        100. * float(torch.sum(model.lstm.weight_ih_l0 == 0))\n",
    "        / float(model.lstm.weight_ih_l0.nelement())\n",
    "    )\n",
    ")\n",
    "print(\n",
    "    \"Sparsity in lstm.weight_hh_l0: {:.2f}%\".format(\n",
    "        100. * float(torch.sum(model.lstm.weight_hh_l0 == 0))\n",
    "        / float(model.lstm.weight_hh_l0.nelement())\n",
    "    )\n",
    ")\n",
    "print(\n",
    "    \"Sparsity in fc.weight: {:.2f}%\".format(\n",
    "        100. * float(torch.sum(model.fc.weight == 0))\n",
    "        / float(model.fc.weight.nelement())\n",
    "    )\n",
    ")\n",
    "print(\n",
    "    \"Sparsity in fc.bias: {:.2f}%\".format(\n",
    "        100. * float(torch.sum(model.fc.bias == 0))\n",
    "        / float(model.fc.bias.nelement())\n",
    "    )\n",
    ")\n",
    "print(\n",
    "    \"Global sparsity: {:.2f}%\".format(\n",
    "        100. * float(\n",
    "            torch.sum(model.embedding.weight == 0)\n",
    "            + torch.sum(model.lstm.weight_ih_l0 == 0)\n",
    "            + torch.sum(model.lstm.weight_hh_l0 == 0)\n",
    "            + torch.sum(model.fc.weight == 0)\n",
    "            + torch.sum(model.fc.bias == 0)\n",
    "        )\n",
    "        / float(\n",
    "            model.embedding.weight.nelement()\n",
    "            + model.lstm.weight_ih_l0.nelement()\n",
    "            + model.lstm.weight_hh_l0.nelement()\n",
    "            + model.fc.weight.nelement()\n",
    "            + model.fc.bias.nelement()\n",
    "        )\n",
    "    )\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Loss: 0.313 | Test Acc: 90.30%\n"
     ]
    }
   ],
   "source": [
    "test_loss, test_acc = evaluate(model, test_loader, criterion, test_loader_length)\n",
    "print(f'Test Loss: {test_loss:.3f} | Test Acc: {test_acc*100:.2f}%')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Wow, pruning 70% of parameters...but still getting 90% of accuracy!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6.5 Custom pruning function\n",
    "\n",
    "To implement your own pruning function, you can extend the `nn.utils.prune` module by subclassing the `BasePruningMethod`\n",
    "base class, the same way all other pruning methods do. The base class implements the following methods for you: `__call__`, `apply_mask`,`apply`, `prune`, and `remove`. Beyond some special cases, you shouldn't have to reimplement these methods for your new pruning technique.\n",
    "\n",
    "You will, however, have to implement `__init__` (the constructor), and `compute_mask` (the instructions on how to compute the mask\n",
    "for the given tensor according to the logic of your pruning technique). In addition, you will have to specify which type of\n",
    "pruning this technique implements (supported options are `global`,`structured`, and `unstructured`). This is needed to determine how to combine masks in the case in which pruning is applied iteratively. In other words, when pruning a pre-pruned parameter, the current prunining techique is expected to act on the unpruned portion of the parameter. Specifying the `PRUNING_TYPE` will enable the `PruningContainer` (which handles the iterative application of pruning masks) to correctly identify the slice of the parameter to prune.\n",
    "\n",
    "Let's assume, for example, that you want to implement a pruning technique that prunes every other entry in a tensor (or -- if the\n",
    "tensor has previously been pruned -- in the remaining unpruned portion of the tensor). This will be of `PRUNING_TYPE='unstructured'`\n",
    "because it acts on individual connections in a layer and not on entire units/channels (`'structured'`), or across different parameters\n",
    "(`'global'`)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "class ExamplePruningMethod(prune.BasePruningMethod):\n",
    "    \"\"\"Prune every other entry in a tensor\n",
    "    \"\"\"\n",
    "    PRUNING_TYPE = 'unstructured'\n",
    "\n",
    "    def compute_mask(self, t, default_mask):\n",
    "        mask = default_mask.clone()\n",
    "        mask.view(-1)[::2] = 0 \n",
    "        return mask"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "Now, to apply this to a parameter in an ``nn.Module``, you should also provide a simple function that instantiates the method and\n",
    "applies it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def prune_custom_unstructured(module, name):\n",
    "    \"\"\"Prunes tensor corresponding to parameter called `name` in `module`\n",
    "    by removing every other entry in the tensors.\n",
    "    Modifies module in place (and also return the modified module) \n",
    "    by:\n",
    "    1) adding a named buffer called `name+'_mask'` corresponding to the \n",
    "    binary mask applied to the parameter `name` by the pruning method.\n",
    "    The parameter `name` is replaced by its pruned version, while the \n",
    "    original (unpruned) parameter is stored in a new parameter named \n",
    "    `name+'_orig'`.\n",
    "\n",
    "    Args:\n",
    "        module (nn.Module): module containing the tensor to prune\n",
    "        name (string): parameter name within `module` on which pruning\n",
    "                will act.\n",
    "\n",
    "    Returns:\n",
    "        module (nn.Module): modified (i.e. pruned) version of the input\n",
    "            module\n",
    "    \n",
    "    Examples:\n",
    "        >>> m = nn.Linear(3, 4)\n",
    "        >>> foobar_unstructured(m, name='bias')\n",
    "    \"\"\"\n",
    "    ExamplePruningMethod.apply(module, name)\n",
    "    return module"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([0., 1., 0., 1.])\n"
     ]
    }
   ],
   "source": [
    "model = reset()\n",
    "prune_custom_unstructured(model.fc, name='bias')\n",
    "\n",
    "print(model.fc.bias_mask)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Conclusion\n",
    "\n",
    "In practice, we may want to perform *iterative pruning* where we iteratively prune the lowest magnitude, perhaps after each finetuning epoch, until sparsity is reached.   You will be very amazed how much can be pruned without much impact on the performance."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.2"
  },
  "vscode": {
   "interpreter": {
    "hash": "becc4c8e5ad229b2591d820334d85e3db0111492344629bf57f272470dce75a5"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
