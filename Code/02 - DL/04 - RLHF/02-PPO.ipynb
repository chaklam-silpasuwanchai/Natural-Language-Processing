{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# [Proximal Policy Optimization Algorithms (PPO)](https://arxiv.org/pdf/1707.06347.pdf)\n",
    "### Reference Code\n",
    "- https://huggingface.co/docs/trl/main/en/ppo_trainer\n",
    "- https://github.com/huggingface/trl/blob/main/examples/scripts/ppo.py\n",
    "- https://github.com/huggingface/trl/blob/main/examples/notebooks/gpt2-sentiment-control.ipynb\n",
    "- https://github.com/huggingface/trl/blob/main/examples/notebooks/gpt2-sentiment.ipynb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip3 install peft trl"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.10/dist-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "/home/todsavadt/.local/lib/python3.10/site-packages/trl/trainer/ppo_config.py:141: UserWarning: The `optimize_cuda_cache` arguement will be deprecated soon, please use `optimize_device_cache` instead.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import tyro\n",
    "from accelerate import Accelerator\n",
    "from datasets import load_dataset\n",
    "from peft import LoraConfig\n",
    "from tqdm.auto import tqdm\n",
    "from transformers import AutoTokenizer, pipeline\n",
    "\n",
    "from trl import AutoModelForCausalLMWithValueHead, AutoModelForSeq2SeqLMWithValueHead, PPOConfig, PPOTrainer, set_seed\n",
    "from trl.core import LengthSampler\n",
    "from trl.import_utils import is_xpu_available"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "ppo_config = PPOConfig(\n",
    "    model_name=\"lvwerra/gpt2-imdb\",\n",
    "    query_dataset=\"imdb\",\n",
    "    reward_model=\"sentiment-analysis:lvwerra/distilbert-imdb\",\n",
    "    learning_rate=1.41e-5,\n",
    "    log_with=None,\n",
    "    mini_batch_size=128,\n",
    "    batch_size=128,\n",
    "    gradient_accumulation_steps=1,\n",
    "    early_stopping=False,\n",
    "    target_kl=6.0,\n",
    "    kl_penalty=\"kl\",\n",
    "    seed=0,\n",
    "    use_score_scaling=False,\n",
    "    use_score_norm=False,\n",
    "    score_clip=None,\n",
    ")\n",
    "\n",
    "use_seq2seq = False\n",
    "\n",
    "use_peft = False\n",
    "\n",
    "peft_config = LoraConfig(\n",
    "    r=16,\n",
    "    lora_alpha=16,\n",
    "    bias=\"none\",\n",
    "    task_type=\"CAUSAL_LM\",\n",
    ")\n",
    "\n",
    "trust_remote_code = False \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We then define the arguments to pass to the sentiment analysis pipeline.\n",
    "# We set `return_all_scores` to True to get the sentiment score for each token.\n",
    "sent_kwargs = {\n",
    "    \"return_all_scores\": True, \n",
    "    \"function_to_apply\": \"none\", \n",
    "    \"batch_size\": 16\n",
    "}\n",
    "\n",
    "trl_model_class = AutoModelForCausalLMWithValueHead if not use_seq2seq else AutoModelForSeq2SeqLMWithValueHead"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Below is an example function to build the dataset. In our case, we use the IMDB dataset\n",
    "# from the `datasets` library. One should customize this function to train the model on\n",
    "# its own dataset.\n",
    "def build_dataset(config, query_dataset, input_min_text_length=2, input_max_text_length=8):\n",
    "    \"\"\"\n",
    "    Build dataset for training. This builds the dataset from `load_dataset`, one should\n",
    "    customize this function to train the model on its own dataset.\n",
    "\n",
    "    Args:\n",
    "        query_dataset (`str`):\n",
    "            The name of the dataset to be loaded.\n",
    "\n",
    "    Returns:\n",
    "        dataloader (`torch.utils.data.DataLoader`):\n",
    "            The dataloader for the dataset.\n",
    "    \"\"\"\n",
    "    tokenizer = AutoTokenizer.from_pretrained(config.model_name, use_fast=True)\n",
    "    tokenizer.pad_token = tokenizer.eos_token\n",
    "    # load imdb with datasets\n",
    "    ds = load_dataset(query_dataset, split=\"train\")\n",
    "    ds = ds.rename_columns({\"text\": \"review\"})\n",
    "    ds = ds.filter(lambda x: len(x[\"review\"]) > 200, batched=False)\n",
    "\n",
    "    input_size = LengthSampler(input_min_text_length, input_max_text_length)\n",
    "\n",
    "    def tokenize(sample):\n",
    "        sample[\"input_ids\"] = tokenizer.encode(sample[\"review\"])[: input_size()]\n",
    "        sample[\"query\"] = tokenizer.decode(sample[\"input_ids\"])\n",
    "        return sample\n",
    "\n",
    "    ds = ds.map(tokenize, batched=False)\n",
    "    ds.set_format(type=\"torch\")\n",
    "    return ds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Found cached dataset imdb (/home/todsavadt/.cache/huggingface/datasets/imdb/plain_text/1.0.0/d613c88cf8fa3bab83b4ded3713f1f74830d1100e171db75bbddb80b3345c9c0)\n",
      "Loading cached processed dataset at /home/todsavadt/.cache/huggingface/datasets/imdb/plain_text/1.0.0/d613c88cf8fa3bab83b4ded3713f1f74830d1100e171db75bbddb80b3345c9c0/cache-cd96d6dda9f74d63.arrow\n",
      "Loading cached processed dataset at /home/todsavadt/.cache/huggingface/datasets/imdb/plain_text/1.0.0/d613c88cf8fa3bab83b4ded3713f1f74830d1100e171db75bbddb80b3345c9c0/cache-52c86166ad9f18f1.arrow\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Dataset({\n",
       "    features: ['review', 'label', 'input_ids', 'query'],\n",
       "    num_rows: 24895\n",
       "})"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# We retrieve the dataloader by calling the `build_dataset` function.\n",
    "dataset = build_dataset(ppo_config, ppo_config.query_dataset)\n",
    "dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'review': '\"I Am Curious: Yellow\" is a risible and pretentious steaming pile. It doesn\\'t matter what one\\'s political views are because this film can hardly be taken seriously on any level. As for the claim that frontal male nudity is an automatic NC-17, that isn\\'t true. I\\'ve seen R-rated films with male nudity. Granted, they only offer some fleeting views, but where are the R-rated films with gaping vulvas and flapping labia? Nowhere, because they don\\'t exist. The same goes for those crappy cable shows: schlongs swinging in the breeze but not a clitoris in sight. And those pretentious indie movies like The Brown Bunny, in which we\\'re treated to the site of Vincent Gallo\\'s throbbing johnson, but not a trace of pink visible on Chloe Sevigny. Before crying (or implying) \"double-standard\" in matters of nudity, the mentally obtuse should take into account one unavoidably obvious anatomical difference between men and women: there are no genitals on display when actresses appears nude, and the same cannot be said for a man. In fact, you generally won\\'t see female genitals in an American film in anything short of porn or explicit erotica. This alleged double-standard is less a double standard than an admittedly depressing ability to come to terms culturally with the insides of women\\'s bodies.',\n",
       " 'label': tensor(0),\n",
       " 'input_ids': tensor([    1,    40,  1703, 44269,    25, 12550]),\n",
       " 'query': '\"I Am Curious: Yellow'}"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def collator(data):\n",
    "    return dict((key, [d[key] for d in data]) for key in data[0])\n",
    "\n",
    "# set seed before initializing value head for deterministic eval\n",
    "set_seed(ppo_config.seed)\n",
    "\n",
    "# Now let's build the model, the reference model, and the tokenizer.\n",
    "if not use_peft:\n",
    "    ref_model = trl_model_class.from_pretrained(ppo_config.model_name, trust_remote_code=trust_remote_code)\n",
    "    device_map = None\n",
    "    peft_config = None\n",
    "else:\n",
    "    peft_config = peft_config\n",
    "    ref_model = None\n",
    "    # Copy the model to each device\n",
    "    device_map = {\"\": Accelerator().local_process_index}"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model & Tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = trl_model_class.from_pretrained(\n",
    "    ppo_config.model_name,\n",
    "    trust_remote_code=trust_remote_code,\n",
    "    device_map=device_map,\n",
    "    peft_config=peft_config,\n",
    ")\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(ppo_config.model_name, use_fast=True)\n",
    "# Some tokenizers like GPT-2's don't have a padding token by default, so we set one here.\n",
    "tokenizer.pad_token_id = tokenizer.eos_token_id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We then build the PPOTrainer, passing the model, the reference model, the tokenizer\n",
    "ppo_trainer = PPOTrainer(ppo_config, model, ref_model, tokenizer, dataset=dataset, data_collator=collator)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "PPOConfig(exp_name='ipykernel_launcher', seed=0, log_with=None, task_name=None, model_name='lvwerra/gpt2-imdb', query_dataset='imdb', reward_model='sentiment-analysis:lvwerra/distilbert-imdb', remove_unused_columns=True, tracker_kwargs={}, accelerator_kwargs={}, project_kwargs={}, tracker_project_name='trl', push_to_hub_if_best_kwargs={}, steps=20000, learning_rate=1.41e-05, adap_kl_ctrl=True, init_kl_coef=0.2, kl_penalty='kl', target=6, horizon=10000, gamma=1, lam=0.95, cliprange=0.2, cliprange_value=0.2, vf_coef=0.1, batch_size=128, forward_batch_size=None, mini_batch_size=128, gradient_accumulation_steps=1, world_size=1, ppo_epochs=4, max_grad_norm=None, optimize_cuda_cache=False, optimize_device_cache=False, early_stopping=False, target_kl=6.0, compare_steps=1, ratio_threshold=10.0, use_score_scaling=False, use_score_norm=False, score_clip=None, whiten_rewards=False, is_encoder_decoder=False, is_peft_model=False, backward_batch_size=128, global_backward_batch_size=128, global_batch_size=128)"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ppo_trainer.config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We then build the sentiment analysis pipeline, passing the model name and the\n",
    "# sentiment analysis pipeline arguments. Let's also make sure to set the device\n",
    "# to the same device as the PPOTrainer.\n",
    "device = ppo_trainer.accelerator.device\n",
    "if ppo_trainer.accelerator.num_processes == 1:\n",
    "    # if is_xpu_available():\n",
    "    #     device = \"xpu:0\"\n",
    "    # else:\n",
    "    device = 1 if torch.cuda.is_available() else \"cpu\"  # to avoid a `pipeline` bug\n",
    "ds_plugin = ppo_trainer.accelerator.state.deepspeed_plugin\n",
    "task, model_name = ppo_config.reward_model.split(\":\")\n",
    "\n",
    "if ds_plugin is not None and ds_plugin.is_zero3_init_enabled():\n",
    "    with ds_plugin.zero3_init_context_manager(enable=False):\n",
    "        sentiment_pipe = pipeline(task, model=model_name, device=device)\n",
    "else:\n",
    "    sentiment_pipe = pipeline(task, model=model_name, device=device)\n",
    "\n",
    "# Some tokenizers like GPT-2's don't have a padding token by default, so we set one here.\n",
    "if sentiment_pipe.tokenizer.pad_token_id is None:\n",
    "    sentiment_pipe.tokenizer.pad_token_id = tokenizer.pad_token_id\n",
    "\n",
    "if sentiment_pipe.model.config.pad_token_id is None:\n",
    "    sentiment_pipe.model.config.pad_token_id = tokenizer.pad_token_id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "PPOConfig(exp_name='ipykernel_launcher', seed=0, log_with=None, task_name=None, model_name='lvwerra/gpt2-imdb', query_dataset='imdb', reward_model='sentiment-analysis:lvwerra/distilbert-imdb', remove_unused_columns=True, tracker_kwargs={}, accelerator_kwargs={}, project_kwargs={}, tracker_project_name='trl', push_to_hub_if_best_kwargs={}, steps=20000, learning_rate=1.41e-05, adap_kl_ctrl=True, init_kl_coef=0.2, kl_penalty='kl', target=6, horizon=10000, gamma=1, lam=0.95, cliprange=0.2, cliprange_value=0.2, vf_coef=0.1, batch_size=128, forward_batch_size=None, mini_batch_size=128, gradient_accumulation_steps=1, world_size=1, ppo_epochs=4, max_grad_norm=None, optimize_cuda_cache=False, optimize_device_cache=False, early_stopping=False, target_kl=6.0, compare_steps=1, ratio_threshold=10.0, use_score_scaling=False, use_score_norm=False, score_clip=None, whiten_rewards=False, is_encoder_decoder=False, is_peft_model=False, backward_batch_size=128, global_backward_batch_size=128, global_batch_size=128)"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ppo_config"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We then define the arguments to pass to the `generate` function. These arguments\n",
    "# are passed to the `generate` function of the PPOTrainer, which is a wrapper around\n",
    "# the `generate` function of the trained model.\n",
    "generation_kwargs = {\n",
    "    \"min_length\": -1,\n",
    "    \"top_k\": 0.0,\n",
    "    \"top_p\": 1.0,\n",
    "    \"do_sample\": True,\n",
    "    \"pad_token_id\": tokenizer.eos_token_id,\n",
    "    \"max_new_tokens\": 32,\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/194 [00:00<?, ?it/s]You're using a GPT2TokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n",
      "/home/todsavadt/.local/lib/python3.10/site-packages/transformers/pipelines/text_classification.py:105: UserWarning: `return_all_scores` is now deprecated,  if want a similar functionality use `top_k=None` instead of `return_all_scores=True` or `top_k=1` instead of `return_all_scores=False`.\n",
      "  warnings.warn(\n",
      "  3%|▎         | 5/194 [01:42<1:17:12, 24.51s/it]/home/todsavadt/.local/lib/python3.10/site-packages/transformers/pipelines/base.py:1101: UserWarning: You seem to be using the pipelines sequentially on GPU. In order to maximize efficiency please use a dataset\n",
      "  warnings.warn(\n",
      "100%|██████████| 194/194 [1:36:29<00:00, 29.84s/it]\n"
     ]
    }
   ],
   "source": [
    "for step, batch in enumerate(tqdm(ppo_trainer.dataloader)):\n",
    "    query_tensors = batch[\"input_ids\"]\n",
    "\n",
    "    # Get response from gpt2\n",
    "    response_tensors, ref_response_tensors = ppo_trainer.generate(\n",
    "        query_tensors, return_prompt=False, generate_ref_response=True, **generation_kwargs\n",
    "    )\n",
    "    batch[\"response\"] = tokenizer.batch_decode(response_tensors)\n",
    "    batch[\"ref_response\"] = tokenizer.batch_decode(ref_response_tensors)\n",
    "\n",
    "    # Compute sentiment score\n",
    "    texts = [q + r for q, r in zip(batch[\"query\"], batch[\"response\"])]\n",
    "    pipe_outputs = sentiment_pipe(texts, **sent_kwargs)\n",
    "    rewards = [torch.tensor(output[1][\"score\"]) for output in pipe_outputs]\n",
    "    \n",
    "    ref_texts = [q + r for q, r in zip(batch[\"query\"], batch[\"ref_response\"])]\n",
    "    ref_pipe_outputs = sentiment_pipe(ref_texts, **sent_kwargs)\n",
    "    ref_rewards = [torch.tensor(output[1][\"score\"]) for output in ref_pipe_outputs]\n",
    "    \n",
    "    batch[\"ref_rewards\"] = ref_rewards\n",
    "\n",
    "    # Run PPO step\n",
    "    stats = ppo_trainer.step(query_tensors, response_tensors, rewards)\n",
    "    ppo_trainer.log_stats(stats, batch, rewards, columns_to_log=[\"query\", \"response\", \"ref_response\", \"ref_rewards\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
