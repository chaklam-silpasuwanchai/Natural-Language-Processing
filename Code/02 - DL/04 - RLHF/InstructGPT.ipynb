{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# [InstructGPT : Training language models to follow instructions with human feedback](https://arxiv.org/pdf/2203.02155.pdf)\n",
    "\n",
    "\n",
    "### Reference Code \n",
    "- https://github.com/xrsrke/instructGOOSE/tree/main\n",
    "\n",
    "<img src=\"./figures/instructGPT.png\" title=\"instructGPT\" />\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
    "from datasets import load_dataset\n",
    "\n",
    "import torch\n",
    "from torch.utils.data import DataLoader, random_split\n",
    "from torch import optim\n",
    "\n",
    "from instruct_goose import Agent, RewardModel, RLHFTrainer, RLHFConfig, create_reference_model"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Load dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = load_dataset(\"imdb\", split=\"train\")\n",
    "dataset, _ = random_split(dataset, lengths=[10, len(dataset) - 10]) # for demenstration purposes\n",
    "train_dataloader = DataLoader(dataset, batch_size=4, shuffle=True)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Load the pre-trained model and tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_base = AutoModelForCausalLM.from_pretrained(\"gpt2\") # for demonstration purposes\n",
    "reward_model = RewardModel(\"gpt2\")\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"gpt2\", padding_side=\"left\")\n",
    "eos_token_id = tokenizer.eos_token_id\n",
    "tokenizer.pad_token = tokenizer.eos_token"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Create the RL-based language model agent and the reference model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Agent(model_base)\n",
    "ref_model = create_reference_model(model)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "max_new_tokens = 20\n",
    "generation_kwargs = {\n",
    "    \"min_length\":-1,\n",
    "    \"top_k\": 0.0,\n",
    "    \"top_p\": 1.0,\n",
    "    \"do_sample\": True,\n",
    "    \"pad_token_id\": tokenizer.eos_token_id,\n",
    "    \"max_new_tokens\": max_new_tokens\n",
    "}\n",
    "\n",
    "config = RLHFConfig()\n",
    "N_EPOCH = 1 # for demonstration purposes\n",
    "trainer = RLHFTrainer(model, ref_model, config)\n",
    "optimizer = optim.SGD(model.parameters(), lr=1e-3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for epoch in range(N_EPOCH):\n",
    "    for batch in train_dataloader:\n",
    "        inputs = tokenizer(batch[\"text\"], padding=True, truncation=True, return_tensors=\"pt\")\n",
    "        response_ids = model.generate(\n",
    "            inputs[\"input_ids\"], attention_mask=inputs[\"attention_mask\"],\n",
    "            **generation_kwargs\n",
    "        )\n",
    "\n",
    "        # extract the generated text\n",
    "        response_ids = response_ids[:, -max_new_tokens:]\n",
    "        response_attention_mask = torch.ones_like(response_ids)\n",
    "\n",
    "        # evaluate from the reward model\n",
    "        with torch.no_grad():\n",
    "            text_input_ids = torch.stack([torch.concat([q, r]) for q, r in zip(inputs[\"input_ids\"], response_ids)], dim=0)\n",
    "            rewards = reward_model(text_input_ids)\n",
    "\n",
    "        # calculate PPO loss\n",
    "        loss = trainer.compute_loss(\n",
    "            query_ids=inputs[\"input_ids\"],\n",
    "            query_attention_mask=inputs[\"attention_mask\"],\n",
    "            response_ids=response_ids,\n",
    "            response_attention_mask=response_attention_mask,\n",
    "            rewards=rewards\n",
    "        )\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        print(f\"loss={loss}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.11.6"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
