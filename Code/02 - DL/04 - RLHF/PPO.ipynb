{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# [Proximal Policy Optimization Algorithms (PPO)](https://arxiv.org/pdf/1707.06347.pdf)\n",
    "### Reference Code\n",
    "- https://huggingface.co/docs/trl/main/en/ppo_trainer\n",
    "- https://github.com/huggingface/trl/blob/main/examples/scripts/ppo.py\n",
    "- https://github.com/huggingface/trl/blob/main/examples/notebooks/gpt2-sentiment-control.ipynb\n",
    "- https://github.com/huggingface/trl/blob/main/examples/notebooks/gpt2-sentiment.ipynb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import tyro\n",
    "from accelerate import Accelerator\n",
    "from datasets import load_dataset\n",
    "from peft import LoraConfig\n",
    "from tqdm import tqdm\n",
    "from transformers import AutoTokenizer, pipeline\n",
    "\n",
    "from trl import AutoModelForCausalLMWithValueHead, AutoModelForSeq2SeqLMWithValueHead, PPOConfig, PPOTrainer, set_seed\n",
    "from trl.core import LengthSampler\n",
    "from trl.import_utils import is_xpu_available"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ppo_config = PPOConfig(\n",
    "        model_name=\"lvwerra/gpt2-imdb\",\n",
    "        query_dataset=\"imdb\",\n",
    "        reward_model=\"sentiment-analysis:lvwerra/distilbert-imdb\",\n",
    "        learning_rate=1.41e-5,\n",
    "        log_with=None,\n",
    "        mini_batch_size=128,\n",
    "        batch_size=128,\n",
    "        gradient_accumulation_steps=1,\n",
    "        early_stopping=False,\n",
    "        target_kl=6.0,\n",
    "        kl_penalty=\"kl\",\n",
    "        seed=0,\n",
    "        use_score_scaling=False,\n",
    "        use_score_norm=False,\n",
    "        score_clip=None,\n",
    "    )\n",
    "\n",
    "use_seq2seq = False\n",
    "\n",
    "use_peft = False\n",
    "\n",
    "peft_config = LoraConfig(\n",
    "        r=16,\n",
    "        lora_alpha=16,\n",
    "        bias=\"none\",\n",
    "        task_type=\"CAUSAL_LM\",\n",
    "    )\n",
    "\n",
    "trust_remote_code = False \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We then define the arguments to pass to the sentiment analysis pipeline.\n",
    "# We set `return_all_scores` to True to get the sentiment score for each token.\n",
    "sent_kwargs = {\"return_all_scores\": True, \"function_to_apply\": \"none\", \"batch_size\": 16}\n",
    "\n",
    "trl_model_class = AutoModelForCausalLMWithValueHead if not use_seq2seq else AutoModelForSeq2SeqLMWithValueHead"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Below is an example function to build the dataset. In our case, we use the IMDB dataset\n",
    "# from the `datasets` library. One should customize this function to train the model on\n",
    "# its own dataset.\n",
    "def build_dataset(config, query_dataset, input_min_text_length=2, input_max_text_length=8):\n",
    "    \"\"\"\n",
    "    Build dataset for training. This builds the dataset from `load_dataset`, one should\n",
    "    customize this function to train the model on its own dataset.\n",
    "\n",
    "    Args:\n",
    "        query_dataset (`str`):\n",
    "            The name of the dataset to be loaded.\n",
    "\n",
    "    Returns:\n",
    "        dataloader (`torch.utils.data.DataLoader`):\n",
    "            The dataloader for the dataset.\n",
    "    \"\"\"\n",
    "    tokenizer = AutoTokenizer.from_pretrained(config.model_name)\n",
    "    tokenizer.pad_token = tokenizer.eos_token\n",
    "    # load imdb with datasets\n",
    "    ds = load_dataset(query_dataset, split=\"train\")\n",
    "    ds = ds.rename_columns({\"text\": \"review\"})\n",
    "    ds = ds.filter(lambda x: len(x[\"review\"]) > 200, batched=False)\n",
    "\n",
    "    input_size = LengthSampler(input_min_text_length, input_max_text_length)\n",
    "\n",
    "    def tokenize(sample):\n",
    "        sample[\"input_ids\"] = tokenizer.encode(sample[\"review\"])[: input_size()]\n",
    "        sample[\"query\"] = tokenizer.decode(sample[\"input_ids\"])\n",
    "        return sample\n",
    "\n",
    "    ds = ds.map(tokenize, batched=False)\n",
    "    ds.set_format(type=\"torch\")\n",
    "    return ds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We retrieve the dataloader by calling the `build_dataset` function.\n",
    "dataset = build_dataset(ppo_config, ppo_config.query_dataset)\n",
    "\n",
    "\n",
    "def collator(data):\n",
    "    return dict((key, [d[key] for d in data]) for key in data[0])\n",
    "\n",
    "# set seed before initializing value head for deterministic eval\n",
    "set_seed(ppo_config.seed)\n",
    "\n",
    "# Now let's build the model, the reference model, and the tokenizer.\n",
    "if not use_peft:\n",
    "    ref_model = trl_model_class.from_pretrained(ppo_config.model_name, trust_remote_code=trust_remote_code)\n",
    "    device_map = None\n",
    "    peft_config = None\n",
    "else:\n",
    "    peft_config = peft_config\n",
    "    ref_model = None\n",
    "    # Copy the model to each device\n",
    "    device_map = {\"\": Accelerator().local_process_index}"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model & Tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = trl_model_class.from_pretrained(\n",
    "    ppo_config.model_name,\n",
    "    trust_remote_code=trust_remote_code,\n",
    "    device_map=device_map,\n",
    "    peft_config=peft_config,\n",
    ")\n",
    "\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(ppo_config.model_name)\n",
    "# Some tokenizers like GPT-2's don't have a padding token by default, so we set one here.\n",
    "tokenizer.pad_token_id = tokenizer.eos_token_id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We then build the PPOTrainer, passing the model, the reference model, the tokenizer\n",
    "ppo_trainer = PPOTrainer(ppo_config, model, ref_model, tokenizer, dataset=dataset, data_collator=collator)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We then build the sentiment analysis pipeline, passing the model name and the\n",
    "# sentiment analysis pipeline arguments. Let's also make sure to set the device\n",
    "# to the same device as the PPOTrainer.\n",
    "device = ppo_trainer.accelerator.device\n",
    "if ppo_trainer.accelerator.num_processes == 1:\n",
    "    if is_xpu_available():\n",
    "        device = \"xpu:0\"\n",
    "    else:\n",
    "        device = 0 if torch.cuda.is_available() else \"cpu\"  # to avoid a `pipeline` bug\n",
    "ds_plugin = ppo_trainer.accelerator.state.deepspeed_plugin\n",
    "task, model_name = ppo_config.reward_model.split(\":\")\n",
    "if ds_plugin is not None and ds_plugin.is_zero3_init_enabled():\n",
    "    with ds_plugin.zero3_init_context_manager(enable=False):\n",
    "        sentiment_pipe = pipeline(task, model=model_name, device=device)\n",
    "else:\n",
    "    sentiment_pipe = pipeline(task, model=model_name, device=device)\n",
    "\n",
    "# Some tokenizers like GPT-2's don't have a padding token by default, so we set one here.\n",
    "if sentiment_pipe.tokenizer.pad_token_id is None:\n",
    "    sentiment_pipe.tokenizer.pad_token_id = tokenizer.pad_token_id\n",
    "\n",
    "if sentiment_pipe.model.config.pad_token_id is None:\n",
    "    sentiment_pipe.model.config.pad_token_id = tokenizer.pad_token_id"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We then define the arguments to pass to the `generate` function. These arguments\n",
    "# are passed to the `generate` function of the PPOTrainer, which is a wrapper around\n",
    "# the `generate` function of the trained model.\n",
    "generation_kwargs = {\n",
    "    \"min_length\": -1,\n",
    "    \"top_k\": 0.0,\n",
    "    \"top_p\": 1.0,\n",
    "    \"do_sample\": True,\n",
    "    \"pad_token_id\": tokenizer.eos_token_id,\n",
    "    \"max_new_tokens\": 32,\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for epoch, batch in tqdm(enumerate(ppo_trainer.dataloader)):\n",
    "    query_tensors = batch[\"input_ids\"]\n",
    "\n",
    "    # Get response from gpt2\n",
    "    response_tensors, ref_response_tensors = ppo_trainer.generate(\n",
    "        query_tensors, return_prompt=False, generate_ref_response=True, **generation_kwargs\n",
    "    )\n",
    "    batch[\"response\"] = tokenizer.batch_decode(response_tensors)\n",
    "    batch[\"ref_response\"] = tokenizer.batch_decode(ref_response_tensors)\n",
    "\n",
    "    # Compute sentiment score\n",
    "    texts = [q + r for q, r in zip(batch[\"query\"], batch[\"response\"])]\n",
    "    pipe_outputs = sentiment_pipe(texts, **sent_kwargs)\n",
    "    rewards = [torch.tensor(output[1][\"score\"]) for output in pipe_outputs]\n",
    "    ref_texts = [q + r for q, r in zip(batch[\"query\"], batch[\"ref_response\"])]\n",
    "    ref_pipe_outputs = sentiment_pipe(ref_texts, **sent_kwargs)\n",
    "    ref_rewards = [torch.tensor(output[1][\"score\"]) for output in ref_pipe_outputs]\n",
    "    batch[\"ref_rewards\"] = ref_rewards\n",
    "\n",
    "    # Run PPO step\n",
    "    stats = ppo_trainer.step(query_tensors, response_tensors, rewards)\n",
    "    ppo_trainer.log_stats(stats, batch, rewards, columns_to_log=[\"query\", \"response\", \"ref_response\", \"ref_rewards\"])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.11.6"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
