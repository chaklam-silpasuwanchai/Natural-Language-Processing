{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# [BEIT: BERT Pre-Training of Image Transformers](https://arxiv.org/pdf/2106.08254.pdf)\n",
    "\n",
    "### Reference Code\n",
    "- https://github.com/microsoft/unilm/tree/master/beit\n",
    "\n",
    "<img src=\"./figures/beit_architecture.png\" title=\"BEIT\" />"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device cuda:1\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import numpy as np\n",
    "import time\n",
    "import math\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.nn import CrossEntropyLoss\n",
    "from torch.optim import Adam\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "from torchvision.datasets.mnist import MNIST\n",
    "from torchvision.transforms import ToTensor\n",
    "\n",
    "from tqdm.auto import tqdm\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "np.random.seed(0)\n",
    "torch.manual_seed(0)\n",
    "\n",
    "device = torch.device(\"cuda:1\" if torch.cuda.is_available() else \"cpu\")\n",
    "print('Using device', device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# --------------------------------------------------------\n",
    "# BEIT: BERT Pre-Training of Image Transformers (https://arxiv.org/abs/2106.08254)\n",
    "# Github source: https://github.com/microsoft/unilm/tree/master/beit\n",
    "# Copyright (c) 2021 Microsoft\n",
    "# Licensed under The MIT License [see LICENSE for details]\n",
    "# By Hangbo Bao\n",
    "# Based on timm, mmseg, setr, xcit and swin code bases\n",
    "# https://github.com/rwightman/pytorch-image-models/tree/master/timm\n",
    "# https://github.com/fudan-zvg/SETR\n",
    "# https://github.com/facebookresearch/xcit/\n",
    "# https://github.com/microsoft/Swin-Transformer\n",
    "# --------------------------------------------------------'\n",
    "import math\n",
    "import torch\n",
    "from functools import partial\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.utils.checkpoint as checkpoint\n",
    "\n",
    "from timm.models.layers import drop_path, to_2tuple, trunc_normal_\n",
    "\n",
    "# from mmcv_custom import load_checkpoint\n",
    "# from mmseg.utils import get_root_logger\n",
    "# from mmseg.models.builder import BACKBONES"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dataset and Dataloder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading http://yann.lecun.com/exdb/mnist/train-images-idx3-ubyte.gz\n",
      "Using downloaded and verified file: ./data/MNIST/raw/train-images-idx3-ubyte.gz\n",
      "Extracting ./data/MNIST/raw/train-images-idx3-ubyte.gz to ./data/MNIST/raw\n",
      "\n",
      "Downloading http://yann.lecun.com/exdb/mnist/train-labels-idx1-ubyte.gz\n",
      "Downloading http://yann.lecun.com/exdb/mnist/train-labels-idx1-ubyte.gz to ./data/MNIST/raw/train-labels-idx1-ubyte.gz\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 28881/28881 [00:00<00:00, 500618.64it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting ./data/MNIST/raw/train-labels-idx1-ubyte.gz to ./data/MNIST/raw\n",
      "\n",
      "Downloading http://yann.lecun.com/exdb/mnist/t10k-images-idx3-ubyte.gz\n",
      "Downloading http://yann.lecun.com/exdb/mnist/t10k-images-idx3-ubyte.gz to ./data/MNIST/raw/t10k-images-idx3-ubyte.gz\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1648877/1648877 [00:00<00:00, 4625086.20it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting ./data/MNIST/raw/t10k-images-idx3-ubyte.gz to ./data/MNIST/raw\n",
      "\n",
      "Downloading http://yann.lecun.com/exdb/mnist/t10k-labels-idx1-ubyte.gz\n",
      "Downloading http://yann.lecun.com/exdb/mnist/t10k-labels-idx1-ubyte.gz to ./data/MNIST/raw/t10k-labels-idx1-ubyte.gz\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 4542/4542 [00:00<00:00, 66378149.02it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting ./data/MNIST/raw/t10k-labels-idx1-ubyte.gz to ./data/MNIST/raw\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from torch.utils.data import random_split\n",
    "from torchvision.transforms import ToTensor, Grayscale, Lambda\n",
    "from torchvision import transforms\n",
    "\n",
    "# Loading data\n",
    "# transform = ToTensor()\n",
    "\n",
    "transform = transforms.Compose([\n",
    "    Grayscale(num_output_channels=3),  # Convert to 3 channels\n",
    "    ToTensor(),\n",
    "])\n",
    "\n",
    "full_dataset = MNIST(root='./data/', train=True, download=True, transform=transform)\n",
    "# Define the sizes for the training and validation sets\n",
    "train_size = int(0.8 * len(full_dataset))  # 80% for training\n",
    "val_size = len(full_dataset) - train_size   # 20% for validation\n",
    "\n",
    "# Split the dataset into training and validation sets\n",
    "train_set, val_set = random_split(full_dataset, [train_size, val_size])\n",
    "test_set = MNIST(root='./data/', train=False, download=True, transform=transform)\n",
    "\n",
    "train_loader = DataLoader(train_set, shuffle=True, batch_size=16)\n",
    "val_loader = DataLoader(val_set, shuffle=False, batch_size=16)\n",
    "test_loader = DataLoader(test_set, shuffle=False, batch_size=16)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(3000, 750, 625)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(train_loader), len(val_loader), len(test_loader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([16, 3, 28, 28])\n",
      "torch.Size([16])\n",
      "torch.Size([28, 28, 3])\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "8"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 200x200 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "for images, labels in train_loader:  #taking 64 images, corresponding 64 labels\n",
    "    break\n",
    "\n",
    "print(images.shape) #(64 images, 1 channel, height, width)\n",
    "print(labels.shape) #(class for the 64 images)\n",
    "\n",
    "the_image = images[0]\n",
    "the_image = np.transpose(the_image, (1, 2, 0))\n",
    "print(the_image.shape)\n",
    "plt.figure(figsize=(2, 2))\n",
    "plt.imshow(the_image) #(h, w, c)\n",
    "\n",
    "labels[0].item()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## BEiT Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DropPath(nn.Module):\n",
    "    \"\"\"Drop paths (Stochastic Depth) per sample  (when applied in main path of residual blocks).\n",
    "    \"\"\"\n",
    "    def __init__(self, drop_prob=None):\n",
    "        super(DropPath, self).__init__()\n",
    "        self.drop_prob = drop_prob\n",
    "\n",
    "    def forward(self, x):\n",
    "        return drop_path(x, self.drop_prob, self.training)\n",
    "    \n",
    "    def extra_repr(self) -> str:\n",
    "        return 'p={}'.format(self.drop_prob)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Mlp(nn.Module):\n",
    "    def __init__(self, in_features, hidden_features=None, out_features=None, act_layer=nn.GELU, drop=0.):\n",
    "        super().__init__()\n",
    "        out_features = out_features or in_features\n",
    "        hidden_features = hidden_features or in_features\n",
    "        self.fc1 = nn.Linear(in_features, hidden_features)\n",
    "        self.act = act_layer()\n",
    "        self.fc2 = nn.Linear(hidden_features, out_features)\n",
    "        self.drop = nn.Dropout(drop)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.fc1(x)\n",
    "        x = self.act(x)\n",
    "        # x = self.drop(x)\n",
    "        # commit this for the orignal BERT implement \n",
    "        x = self.fc2(x)\n",
    "        x = self.drop(x)\n",
    "        return x\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Attention(nn.Module):\n",
    "    def __init__(\n",
    "            self, dim, num_heads=8, qkv_bias=False, qk_scale=None, attn_drop=0.,\n",
    "            proj_drop=0., window_size=None, attn_head_dim=None):\n",
    "        super().__init__()\n",
    "        self.num_heads = num_heads\n",
    "        head_dim = dim // num_heads\n",
    "        if attn_head_dim is not None:\n",
    "            head_dim = attn_head_dim\n",
    "        all_head_dim = head_dim * self.num_heads\n",
    "        # NOTE scale factor was wrong in my original version, can set manually to be compat with prev weights\n",
    "        self.scale = qk_scale or head_dim ** -0.5\n",
    "\n",
    "        self.qkv = nn.Linear(dim, all_head_dim * 3, bias=False)\n",
    "        if qkv_bias:\n",
    "            self.q_bias = nn.Parameter(torch.zeros(all_head_dim))\n",
    "            self.v_bias = nn.Parameter(torch.zeros(all_head_dim))\n",
    "        else:\n",
    "            self.q_bias = None\n",
    "            self.v_bias = None\n",
    "\n",
    "        if window_size:\n",
    "            self.window_size = window_size\n",
    "            self.num_relative_distance = (2 * window_size[0] - 1) * (2 * window_size[1] - 1) + 3\n",
    "            self.relative_position_bias_table = nn.Parameter(\n",
    "                torch.zeros(self.num_relative_distance, num_heads))  # 2*Wh-1 * 2*Ww-1, nH\n",
    "            # cls to token & token 2 cls & cls to cls\n",
    "\n",
    "            # get pair-wise relative position index for each token inside the window\n",
    "            coords_h = torch.arange(window_size[0])\n",
    "            coords_w = torch.arange(window_size[1])\n",
    "            coords = torch.stack(torch.meshgrid([coords_h, coords_w]))  # 2, Wh, Ww\n",
    "            coords_flatten = torch.flatten(coords, 1)  # 2, Wh*Ww\n",
    "            relative_coords = coords_flatten[:, :, None] - coords_flatten[:, None, :]  # 2, Wh*Ww, Wh*Ww\n",
    "            relative_coords = relative_coords.permute(1, 2, 0).contiguous()  # Wh*Ww, Wh*Ww, 2\n",
    "            relative_coords[:, :, 0] += window_size[0] - 1  # shift to start from 0\n",
    "            relative_coords[:, :, 1] += window_size[1] - 1\n",
    "            relative_coords[:, :, 0] *= 2 * window_size[1] - 1\n",
    "            relative_position_index = \\\n",
    "                torch.zeros(size=(window_size[0] * window_size[1] + 1, ) * 2, dtype=relative_coords.dtype)\n",
    "            relative_position_index[1:, 1:] = relative_coords.sum(-1)  # Wh*Ww, Wh*Ww\n",
    "            relative_position_index[0, 0:] = self.num_relative_distance - 3\n",
    "            relative_position_index[0:, 0] = self.num_relative_distance - 2\n",
    "            relative_position_index[0, 0] = self.num_relative_distance - 1\n",
    "\n",
    "            self.register_buffer(\"relative_position_index\", relative_position_index)\n",
    "\n",
    "            # trunc_normal_(self.relative_position_bias_table, std=.0)\n",
    "        else:\n",
    "            self.window_size = None\n",
    "            self.relative_position_bias_table = None\n",
    "            self.relative_position_index = None\n",
    "\n",
    "        self.attn_drop = nn.Dropout(attn_drop)\n",
    "        self.proj = nn.Linear(all_head_dim, dim)\n",
    "        self.proj_drop = nn.Dropout(proj_drop)\n",
    "\n",
    "    def forward(self, x, rel_pos_bias=None):\n",
    "        B, N, C = x.shape\n",
    "        qkv_bias = None\n",
    "        if self.q_bias is not None:\n",
    "            qkv_bias = torch.cat((self.q_bias, torch.zeros_like(self.v_bias, requires_grad=False), self.v_bias))\n",
    "        # qkv = self.qkv(x).reshape(B, N, 3, self.num_heads, C // self.num_heads).permute(2, 0, 3, 1, 4)\n",
    "        qkv = F.linear(input=x, weight=self.qkv.weight, bias=qkv_bias)\n",
    "        qkv = qkv.reshape(B, N, 3, self.num_heads, -1).permute(2, 0, 3, 1, 4)\n",
    "        q, k, v = qkv[0], qkv[1], qkv[2]   # make torchscript happy (cannot use tensor as tuple)\n",
    "\n",
    "        q = q * self.scale\n",
    "        attn = (q @ k.transpose(-2, -1))\n",
    "\n",
    "        if self.relative_position_bias_table is not None:\n",
    "            relative_position_bias = \\\n",
    "                self.relative_position_bias_table[self.relative_position_index.view(-1)].view(\n",
    "                    self.window_size[0] * self.window_size[1] + 1,\n",
    "                    self.window_size[0] * self.window_size[1] + 1, -1)  # Wh*Ww,Wh*Ww,nH\n",
    "            relative_position_bias = relative_position_bias.permute(2, 0, 1).contiguous()  # nH, Wh*Ww, Wh*Ww\n",
    "            attn = attn + relative_position_bias.unsqueeze(0)\n",
    "\n",
    "        if rel_pos_bias is not None:\n",
    "            attn = attn + rel_pos_bias\n",
    "        \n",
    "        attn = attn.softmax(dim=-1)\n",
    "        attn = self.attn_drop(attn)\n",
    "\n",
    "        x = (attn @ v).transpose(1, 2).reshape(B, N, -1)\n",
    "        x = self.proj(x)\n",
    "        x = self.proj_drop(x)\n",
    "        return x\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Block(nn.Module):\n",
    "\n",
    "    def __init__(self, dim, num_heads, mlp_ratio=4., qkv_bias=False, qk_scale=None, drop=0., attn_drop=0.,\n",
    "                 drop_path=0., init_values=None, act_layer=nn.GELU, norm_layer=nn.LayerNorm,\n",
    "                 window_size=None, attn_head_dim=None):\n",
    "        super().__init__()\n",
    "        self.norm1 = norm_layer(dim)\n",
    "        self.attn = Attention(\n",
    "            dim, num_heads=num_heads, qkv_bias=qkv_bias, qk_scale=qk_scale,\n",
    "            attn_drop=attn_drop, proj_drop=drop, window_size=window_size, attn_head_dim=attn_head_dim)\n",
    "        # NOTE: drop path for stochastic depth, we shall see if this is better than dropout here\n",
    "        self.drop_path = DropPath(drop_path) if drop_path > 0. else nn.Identity()\n",
    "        self.norm2 = norm_layer(dim)\n",
    "        mlp_hidden_dim = int(dim * mlp_ratio)\n",
    "        self.mlp = Mlp(in_features=dim, hidden_features=mlp_hidden_dim, act_layer=act_layer, drop=drop)\n",
    "\n",
    "        if init_values is not None:\n",
    "            self.gamma_1 = nn.Parameter(init_values * torch.ones((dim)),requires_grad=True)\n",
    "            self.gamma_2 = nn.Parameter(init_values * torch.ones((dim)),requires_grad=True)\n",
    "        else:\n",
    "            self.gamma_1, self.gamma_2 = None, None\n",
    "\n",
    "    def forward(self, x, rel_pos_bias=None):\n",
    "        if self.gamma_1 is None:\n",
    "            x = x + self.drop_path(self.attn(self.norm1(x), rel_pos_bias=rel_pos_bias))\n",
    "            x = x + self.drop_path(self.mlp(self.norm2(x)))\n",
    "        else:\n",
    "            x = x + self.drop_path(self.gamma_1 * self.attn(self.norm1(x), rel_pos_bias=rel_pos_bias))\n",
    "            x = x + self.drop_path(self.gamma_2 * self.mlp(self.norm2(x)))\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PatchEmbed(nn.Module):\n",
    "    \"\"\" Image to Patch Embedding\n",
    "    \"\"\"\n",
    "    def __init__(self, img_size=224, patch_size=16, in_chans=3, embed_dim=768):\n",
    "        super().__init__()\n",
    "        img_size = to_2tuple(img_size)\n",
    "        patch_size = to_2tuple(patch_size)\n",
    "        num_patches = (img_size[1] // patch_size[1]) * (img_size[0] // patch_size[0])\n",
    "        self.patch_shape = (img_size[0] // patch_size[0], img_size[1] // patch_size[1])\n",
    "        self.img_size = img_size\n",
    "        self.patch_size = patch_size\n",
    "        self.num_patches = num_patches\n",
    "\n",
    "        self.proj = nn.Conv2d(in_chans, embed_dim, kernel_size=patch_size, stride=patch_size)\n",
    "\n",
    "    def forward(self, x, **kwargs):\n",
    "        B, C, H, W = x.shape\n",
    "        # FIXME look at relaxing size constraints\n",
    "        # assert H == self.img_size[0] and W == self.img_size[1], \\\n",
    "        #     f\"Input image size ({H}*{W}) doesn't match model ({self.img_size[0]}*{self.img_size[1]}).\"\n",
    "        x = self.proj(x)\n",
    "        Hp, Wp = x.shape[2], x.shape[3]\n",
    "\n",
    "        x = x.flatten(2).transpose(1, 2)\n",
    "        return x, (Hp, Wp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([16, 1, 768]), 1, 1)"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "img_size = 224 \n",
    "patch_size = 28\n",
    "in_chans = 3 \n",
    "embed_dim = 768\n",
    "\n",
    "patch_embed = PatchEmbed(\n",
    "    img_size=img_size, patch_size=patch_size, in_chans=in_chans, embed_dim=embed_dim)\n",
    "\n",
    "x, (Hp, Wp) = patch_embed(images)\n",
    "x.shape, Hp, Wp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "class HybridEmbed(nn.Module):\n",
    "    \"\"\" CNN Feature Map Embedding\n",
    "    Extract feature map from CNN, flatten, project to embedding dim.\n",
    "    \"\"\"\n",
    "    def __init__(self, backbone, img_size=224, feature_size=None, in_chans=3, embed_dim=768):\n",
    "        super().__init__()\n",
    "        assert isinstance(backbone, nn.Module)\n",
    "        img_size = to_2tuple(img_size)\n",
    "        self.img_size = img_size\n",
    "        self.backbone = backbone\n",
    "        if feature_size is None:\n",
    "            with torch.no_grad():\n",
    "                # FIXME this is hacky, but most reliable way of determining the exact dim of the output feature\n",
    "                # map for all networks, the feature metadata has reliable channel and stride info, but using\n",
    "                # stride to calc feature dim requires info about padding of each stage that isn't captured.\n",
    "                training = backbone.training\n",
    "                if training:\n",
    "                    backbone.eval()\n",
    "                o = self.backbone(torch.zeros(1, in_chans, img_size[0], img_size[1]))[-1]\n",
    "                feature_size = o.shape[-2:]\n",
    "                feature_dim = o.shape[1]\n",
    "                backbone.train(training)\n",
    "        else:\n",
    "            feature_size = to_2tuple(feature_size)\n",
    "            feature_dim = self.backbone.feature_info.channels()[-1]\n",
    "        self.num_patches = feature_size[0] * feature_size[1]\n",
    "        self.proj = nn.Linear(feature_dim, embed_dim)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.backbone(x)[-1]\n",
    "        x = x.flatten(2).transpose(1, 2)\n",
    "        x = self.proj(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "class RelativePositionBias(nn.Module):\n",
    "\n",
    "    def __init__(self, window_size, num_heads):\n",
    "        super().__init__()\n",
    "        self.window_size = window_size\n",
    "        self.num_relative_distance = (2 * window_size[0] - 1) * (2 * window_size[1] - 1) + 3\n",
    "        self.relative_position_bias_table = nn.Parameter(\n",
    "            torch.zeros(self.num_relative_distance, num_heads))  # 2*Wh-1 * 2*Ww-1, nH\n",
    "        # cls to token & token 2 cls & cls to cls\n",
    "\n",
    "        # get pair-wise relative position index for each token inside the window\n",
    "        coords_h = torch.arange(window_size[0])\n",
    "        coords_w = torch.arange(window_size[1])\n",
    "        coords = torch.stack(torch.meshgrid([coords_h, coords_w]))  # 2, Wh, Ww\n",
    "        coords_flatten = torch.flatten(coords, 1)  # 2, Wh*Ww\n",
    "        relative_coords = coords_flatten[:, :, None] - coords_flatten[:, None, :]  # 2, Wh*Ww, Wh*Ww\n",
    "        relative_coords = relative_coords.permute(1, 2, 0).contiguous()  # Wh*Ww, Wh*Ww, 2\n",
    "        relative_coords[:, :, 0] += window_size[0] - 1  # shift to start from 0\n",
    "        relative_coords[:, :, 1] += window_size[1] - 1\n",
    "        relative_coords[:, :, 0] *= 2 * window_size[1] - 1\n",
    "        relative_position_index = \\\n",
    "            torch.zeros(size=(window_size[0] * window_size[1] + 1,) * 2, dtype=relative_coords.dtype)\n",
    "        relative_position_index[1:, 1:] = relative_coords.sum(-1)  # Wh*Ww, Wh*Ww\n",
    "        relative_position_index[0, 0:] = self.num_relative_distance - 3\n",
    "        relative_position_index[0:, 0] = self.num_relative_distance - 2\n",
    "        relative_position_index[0, 0] = self.num_relative_distance - 1\n",
    "\n",
    "        self.register_buffer(\"relative_position_index\", relative_position_index)\n",
    "\n",
    "        # trunc_normal_(self.relative_position_bias_table, std=.02)\n",
    "\n",
    "    def forward(self):\n",
    "        relative_position_bias = \\\n",
    "            self.relative_position_bias_table[self.relative_position_index.view(-1)].view(\n",
    "                self.window_size[0] * self.window_size[1] + 1,\n",
    "                self.window_size[0] * self.window_size[1] + 1, -1)  # Wh*Ww,Wh*Ww,nH\n",
    "        return relative_position_bias.permute(2, 0, 1).contiguous()  # nH, Wh*Ww, Wh*Ww\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BEiT(nn.Module):\n",
    "    \"\"\" \n",
    "    Vision Transformer with support for patch or hybrid CNN input stage\n",
    "    \"\"\"\n",
    "    def __init__(\n",
    "        self, \n",
    "        img_size=224, \n",
    "        patch_size=16, \n",
    "        in_chans=3, \n",
    "        num_classes=1000, \n",
    "        embed_dim=768, \n",
    "        depth=12,\n",
    "        num_heads=12, \n",
    "        mlp_ratio=4., \n",
    "        qkv_bias=False, \n",
    "        qk_scale=None, \n",
    "        drop_rate=0., \n",
    "        attn_drop_rate=0.,\n",
    "        drop_path_rate=0., \n",
    "        norm_layer=nn.LayerNorm, \n",
    "        init_values=None,\n",
    "        use_abs_pos_emb=False, \n",
    "        use_rel_pos_bias=False, \n",
    "        use_shared_rel_pos_bias=False,\n",
    "        use_mean_pooling=True, \n",
    "        init_scale=0.001\n",
    "    ):\n",
    "        super().__init__()\n",
    "        self.num_classes = num_classes\n",
    "        self.num_features = self.embed_dim = embed_dim  # num_features for consistency with other models\n",
    "\n",
    "        self.patch_embed = PatchEmbed(\n",
    "            img_size=img_size, patch_size=patch_size, in_chans=in_chans, embed_dim=embed_dim)\n",
    "        num_patches = self.patch_embed.num_patches\n",
    "\n",
    "        self.cls_token = nn.Parameter(torch.zeros(1, 1, embed_dim))\n",
    "        # self.mask_token = nn.Parameter(torch.zeros(1, 1, embed_dim))\n",
    "        if use_abs_pos_emb:\n",
    "            self.pos_embed = nn.Parameter(torch.zeros(1, num_patches + 1, embed_dim))\n",
    "        else:\n",
    "            self.pos_embed = None\n",
    "        self.pos_drop = nn.Dropout(p=drop_rate)\n",
    "\n",
    "        if use_shared_rel_pos_bias:\n",
    "            self.rel_pos_bias = RelativePositionBias(window_size=self.patch_embed.patch_shape, num_heads=num_heads)\n",
    "        else:\n",
    "            self.rel_pos_bias = None\n",
    "\n",
    "        dpr = [x.item() for x in torch.linspace(0, drop_path_rate, depth)]  # stochastic depth decay rule\n",
    "        self.use_rel_pos_bias = use_rel_pos_bias\n",
    "        self.blocks = nn.ModuleList([\n",
    "            Block(\n",
    "                dim=embed_dim, num_heads=num_heads, mlp_ratio=mlp_ratio, qkv_bias=qkv_bias, qk_scale=qk_scale,\n",
    "                drop=drop_rate, attn_drop=attn_drop_rate, drop_path=dpr[i], norm_layer=norm_layer,\n",
    "                init_values=init_values, window_size=self.patch_embed.patch_shape if use_rel_pos_bias else None)\n",
    "            for i in range(depth)])\n",
    "        self.norm = nn.Identity() if use_mean_pooling else norm_layer(embed_dim)\n",
    "        self.fc_norm = norm_layer(embed_dim) if use_mean_pooling else None\n",
    "        self.head = nn.Linear(embed_dim, num_classes) if num_classes > 0 else nn.Identity()\n",
    "\n",
    "        if self.pos_embed is not None:\n",
    "            trunc_normal_(self.pos_embed, std=.02)\n",
    "        trunc_normal_(self.cls_token, std=.02)\n",
    "        # trunc_normal_(self.mask_token, std=.02)\n",
    "        if isinstance(self.head, nn.Linear):\n",
    "            trunc_normal_(self.head.weight, std=.02)\n",
    "        self.apply(self._init_weights)\n",
    "        self.fix_init_weight()\n",
    "\n",
    "        if isinstance(self.head, nn.Linear):\n",
    "            self.head.weight.data.mul_(init_scale)\n",
    "            self.head.bias.data.mul_(init_scale)\n",
    "\n",
    "    def fix_init_weight(self):\n",
    "        def rescale(param, layer_id):\n",
    "            param.div_(math.sqrt(2.0 * layer_id))\n",
    "\n",
    "        for layer_id, layer in enumerate(self.blocks):\n",
    "            rescale(layer.attn.proj.weight.data, layer_id + 1)\n",
    "            rescale(layer.mlp.fc2.weight.data, layer_id + 1)\n",
    "\n",
    "    def _init_weights(self, m):\n",
    "        if isinstance(m, nn.Linear):\n",
    "            trunc_normal_(m.weight, std=.02)\n",
    "            if isinstance(m, nn.Linear) and m.bias is not None:\n",
    "                nn.init.constant_(m.bias, 0)\n",
    "        elif isinstance(m, nn.LayerNorm):\n",
    "            nn.init.constant_(m.bias, 0)\n",
    "            nn.init.constant_(m.weight, 1.0)\n",
    "\n",
    "    def get_num_layers(self):\n",
    "        return len(self.blocks)\n",
    "\n",
    "    @torch.jit.ignore\n",
    "    def no_weight_decay(self):\n",
    "        return {'pos_embed', 'cls_token'}\n",
    "\n",
    "    def get_classifier(self):\n",
    "        return self.head\n",
    "\n",
    "    def reset_classifier(self, num_classes, global_pool=''):\n",
    "        self.num_classes = num_classes\n",
    "        self.head = nn.Linear(self.embed_dim, num_classes) if num_classes > 0 else nn.Identity()\n",
    "\n",
    "    def forward_features(self, x):\n",
    "        x, (Hp, Wp) = self.patch_embed(x)\n",
    "        batch_size, seq_len, _ = x.size()\n",
    "\n",
    "        cls_tokens = self.cls_token.expand(batch_size, -1, -1)  # stole cls_tokens impl from Phil Wang, thanks\n",
    "        x = torch.cat((cls_tokens, x), dim=1)\n",
    "        if self.pos_embed is not None:\n",
    "            x = x + self.pos_embed\n",
    "        x = self.pos_drop(x)\n",
    "\n",
    "        rel_pos_bias = self.rel_pos_bias() if self.rel_pos_bias is not None else None\n",
    "        for blk in self.blocks:\n",
    "            x = blk(x, rel_pos_bias=rel_pos_bias)\n",
    "\n",
    "        x = self.norm(x)\n",
    "        if self.fc_norm is not None:\n",
    "            t = x[:, 1:, :]\n",
    "            return self.fc_norm(t.mean(1))\n",
    "        else:\n",
    "            return x[:, 0]\n",
    "        \n",
    "    def forward(self, x):\n",
    "        x = self.forward_features(x)\n",
    "        x = self.head(x)\n",
    "        return x\n",
    "\n",
    "    def get_intermediate_layers(self, x):\n",
    "        x, (Hp, Wp) = self.patch_embed(x)\n",
    "        batch_size, seq_len, _ = x.size()\n",
    "\n",
    "        cls_tokens = self.cls_token.expand(batch_size, -1, -1)  # stole cls_tokens impl from Phil Wang, thanks\n",
    "        x = torch.cat((cls_tokens, x), dim=1)\n",
    "        if self.pos_embed is not None:\n",
    "            x = x + self.pos_embed\n",
    "        x = self.pos_drop(x)\n",
    "\n",
    "        features = []\n",
    "        rel_pos_bias = self.rel_pos_bias() if self.rel_pos_bias is not None else None\n",
    "        for blk in self.blocks:\n",
    "            x = blk(x, rel_pos_bias)\n",
    "            features.append(x)\n",
    "\n",
    "        return features\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "BEiT(\n",
       "  (patch_embed): PatchEmbed(\n",
       "    (proj): Conv2d(3, 768, kernel_size=(16, 16), stride=(16, 16))\n",
       "  )\n",
       "  (pos_drop): Dropout(p=0.0, inplace=False)\n",
       "  (blocks): ModuleList(\n",
       "    (0-11): 12 x Block(\n",
       "      (norm1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "      (attn): Attention(\n",
       "        (qkv): Linear(in_features=768, out_features=2304, bias=False)\n",
       "        (attn_drop): Dropout(p=0.0, inplace=False)\n",
       "        (proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (proj_drop): Dropout(p=0.0, inplace=False)\n",
       "      )\n",
       "      (drop_path): Identity()\n",
       "      (norm2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "      (mlp): Mlp(\n",
       "        (fc1): Linear(in_features=768, out_features=3072, bias=True)\n",
       "        (act): GELU(approximate='none')\n",
       "        (fc2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "        (drop): Dropout(p=0.0, inplace=False)\n",
       "      )\n",
       "    )\n",
       "  )\n",
       "  (norm): Identity()\n",
       "  (fc_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "  (head): Linear(in_features=768, out_features=10, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = BEiT(\n",
    "    img_size = 224, \n",
    "    patch_size = 16,\n",
    "    in_chans = 3,\n",
    "    embed_dim = 768,\n",
    "    num_classes = 10,\n",
    "    depth       = 12,\n",
    "    num_heads   = 12,\n",
    "    mlp_ratio   = 4\n",
    ").to(device)\n",
    "\n",
    "model"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.optim as optim\n",
    "\n",
    "num_epochs = 5\n",
    "lr = 0.01\n",
    "\n",
    "#training hyperparameters\n",
    "optimizer = optim.Adam(model.parameters(), lr=lr)\n",
    "criterion = CrossEntropyLoss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(model, train_loader, optimizer, criterion, device=\"cpu\"):\n",
    "    \n",
    "    model.train()\n",
    "    train_loss = 0.0\n",
    "    train_correct, total = 0, 0\n",
    "\n",
    "    for batch in tqdm(train_loader):\n",
    "        x, y = batch\n",
    "        x = x.to(device) #16, 3, 28, 28 \n",
    "        y = y.to(device) #16\n",
    "\n",
    "        y_hat = model(x)\n",
    "        loss = criterion(y_hat, y) / len(x)\n",
    "        train_loss += loss.item()\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        train_correct += torch.sum(torch.argmax(y_hat, dim=1) == y).item()\n",
    "        total += len(x)\n",
    "\n",
    "    return train_loss, train_correct/total\n",
    "\n",
    "def evaluate(model, loader, criterion):\n",
    "\n",
    "    model.eval()\n",
    "    test_loss = 0.0\n",
    "    test_correct, total = 0, 0\n",
    "    \n",
    "    for batch in tqdm(loader):\n",
    "        x, y = batch\n",
    "        x = x.to(device)\n",
    "        y = y.to(device)\n",
    "        \n",
    "        y_hat = model(x)\n",
    "        loss = criterion(y_hat, y) / len(x)\n",
    "        test_loss += loss.item()\n",
    "\n",
    "        test_correct += torch.sum(torch.argmax(y_hat, dim=1) == y).item()\n",
    "        total += len(x)\n",
    "\n",
    "    return test_loss, test_correct/total"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def epoch_time(start_time, end_time):\n",
    "    elapsed_time = end_time - start_time\n",
    "    elapsed_mins = int(elapsed_time / 60)\n",
    "    elapsed_secs = int(elapsed_time - (elapsed_mins * 60))\n",
    "    return elapsed_mins, elapsed_secs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9a71f1f6391e493d812232021a7fa1e5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/3000 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "dfeb99d9a9324863a59a0a137964d3dc",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/750 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 01 | Time: 1m 28s\n",
      "\tTrain Loss: 230.185 | Train Acc: 59.26%\n",
      "\t Val. Loss: 36.520 | Test  Acc: 74.02%\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b932c19805294ad88897b1bd56c3927f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/3000 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "611fb97b55ae4d18900febc7154dcec6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/750 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 02 | Time: 1m 43s\n",
      "\tTrain Loss: 134.680 | Train Acc: 75.91%\n",
      "\t Val. Loss: 29.218 | Test  Acc: 79.74%\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7ef564d23b504654acde6b9135f6ad8f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/3000 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2e5699ed3fb149198ee409dcd7ef03a4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/750 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 03 | Time: 1m 33s\n",
      "\tTrain Loss: 109.066 | Train Acc: 80.75%\n",
      "\t Val. Loss: 24.959 | Test  Acc: 82.90%\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4dad3cd2c507471698b42ee35bfccc52",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/3000 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1991f1033e16443e9f2b46cd42291912",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/750 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 04 | Time: 1m 22s\n",
      "\tTrain Loss: 101.547 | Train Acc: 82.10%\n",
      "\t Val. Loss: 22.062 | Test  Acc: 84.44%\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "28ee62321ae44719817932c0a086f515",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/3000 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "69b3f27cffdb4cb3b121a3863861025a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/750 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 05 | Time: 1m 38s\n",
      "\tTrain Loss: 92.482 | Train Acc: 83.59%\n",
      "\t Val. Loss: 24.367 | Test  Acc: 82.67%\n"
     ]
    }
   ],
   "source": [
    "best_valid_loss = float('inf')\n",
    "num_epochs = 5\n",
    "clip       = 1\n",
    "\n",
    "save_path = f'models/{model.__class__.__name__}.pt'\n",
    "\n",
    "train_losses = []\n",
    "valid_losses = []\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "\n",
    "    start_time = time.time()\n",
    "\n",
    "    train_loss, train_acc = train(model, train_loader, optimizer, criterion, device)\n",
    "    valid_loss, test_acc = evaluate(model, val_loader, criterion)\n",
    "\n",
    "    #for plotting\n",
    "    train_losses.append(train_loss)\n",
    "    valid_losses.append(valid_loss)\n",
    "    \n",
    "    end_time = time.time()\n",
    "    \n",
    "    epoch_mins, epoch_secs = epoch_time(start_time, end_time)\n",
    "    \n",
    "    if valid_loss < best_valid_loss:\n",
    "        best_valid_loss = valid_loss\n",
    "        # torch.save(model.state_dict(), save_path)\n",
    "    \n",
    "    print(f'Epoch: {epoch+1:02} | Time: {epoch_mins}m {epoch_secs}s')\n",
    "    print(f'\\tTrain Loss: {train_loss:.3f} | Train Acc: {train_acc*100:.2f}%')\n",
    "    print(f'\\t Val. Loss: {valid_loss:.3f} | Test  Acc: {test_acc * 100:.2f}%')\n",
    "    #lower perplexity is better"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Text(0, 0.5, 'loss')"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 500x300 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "fig = plt.figure(figsize=(5, 3))\n",
    "ax = fig.add_subplot(1, 1, 1)\n",
    "ax.plot(train_losses, label = 'train loss')\n",
    "ax.plot(valid_losses, label = 'valid loss')\n",
    "plt.legend()\n",
    "ax.set_xlabel('updates')\n",
    "ax.set_ylabel('loss')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a069cee57b3f49fb9f75ac1fbd00a62f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/625 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\t Test. Loss: 19.469 | Test Acc: 83.30%\n"
     ]
    }
   ],
   "source": [
    "# model.load_state_dict(torch.load(save_path))\n",
    "test_loss, test_acc = evaluate(model, test_loader, criterion)\n",
    "print(f'\\t Test. Loss: {test_loss:.3f} | Test Acc: {test_acc * 100:.2f}%')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
