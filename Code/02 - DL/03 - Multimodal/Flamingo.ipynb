{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# [Flamingo: a Visual Language Model for Few-Shot Learning](https://arxiv.org/pdf/2204.14198.pdf)\n",
    "\n",
    "Reference Code\n",
    "- https://github.com/lucidrains/flamingo-pytorch/tree/main\n",
    "\n",
    "\n",
    "<img src=\"./figures/flamingo.png\">\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install flamingo-pytorch"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## PerceiverResampler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from flamingo_pytorch import PerceiverResampler\n",
    "\n",
    "perceive = PerceiverResampler(\n",
    "    dim = 1024,\n",
    "    depth = 2,\n",
    "    dim_head = 64,\n",
    "    heads = 8,\n",
    "    num_latents = 64,    # the number of latents to shrink your media sequence to, perceiver style\n",
    "    num_time_embeds = 4  # say you have 4 images maximum in your dialogue\n",
    ")\n",
    "\n",
    "medias = torch.randn(1, 2, 256, 1024) # (batch, time, sequence length, dimension)\n",
    "perceived = perceive(medias) # (1, 2, 64, 1024) - (batch, time, num latents, dimension)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## GatedCrossAttentionBlock"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from flamingo_pytorch import GatedCrossAttentionBlock\n",
    "\n",
    "cross_attn = GatedCrossAttentionBlock(\n",
    "    dim = 1024,\n",
    "    dim_head = 64,\n",
    "    heads = 8\n",
    ")\n",
    "\n",
    "text = torch.randn(1, 512, 1024)\n",
    "perceived = torch.randn(1, 2, 64, 1024)\n",
    "\n",
    "media_locations = torch.randint(0, 2, (1, 512)).bool()\n",
    "\n",
    "text = cross_attn(\n",
    "    text,\n",
    "    perceived,\n",
    "    media_locations = media_locations\n",
    ")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Visual Encoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install vit-pytorch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from vit_pytorch.vit import ViT\n",
    "from vit_pytorch.extractor import Extractor\n",
    "\n",
    "vit = ViT(\n",
    "    image_size = 256,\n",
    "    patch_size = 32,\n",
    "    num_classes = 1000,\n",
    "    dim = 1024,\n",
    "    depth = 6,\n",
    "    heads = 16,\n",
    "    mlp_dim = 2048,\n",
    "    dropout = 0.1,\n",
    "    emb_dropout = 0.1\n",
    ")\n",
    "\n",
    "vit = Extractor(vit, return_embeddings_only = True)\n",
    "\n",
    "# first take your trained image encoder and wrap it in an adapter that returns the image embeddings\n",
    "# here we use the ViT from the vit-pytorch library\n",
    "\n",
    "import torch\n",
    "from flamingo_pytorch import FlamingoPaLM\n",
    "\n",
    "# a PaLM language model, the 540 billion parameter model from google that shows signs of general intelligence\n",
    "\n",
    "flamingo_palm = FlamingoPaLM(\n",
    "    num_tokens = 20000,          # number of tokens\n",
    "    dim = 1024,                  # dimensions\n",
    "    depth = 12,                  # depth\n",
    "    heads = 8,                   # attention heads\n",
    "    dim_head = 64,               # dimension per attention head\n",
    "    img_encoder = vit,           # plugin your image encoder (this can be optional if you pass in the image embeddings separately, but probably want to train end to end given the perceiver resampler)\n",
    "    media_token_id = 3,          # the token id representing the [media] or [image]\n",
    "    cross_attn_every = 3,        # how often to cross attend\n",
    "    perceiver_num_latents = 64,  # perceiver number of latents, should be smaller than the sequence length of the image tokens\n",
    "    perceiver_depth = 2          # perceiver resampler depth\n",
    ")\n",
    "\n",
    "# train your PaLM as usual\n",
    "\n",
    "text = torch.randint(0, 20000, (2, 512))\n",
    "\n",
    "palm_logits = flamingo_palm(text)\n",
    "\n",
    "# after much training off the regular PaLM logits\n",
    "# now you are ready to train Flamingo + PaLM\n",
    "# by passing in images, it automatically freezes everything but the perceiver and cross attention blocks, as in the paper\n",
    "\n",
    "dialogue = torch.randint(0, 20000, (4, 512))\n",
    "images = torch.randn(4, 2, 3, 256, 256)\n",
    "\n",
    "flamingo_logits = flamingo_palm(dialogue, images)\n",
    "\n",
    "# do your usual cross entropy loss"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
