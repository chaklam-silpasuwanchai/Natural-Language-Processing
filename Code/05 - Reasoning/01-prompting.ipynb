{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Prompting Techniques\n",
    "Prompt Engineering helps to effectively design and improve prompts to get better results on different tasks with LLMs.\n",
    "\n",
    "While the previous basic examples were fun, in this section we cover more advanced prompting engineering techniques that allow us to achieve more complex tasks and improve reliability and performance of LLMs.\n",
    "\n",
    "\n",
    "- [Prompting Techniques](#prompting-techniques)\n",
    "  - [Zero-Shot Prompting](#zero-shot-prompting)\n",
    "  - [Few-Shot Prompting](#few-shot-prompting)\n",
    "    - [Limitations of Few-shot Prompting](#limitations-of-few-shot-prompting)\n",
    "  - [Chain-of-Thought](#chain-of-thought)\n",
    "  - [Zero-Shot CoT](#zero-shot-cot)\n",
    "  - [Self-Consistency](#self-consistency)\n",
    "  - [Tree of Thought](#tree-of-thought-tot)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import openai\n",
    "import os\n",
    "\n",
    "api_key = os.getenv(\"OPENAI_API_KEY\")\n",
    "organization = os.getenv(\"OPEN_AI_ORG\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Zero-Shot Prompting\n",
    "LLMs today trained on large amounts of data and tuned to follow instructions, are capable of performing tasks zero-shot. We tried a few zero-shot examples in the previous section. Here is one of the examples we used:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*Prompt:*\n",
    "```\n",
    "Classify the text into neutral, negative, or positive. \n",
    "\n",
    "Text: I think the vacation is okay.\n",
    "Sentiment:\n",
    "```\n",
    "\n",
    "*Output:*\n",
    "```\n",
    "Neutral\n",
    "```\n",
    "\n",
    "Note that in the prompt above we didn't provide the model with any examples -- that's the zero-shot capabilities at work. When zero-shot doesn't work, it's recommended to provide demonstrations or examples in the prompt. Below we discuss the approach known as few-shot prompting.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Few-Shot Prompting\n",
    "\n",
    "While large-language models already demonstrate remarkable zero-shot capabilities, they still fall short on more complex tasks when using the zero-shot setting. To improve on this, few-shot prompting is used as a technique to enable in-context learning where we provide demonstrations in the prompt to steer the model to better performance. The demonstrations serve as conditioning for subsequent examples where we would like the model to generate a response. \n",
    "\n",
    "Let's demonstrate few-shot prompting via an example that was presented by [Brown et al. 2020](https://arxiv.org/abs/2005.14165). In the example, the task is to correctly use a new word in a sentence."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*Prompt:*\n",
    "```\n",
    "A \"whatpu\" is a small, furry animal native to Tanzania. An example of a sentence that uses\n",
    "the word whatpu is:\n",
    "We were traveling in Africa and we saw these very cute whatpus.\n",
    "To do a \"farduddle\" means to jump up and down really fast. An example of a sentence that uses\n",
    "the word farduddle is:\n",
    "```\n",
    "\n",
    "*Output:*\n",
    "```\n",
    "When we won the game, we all started to farduddle in celebration.\n",
    "```\n",
    "\n",
    "We can observe that the model has somehow learned how to perform the task by providing it with just one example (i.e., 1-shot). For more difficult tasks, we can experiment with increasing the demonstrations (e.g., 3-shot, 5-shot, 10-shot, etc.). \n",
    "\n",
    "Following the findings from [Min et al. (2022)](https://arxiv.org/abs/2202.12837), here are a few more tips about demonstrations/exemplars when doing few-shot:\n",
    "\n",
    "- \"the label space and the distribution of the input text specified by the demonstrations are both important (regardless of whether the labels are correct for individual inputs)\"\n",
    "- the format you use also plays a key role in performance, even if you just use random labels, this is much better than no labels at all.  \n",
    "- additional results show that selecting random labels from a true distribution of labels (instead of a uniform distribution) also helps.\n",
    "\n",
    "Let's try out a few examples. Let's first try an example with random labels (meaning the labels Negative and Positive are randomly assigned to the inputs):"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*Prompt:*\n",
    "```\n",
    "This is awesome! // Negative\n",
    "This is bad! // Positive\n",
    "Wow that movie was rad! // Positive\n",
    "What a horrible show! //\n",
    "```\n",
    "\n",
    "*Output:*\n",
    "```\n",
    "Negative\n",
    "```\n",
    "\n",
    "We still get the correct answer, even though the labels have been randomized. Note that we also kept the format, which helps too. In fact, with further experimentation, it seems the newer GPT models we are experimenting with are becoming more robust to even random formats. Example:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*Prompt:*\n",
    "```\n",
    "Positive This is awesome! \n",
    "This is bad! Negative\n",
    "Wow that movie was rad!\n",
    "Positive\n",
    "What a horrible show! --\n",
    "```\n",
    "\n",
    "*Output:*\n",
    "```\n",
    "Negative\n",
    "```\n",
    "\n",
    "There is no consistency in the format above but the model still predicted the correct label. We have to conduct a more thorough analysis to confirm if this holds for different and more complex tasks, including different variations of prompts."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Limitations of Few-shot Prompting\n",
    "\n",
    "Standard few-shot prompting works well for many tasks but is still not a perfect technique, especially when dealing with more complex reasoning tasks. Let's demonstrate why this is the case. Do you recall the previous example where we provided the following task:\n",
    "\n",
    "```\n",
    "The odd numbers in this group add up to an even number: 15, 32, 5, 13, 82, 7, 1. \n",
    "\n",
    "A: \n",
    "```\n",
    "\n",
    "If we try this again, the model outputs the following:\n",
    "\n",
    "```\n",
    "Yes, the odd numbers in this group add up to 107, which is an even number.\n",
    "```\n",
    "\n",
    "This is not the correct response, which not only highlights the limitations of these systems but that there is a need for more advanced prompt engineering. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's try to add some examples to see if few-shot prompting improves the results.\n",
    "\n",
    "*Prompt:*\n",
    "```\n",
    "The odd numbers in this group add up to an even number: 4, 8, 9, 15, 12, 2, 1.\n",
    "A: The answer is False.\n",
    "\n",
    "The odd numbers in this group add up to an even number: 17,  10, 19, 4, 8, 12, 24.\n",
    "A: The answer is True.\n",
    "\n",
    "The odd numbers in this group add up to an even number: 16,  11, 14, 4, 8, 13, 24.\n",
    "A: The answer is True.\n",
    "\n",
    "The odd numbers in this group add up to an even number: 17,  9, 10, 12, 13, 4, 2.\n",
    "A: The answer is False.\n",
    "\n",
    "The odd numbers in this group add up to an even number: 15, 32, 5, 13, 82, 7, 1. \n",
    "A: \n",
    "```\n",
    "\n",
    "*Output:*\n",
    "```\n",
    "The answer is True.\n",
    "```\n",
    "\n",
    "That didn't work. It seems like few-shot prompting is not enough to get reliable responses for this type of reasoning problem. The example above provides basic information on the task. If you take a closer look, the type of task we have introduced involves a few more reasoning steps. In other words, it might help if we break the problem down into steps and demonstrate that to the model. More recently, [chain-of-thought (CoT) prompting](https://arxiv.org/abs/2201.11903) has been popularized to address more complex arithmetic, commonsense, and symbolic reasoning tasks.\n",
    "\n",
    "Overall, it seems that providing examples is useful for solving some tasks. When zero-shot prompting and few-shot prompting are not sufficient, it might mean that whatever was learned by the model isn't enough to do well at the task. From here it is recommended to start thinking about fine-tuning your models or experimenting with more advanced prompting techniques. Up next we talk about one of the popular prompting techniques called chain-of-thought prompting which has gained a lot of popularity. "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Chain-of-Thought\n",
    "\n",
    "Introduced in [Wei et al. (2022)](https://arxiv.org/abs/2201.11903), chain-of-thought (CoT) prompting enables complex reasoning capabilities through intermediate reasoning steps. You can combine it with few-shot prompting to get better results on more complex tasks that require reasoning before responding.\n",
    "\n",
    "<img src=\"./figures/cot-prompting.png\" >"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Human: Roger has 5 tennis balls. He buys 2 more cans of tennis balls. Each can has 3 tennis balls. How many tennis balls does he have now?\n",
      "AI: Roger started with 5 balls. 2 cans of 3 tennis balls each is 6 tennis balls. 5 + 6 = 11. The answer is 11.\n"
     ]
    }
   ],
   "source": [
    "from langchain.prompts import (\n",
    "    ChatPromptTemplate,\n",
    "    FewShotChatMessagePromptTemplate,\n",
    ")\n",
    "\n",
    "examples = [\n",
    "    {\"input\": \"Roger has 5 tennis balls. He buys 2 more cans of tennis balls. Each can has 3 tennis balls. How many tennis balls does he have now?\", \n",
    "     \"output\": \"Roger started with 5 balls. 2 cans of 3 tennis balls each is 6 tennis balls. 5 + 6 = 11. The answer is 11.\"},\n",
    "]\n",
    "\n",
    "# This is a prompt template used to format each individual example.\n",
    "example_prompt = ChatPromptTemplate.from_messages(\n",
    "    [\n",
    "        (\"human\", \"{input}\"),\n",
    "        (\"ai\", \"{output}\"),\n",
    "    ]\n",
    ")\n",
    "few_shot_prompt = FewShotChatMessagePromptTemplate(\n",
    "    example_prompt=example_prompt,\n",
    "    examples=examples,\n",
    ")\n",
    "\n",
    "print(few_shot_prompt.format())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "final_prompt = ChatPromptTemplate.from_messages(\n",
    "    [\n",
    "        (\"system\", \"You are a wondrous wizard of math.\"),\n",
    "        few_shot_prompt,\n",
    "        (\"human\", \"{input}\"),\n",
    "    ]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "AIMessage(content='The cafeteria started with 23 apples. They used 20 for lunch, so they have 23 - 20 = 3 apples left. Then they bought 6 more, so they have 3 + 6 = 9 apples now. The answer is 9.')"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain.chat_models import ChatOpenAI\n",
    "\n",
    "chain = final_prompt | ChatOpenAI(\n",
    "    temperature=0,\n",
    "    openai_api_key=api_key,\n",
    "    openai_organization=organization,\n",
    "    )\n",
    "\n",
    "chain.invoke({\"input\": \"The cafeteria had 23 apples. If they used 20 to make lunch and bought 6 more, how many apples do they have?\"})"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Zero-shot CoT\n",
    "\n",
    "One recent idea that came out more recently is the idea of [zero-shot CoT](https://arxiv.org/abs/2205.11916) (Kojima et al. 2022) that essentially involves adding \"Let's think step by step\" to the original prompt. Let's try a simple problem and see how the model performs:\n",
    "\n",
    "\n",
    "<img src=\"./figures/zero-shot-cot.png\" >"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "AIMessage(content='Step 1: Start with the initial number of apples in the cafeteria, which is 23.\\n\\nStep 2: Subtract the number of apples used to make lunch, which is 20. \\n\\n23 - 20 = 3\\n\\nStep 3: Add the number of apples bought, which is 6.\\n\\n3 + 6 = 9\\n\\nTherefore, the cafeteria now has 9 apples.')"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain.prompts import ChatPromptTemplate\n",
    "from langchain.chat_models import ChatOpenAI\n",
    "\n",
    "chat_template = ChatPromptTemplate.from_messages(\n",
    "    [\n",
    "        (\"human\", \"{input} Let's think step by step.\"),\n",
    "\n",
    "    ]\n",
    ")\n",
    "\n",
    "chain = chat_template | ChatOpenAI(\n",
    "    temperature=0,\n",
    "    openai_api_key=api_key,\n",
    "    openai_organization=organization,\n",
    "    )\n",
    "\n",
    "chain.invoke({\"input\" : \"The cafeteria had 23 apples. If they used 20 to make lunch and bought 6 more, how many apples do they have?\"})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Self-Consistency\n",
    "\n",
    "Perhaps one of the more advanced techniques out there for prompt engineering is self-consistency. Proposed by [Wang et al. (2022)](https://arxiv.org/pdf/2203.11171.pdf), self-consistency aims \"to replace the naive greedy decoding used in chain-of-thought prompting\". The idea is to sample multiple, diverse reasoning paths through few-shot CoT, and use the generations to select the most consistent answer. This helps to boost the performance of CoT prompting on tasks involving arithmetic and commonsense reasoning.\n",
    "\n",
    "<img src=\"./figures/self-consistency.jpeg\" >"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's try the following example for arithmetic reasoning:\n",
    "\n",
    "*Prompt:*\n",
    "```\n",
    "When I was 6 my sister was half my age. Now\n",
    "I’m 70 how old is my sister?\n",
    "```\n",
    "\n",
    "*Output:*\n",
    "```\n",
    "35\n",
    "```\n",
    "\n",
    "The output is wrong! How may we improve this with self-consistency? Let's try it out. We will use the few-shot exemplars from Wang et al. 2022 (Table 17):"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*Prompt:*\n",
    "```\n",
    "Q: There are 15 trees in the grove. Grove workers will plant trees in the grove today. After they are done,\n",
    "there will be 21 trees. How many trees did the grove workers plant today?\n",
    "A: We start with 15 trees. Later we have 21 trees. The difference must be the number of trees they planted.\n",
    "So, they must have planted 21 - 15 = 6 trees. The answer is 6.\n",
    "\n",
    "Q: If there are 3 cars in the parking lot and 2 more cars arrive, how many cars are in the parking lot?\n",
    "A: There are 3 cars in the parking lot already. 2 more arrive. Now there are 3 + 2 = 5 cars. The answer is 5.\n",
    "\n",
    "Q: Leah had 32 chocolates and her sister had 42. If they ate 35, how many pieces do they have left in total?\n",
    "A: Leah had 32 chocolates and Leah’s sister had 42. That means there were originally 32 + 42 = 74\n",
    "chocolates. 35 have been eaten. So in total they still have 74 - 35 = 39 chocolates. The answer is 39.\n",
    "\n",
    "Q: Jason had 20 lollipops. He gave Denny some lollipops. Now Jason has 12 lollipops. How many lollipops\n",
    "did Jason give to Denny?\n",
    "A: Jason had 20 lollipops. Since he only has 12 now, he must have given the rest to Denny. The number of\n",
    "lollipops he has given to Denny must have been 20 - 12 = 8 lollipops. The answer is 8.\n",
    "\n",
    "Q: Shawn has five toys. For Christmas, he got two toys each from his mom and dad. How many toys does\n",
    "he have now?\n",
    "A: He has 5 toys. He got 2 from mom, so after that he has 5 + 2 = 7 toys. Then he got 2 more from dad, so\n",
    "in total he has 7 + 2 = 9 toys. The answer is 9.\n",
    "\n",
    "Q: There were nine computers in the server room. Five more computers were installed each day, from\n",
    "monday to thursday. How many computers are now in the server room?\n",
    "A: There are 4 days from monday to thursday. 5 computers were added each day. That means in total 4 * 5 =\n",
    "20 computers were added. There were 9 computers in the beginning, so now there are 9 + 20 = 29 computers.\n",
    "The answer is 29.\n",
    "\n",
    "Q: Michael had 58 golf balls. On tuesday, he lost 23 golf balls. On wednesday, he lost 2 more. How many\n",
    "golf balls did he have at the end of wednesday?\n",
    "A: Michael initially had 58 balls. He lost 23 on Tuesday, so after that he has 58 - 23 = 35 balls. On\n",
    "Wednesday he lost 2 more so now he has 35 - 2 = 33 balls. The answer is 33.\n",
    "\n",
    "Q: Olivia has $23. She bought five bagels for $3 each. How much money does she have left?\n",
    "A: She bought 5 bagels for $3 each. This means she spent 5\n",
    "\n",
    "Q: When I was 6 my sister was half my age. Now I’m 70 how old is my sister?\n",
    "A:\n",
    "```\n",
    "\n",
    "*Output 1:*\n",
    "```\n",
    "When I was 6 my sister was half my age, so she was 3. Now I am 70, so she is 70 - 3 = 67. The answer is 67.\n",
    "```\n",
    "\n",
    "*Output 2:*\n",
    "```\n",
    "When the narrator was 6, his sister was half his age, which is 3. Now that the narrator is 70, his sister would be 70 - 3 = 67 years old. The answer is 67.\n",
    "```\n",
    "\n",
    "*Output 3:*\n",
    "```\n",
    "When I was 6 my sister was half my age, so she was 3. Now I am 70, so she is 70/2 = 35. The answer is 35.\n",
    "```\n",
    "\n",
    "Computing for the final answer involves a few steps (check out the paper for the details) but for the sake of simplicity, we can see that there is already a majority answer emerging so that would essentially become the final answer. "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tree-of-Thought (ToT)\n",
    "\n",
    "The Tree of Thought (ToT) is a chain that allows you to query a Large Language Model (LLM) using the Tree of Thought technique. \n",
    "This is based on the paper [\\\"Large Language Model Guided Tree-of-Thought\\\"](https://arxiv.org/pdf/2305.08291.pdf)\"\n",
    "\n",
    "<img src=\"./figures/tot.jpeg\" >"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Using Sequential Chain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.chains import LLMChain\n",
    "from langchain.llms import OpenAI\n",
    "from langchain.prompts import PromptTemplate\n",
    "from langchain.chat_models import ChatOpenAI\n",
    "\n",
    "\n",
    "template =\"\"\"\n",
    "Step1 :\n",
    " \n",
    "I have a problem related to {input}. Could you brainstorm three distinct solutions? Please consider a variety of factors such as {perfect_factors}\n",
    "A:\n",
    "\"\"\"\n",
    "\n",
    "prompt1 = PromptTemplate(\n",
    "    input_variables=[\"input\",\"perfect_factors\"],\n",
    "    template = template                      \n",
    ")\n",
    "\n",
    "chain1 = LLMChain(\n",
    "    llm=ChatOpenAI(\n",
    "        temperature=0,\n",
    "        openai_api_key=api_key,\n",
    "        openai_organization=organization,\n",
    "        ),\n",
    "    prompt=prompt1,\n",
    "    output_key=\"solutions\",\n",
    "    verbose=True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "template =\"\"\"\n",
    "Step 2:\n",
    "\n",
    "For each of the three proposed solutions, evaluate their potential. Consider their pros and cons, initial effort needed, implementation difficulty, potential challenges, and the expected outcomes. Assign a probability of success and a confidence level to each option based on these factors\n",
    "\n",
    "{solutions}\n",
    "\n",
    "A:\"\"\"\n",
    "\n",
    "prompt2 = PromptTemplate(\n",
    "    input_variables=[\"solutions\"],\n",
    "    template = template                      \n",
    ")\n",
    "\n",
    "chain2 = LLMChain(\n",
    "    llm=ChatOpenAI(\n",
    "        temperature=0,\n",
    "        openai_api_key=api_key,\n",
    "        openai_organization=organization,\n",
    "        ),\n",
    "    prompt=prompt2,\n",
    "    output_key=\"review\",\n",
    "    verbose=True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "template =\"\"\"\n",
    "Step 3:\n",
    "\n",
    "For each solution, deepen the thought process. Generate potential scenarios, strategies for implementation, any necessary partnerships or resources, and how potential obstacles might be overcome. Also, consider any potential unexpected outcomes and how they might be handled.\n",
    "\n",
    "{review}\n",
    "\n",
    "A:\"\"\"\n",
    "\n",
    "prompt3 = PromptTemplate(\n",
    "    input_variables=[\"review\"],\n",
    "    template = template                      \n",
    ")\n",
    "\n",
    "chain3 = LLMChain(\n",
    "    llm=ChatOpenAI(\n",
    "        temperature=0,\n",
    "        openai_api_key=api_key,\n",
    "        openai_organization=organization,\n",
    "        ),\n",
    "    prompt=prompt3,\n",
    "    output_key=\"deepen_thought_process\",\n",
    "    verbose=True\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "template =\"\"\"\n",
    "Step 4:\n",
    "\n",
    "Based on the evaluations and scenarios, rank the solutions in order of promise. Provide a justification for each ranking and offer any final thoughts or considerations for each solution\n",
    "{deepen_thought_process}\n",
    "\n",
    "A:\"\"\"\n",
    "\n",
    "prompt4 = PromptTemplate(\n",
    "    input_variables=[\"deepen_thought_process\"],\n",
    "    template = template                      \n",
    ")\n",
    "\n",
    "chain4 = LLMChain(\n",
    "    llm=ChatOpenAI(\n",
    "        temperature=0,\n",
    "        openai_api_key=api_key,\n",
    "        openai_organization=organization,\n",
    "        ),\n",
    "    prompt=prompt4,\n",
    "    output_key=\"ranked_solutions\",\n",
    "    verbose=True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\u001b[1m> Entering new SequentialChain chain...\u001b[0m\n",
      "\n",
      "\n",
      "\u001b[1m> Entering new LLMChain chain...\u001b[0m\n",
      "Prompt after formatting:\n",
      "\u001b[32;1m\u001b[1;3m\n",
      "Step1 :\n",
      " \n",
      "I have a problem related to human colonization of Mars. Could you brainstorm three distinct solutions? Please consider a variety of factors such as The distance between Earth and Mars is very large, making regular resupply difficult\n",
      "A:\n",
      "\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n",
      "\n",
      "\n",
      "\u001b[1m> Entering new LLMChain chain...\u001b[0m\n",
      "Prompt after formatting:\n",
      "\u001b[32;1m\u001b[1;3m\n",
      "Step 2:\n",
      "\n",
      "For each of the three proposed solutions, evaluate their potential. Consider their pros and cons, initial effort needed, implementation difficulty, potential challenges, and the expected outcomes. Assign a probability of success and a confidence level to each option based on these factors\n",
      "\n",
      "1. Sustainable Resource Management: One solution could be to focus on sustainable resource management on Mars. This would involve developing technologies and systems that allow for efficient use of available resources, such as water, energy, and food. By maximizing resource utilization and minimizing waste, colonizers could reduce their dependence on regular resupply from Earth.\n",
      "\n",
      "2. In-Situ Resource Utilization (ISRU): Another solution could be to prioritize the development of ISRU technologies on Mars. ISRU involves utilizing the resources available on Mars, such as extracting water from ice deposits or utilizing the Martian atmosphere for oxygen production. By relying on local resources, colonizers could reduce the need for regular resupply and establish a more self-sustaining colony.\n",
      "\n",
      "3. Interplanetary Supply Chain: To address the challenge of regular resupply, establishing an efficient interplanetary supply chain could be crucial. This would involve developing advanced spacecraft and logistics systems capable of transporting necessary supplies from Earth to Mars on a regular basis. Additionally, establishing a network of resupply stations or depots along the route could help optimize the transportation process and reduce costs.\n",
      "\n",
      "These solutions aim to address the challenges posed by the large distance between Earth and Mars, ensuring the sustainability and self-sufficiency of a Martian colony while minimizing the need for regular resupply.\n",
      "\n",
      "A:\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n",
      "\n",
      "\n",
      "\u001b[1m> Entering new LLMChain chain...\u001b[0m\n",
      "Prompt after formatting:\n",
      "\u001b[32;1m\u001b[1;3m\n",
      "Step 3:\n",
      "\n",
      "For each solution, deepen the thought process. Generate potential scenarios, strategies for implementation, any necessary partnerships or resources, and how potential obstacles might be overcome. Also, consider any potential unexpected outcomes and how they might be handled.\n",
      "\n",
      "Sustainable Resource Management:\n",
      "Pros:\n",
      "- Reduces dependence on regular resupply from Earth\n",
      "- Maximizes resource utilization and minimizes waste\n",
      "- Promotes self-sufficiency and sustainability of the colony\n",
      "\n",
      "Cons:\n",
      "- Requires significant initial effort and investment in developing technologies and systems\n",
      "- Implementation difficulty may be high due to the need for advanced resource management systems\n",
      "- Potential challenges include the need for continuous monitoring and maintenance of resource utilization systems\n",
      "\n",
      "Probability of success: 80%\n",
      "Confidence level: High\n",
      "\n",
      "B: In-Situ Resource Utilization (ISRU):\n",
      "Pros:\n",
      "- Reduces dependence on regular resupply from Earth\n",
      "- Utilizes local resources, making the colony more self-sustaining\n",
      "- Potential for long-term resource availability on Mars\n",
      "\n",
      "Cons:\n",
      "- Requires development of advanced ISRU technologies\n",
      "- Implementation difficulty may be high due to the need for resource extraction and processing systems\n",
      "- Potential challenges include the need for continuous research and development to optimize resource utilization\n",
      "\n",
      "Probability of success: 70%\n",
      "Confidence level: Medium\n",
      "\n",
      "C: Interplanetary Supply Chain:\n",
      "Pros:\n",
      "- Addresses the challenge of regular resupply from Earth\n",
      "- Enables efficient transportation of necessary supplies\n",
      "- Potential for cost optimization through the establishment of resupply stations or depots\n",
      "\n",
      "Cons:\n",
      "- Requires significant investment in developing advanced spacecraft and logistics systems\n",
      "- Implementation difficulty may be high due to the need for interplanetary transportation capabilities\n",
      "- Potential challenges include the need for continuous maintenance and upgrades of the supply chain infrastructure\n",
      "\n",
      "Probability of success: 75%\n",
      "Confidence level: Medium\n",
      "\n",
      "A:\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n",
      "\n",
      "\n",
      "\u001b[1m> Entering new LLMChain chain...\u001b[0m\n",
      "Prompt after formatting:\n",
      "\u001b[32;1m\u001b[1;3m\n",
      "Step 4:\n",
      "\n",
      "Based on the evaluations and scenarios, rank the solutions in order of promise. Provide a justification for each ranking and offer any final thoughts or considerations for each solution\n",
      "Vertical Farming:\n",
      "Pros:\n",
      "- Reduces dependence on regular resupply from Earth for food\n",
      "- Maximizes food production in a limited space\n",
      "- Provides fresh and nutritious food for the colony\n",
      "\n",
      "Cons:\n",
      "- Requires significant initial investment in developing vertical farming systems\n",
      "- Implementation difficulty may be high due to the need for controlled environments and efficient use of resources\n",
      "- Potential challenges include the need for continuous monitoring and maintenance of the farming systems\n",
      "\n",
      "Probability of success: 85%\n",
      "Confidence level: High\n",
      "\n",
      "Deepening the thought process for Vertical Farming:\n",
      "\n",
      "Potential scenarios:\n",
      "- The colony successfully establishes a vertical farming system that produces enough food to sustain the population.\n",
      "- The vertical farming system experiences technical difficulties, leading to a temporary shortage of food.\n",
      "- The vertical farming system exceeds expectations and produces surplus food, allowing for trade with other colonies or Earth.\n",
      "\n",
      "Strategies for implementation:\n",
      "- Conduct thorough research and development to optimize vertical farming systems for Mars' unique conditions.\n",
      "- Collaborate with experts in agriculture and hydroponics to design efficient and sustainable farming systems.\n",
      "- Establish a training program to educate colony residents on vertical farming techniques and maintenance.\n",
      "\n",
      "Partnerships and resources:\n",
      "- Partner with agricultural companies and universities to access their expertise and resources in vertical farming.\n",
      "- Seek funding from government agencies, private investors, and philanthropic organizations to support the development and implementation of vertical farming systems.\n",
      "\n",
      "Overcoming potential obstacles:\n",
      "- Continuous monitoring and maintenance of the farming systems can be achieved through regular inspections and training of colony residents.\n",
      "- Technical difficulties can be addressed through a dedicated team of engineers and technicians who are trained to troubleshoot and repair the farming systems.\n",
      "- In the event of a temporary shortage of food, alternative food sources such as emergency rations or imported supplies can be utilized until the vertical farming system is back on track.\n",
      "\n",
      "Potential unexpected outcomes:\n",
      "- The vertical farming system may face unexpected challenges such as pest infestations or crop diseases. These can be addressed through the implementation of integrated pest management strategies and disease prevention measures.\n",
      "- The surplus food produced by the vertical farming system may lead to economic opportunities for the colony, such as exporting food to other colonies or Earth. This can contribute to the overall sustainability and self-sufficiency of the colony.\n",
      "\n",
      "A:\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "from langchain.chains import SequentialChain\n",
    "\n",
    "overall_chain = SequentialChain(\n",
    "    chains=[chain1, chain2, chain3, chain4],\n",
    "    input_variables=[\"input\", \"perfect_factors\"],\n",
    "    output_variables=[\"ranked_solutions\"],\n",
    "    verbose=True\n",
    ")\n",
    "\n",
    "\n",
    "reuslt = overall_chain({\n",
    "        \"input\": \"human colonization of Mars\", \n",
    "        \"perfect_factors\": \"The distance between Earth and Mars is very large, making regular resupply difficult\"\n",
    "    })\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_keys(['input', 'perfect_factors', 'ranked_solutions'])"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "reuslt.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Rank: 1\n",
      "\n",
      "Justification: Vertical farming has the highest promise due to its potential to reduce dependence on resupply from Earth, maximize food production in a limited space, and provide fresh and nutritious food for the colony. While there are initial investment and implementation challenges, the probability of success is high, and the confidence level is also high. The potential scenarios, strategies for implementation, partnerships and resources, and overcoming potential obstacles all support the feasibility and potential success of vertical farming.\n",
      "\n",
      "B: Rank: 2\n",
      "\n",
      "Justification: The second-highest promise solution is the development of advanced water recycling systems. While it is crucial for sustaining life on Mars, it is not as promising as vertical farming in terms of reducing dependence on Earth for food. However, it is still a critical solution for ensuring a sustainable water supply for the colony. The potential scenarios, strategies for implementation, partnerships and resources, and overcoming potential obstacles all support the feasibility and potential success of advanced water recycling systems.\n",
      "\n",
      "C: Rank: 3\n",
      "\n",
      "Justification: The third-ranked solution is the establishment of renewable energy sources. While renewable energy is essential for reducing dependence on Earth for power, it is not as directly related to the immediate needs of food production and water supply. However, it is still a crucial solution for long-term sustainability and reducing reliance on limited resources. The potential scenarios, strategies for implementation, partnerships and resources, and overcoming potential obstacles all support the feasibility and potential success of renewable energy sources.\n",
      "\n",
      "D: Rank: 4\n",
      "\n",
      "Justification: The fourth-ranked solution is the development of advanced 3D printing technology. While 3D printing can be beneficial for manufacturing and construction on Mars, it is not as directly related to the immediate needs of food production, water supply, or energy generation. However, it can still contribute to the overall self-sufficiency and resource utilization of the colony. The potential scenarios, strategies for implementation, partnerships and resources, and overcoming potential obstacles all support the feasibility and potential success of advanced 3D printing technology.\n",
      "\n",
      "Final thoughts and considerations:\n",
      "\n",
      "- It is important to prioritize solutions that directly address the immediate needs of food production, water supply, and energy generation, as these are crucial for the survival and sustainability of the colony.\n",
      "- Collaboration with experts, partnerships with relevant organizations, and securing funding are essential for the successful implementation of these solutions.\n",
      "- Continuous monitoring, maintenance, and troubleshooting plans should be in place to address any technical difficulties or unexpected challenges that may arise.\n",
      "- The potential for economic opportunities, such as exporting surplus food or utilizing advanced 3D printing technology for manufacturing, should be considered to enhance the self-sufficiency and sustainability of the colony.\n"
     ]
    }
   ],
   "source": [
    "print(reuslt['ranked_solutions'])"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Using ToTChain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3,*,*,2|1,*,3,*|*,1,*,3|4,*,*,1\n",
      "\n",
      "- This is a 4x4 Sudoku puzzle.\n",
      "- The * represents a cell to be filled.\n",
      "- The | character separates rows.\n",
      "- At each step, replace one or more * with digits 1-4.\n",
      "- There must be no duplicate digits in any row, column or 2x2 subgrid.\n",
      "- Keep the known digits from previous valid thoughts in place.\n",
      "- Each thought can be a partial or the final solution.\n"
     ]
    }
   ],
   "source": [
    "sudoku_puzzle =   \"3,*,*,2|1,*,3,*|*,1,*,3|4,*,*,1\"\n",
    "sudoku_solution = \"3,4,1,2|1,2,3,4|2,1,4,3|4,3,2,1\"\n",
    "problem_description = f\"\"\"\n",
    "{sudoku_puzzle}\n",
    "\n",
    "- This is a 4x4 Sudoku puzzle.\n",
    "- The * represents a cell to be filled.\n",
    "- The | character separates rows.\n",
    "- At each step, replace one or more * with digits 1-4.\n",
    "- There must be no duplicate digits in any row, column or 2x2 subgrid.\n",
    "- Keep the known digits from previous valid thoughts in place.\n",
    "- Each thought can be a partial or the final solution.\n",
    "\"\"\".strip()\n",
    "\n",
    "print(problem_description)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('3,*,*,2|1,*,3,*|*,1,*,3|4,*,*,1',)\n",
      "('3,4,1,2|1,2,3,4|2,1,4,3|4,3,2,1',)\n",
      "('3,4,1,2|1,2,3,4|2,1,4,3|4,3,*,1',)\n",
      "('3,4,1,2|1,2,3,4|2,1,4,3|4,*,3,1',)\n"
     ]
    }
   ],
   "source": [
    "from langchain_experimental.tot.base import ToTChain\n",
    "from langchain.memory import ConversationBufferWindowMemory\n",
    "from langchain_experimental.tot.checker import ToTChecker\n",
    "from langchain_experimental.tot.thought import ThoughtValidity\n",
    "import re\n",
    "from typing import Tuple\n",
    "\n",
    "class MyChecker(ToTChecker):\n",
    "    def evaluate(self, problem_description: str, thoughts: Tuple[str, ...] = ()) -> ThoughtValidity:\n",
    "        print(thoughts)\n",
    "        last_thought = thoughts[-1]\n",
    "        clean_solution = last_thought.replace(\" \", \"\").replace('\"', \"\")\n",
    "        regex_solution = clean_solution.replace(\"*\", \".\").replace(\"|\", \"\\\\|\")\n",
    "        if sudoku_solution in clean_solution:\n",
    "            return ThoughtValidity.VALID_FINAL\n",
    "        elif re.search(regex_solution, sudoku_solution):\n",
    "            return ThoughtValidity.VALID_INTERMEDIATE\n",
    "        else:\n",
    "            return ThoughtValidity.INVALID\n",
    "\n",
    "checker = MyChecker()\n",
    "\n",
    "assert checker.evaluate(\"\", (\"3,*,*,2|1,*,3,*|*,1,*,3|4,*,*,1\",)) == ThoughtValidity.VALID_INTERMEDIATE\n",
    "assert checker.evaluate(\"\", (\"3,4,1,2|1,2,3,4|2,1,4,3|4,3,2,1\",)) == ThoughtValidity.VALID_FINAL\n",
    "assert checker.evaluate(\"\", (\"3,4,1,2|1,2,3,4|2,1,4,3|4,3,*,1\",)) == ThoughtValidity.VALID_INTERMEDIATE\n",
    "assert checker.evaluate(\"\", (\"3,4,1,2|1,2,3,4|2,1,4,3|4,*,3,1\",)) == ThoughtValidity.INVALID"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\u001b[1m> Entering new ToTChain chain...\u001b[0m\n",
      "Starting the ToT solve procedure.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/homebrew/lib/python3.11/site-packages/langchain/chains/llm.py:321: UserWarning: The predict_and_parse method is deprecated, instead pass an output parser directly to LLMChain.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('3,*,*,2|1,*,3,*|*,1,*,3|4,*,*,1',)\n",
      "\u001b[33;1m\u001b[1;3mThought: 3,*,*,2|1,*,3,*|*,1,*,3|4,*,*,1\n",
      "\u001b[0m"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/homebrew/lib/python3.11/site-packages/langchain/chains/llm.py:321: UserWarning: The predict_and_parse method is deprecated, instead pass an output parser directly to LLMChain.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('3,*,*,2|1,*,3,*|*,1,*,3|4,*,*,1', '3,*,*,2|1,*,3,*|*,1,*,3|4,*,*,1')\n",
      "\u001b[33;1m\u001b[1;3m    Thought: 3,*,*,2|1,*,3,*|*,1,*,3|4,*,*,1\n",
      "\u001b[0m"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/homebrew/lib/python3.11/site-packages/langchain/chains/llm.py:321: UserWarning: The predict_and_parse method is deprecated, instead pass an output parser directly to LLMChain.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('3,*,*,2|1,*,3,*|*,1,*,3|4,*,*,1', '3,*,*,2|1,*,3,*|*,1,*,3|4,*,*,1', '3,*,*,2|1,*,3,*|*,1,*,3|4,*,*,1')\n",
      "\u001b[33;1m\u001b[1;3m        Thought: 3,*,*,2|1,*,3,*|*,1,*,3|4,*,*,1\n",
      "\u001b[0m"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/homebrew/lib/python3.11/site-packages/langchain/chains/llm.py:321: UserWarning: The predict_and_parse method is deprecated, instead pass an output parser directly to LLMChain.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('3,*,*,2|1,*,3,*|*,1,*,3|4,*,*,1', '3,*,*,2|1,*,3,*|*,1,*,3|4,*,*,1', '3,*,*,2|1,*,3,*|*,1,*,3|4,*,*,1', '3,*,*,2|1,*,3,*|*,1,*,3|4,*,*,1')\n",
      "\u001b[33;1m\u001b[1;3m            Thought: 3,*,*,2|1,*,3,*|*,1,*,3|4,*,*,1\n",
      "\u001b[0m"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/homebrew/lib/python3.11/site-packages/langchain/chains/llm.py:321: UserWarning: The predict_and_parse method is deprecated, instead pass an output parser directly to LLMChain.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('3,*,*,2|1,*,3,*|*,1,*,3|4,*,*,1', '3,*,*,2|1,*,3,*|*,1,*,3|4,*,*,1', '3,*,*,2|1,*,3,*|*,1,*,3|4,*,*,1', '3,*,*,2|1,*,3,*|*,1,*,3|4,*,*,1', '3,*,*,2|1,*,3,*|*,1,*,3|4,*,*,1')\n",
      "\u001b[33;1m\u001b[1;3m                Thought: 3,*,*,2|1,*,3,*|*,1,*,3|4,*,*,1\n",
      "\u001b[0m"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/homebrew/lib/python3.11/site-packages/langchain/chains/llm.py:321: UserWarning: The predict_and_parse method is deprecated, instead pass an output parser directly to LLMChain.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('3,*,*,2|1,*,3,*|*,1,*,3|4,*,*,1', '3,*,*,2|1,*,3,*|*,1,*,3|4,*,*,1', '3,*,*,2|1,*,3,*|*,1,*,3|4,*,*,1', '3,*,*,2|1,*,3,*|*,1,*,3|4,*,*,1', '3,*,*,2|1,*,3,*|*,1,*,3|4,*,*,1', '3,*,*,2|1,*,3,*|*,1,*,3|4,*,*,1')\n",
      "\u001b[33;1m\u001b[1;3m                    Thought: 3,*,*,2|1,*,3,*|*,1,*,3|4,*,*,1\n",
      "\u001b[0m"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/homebrew/lib/python3.11/site-packages/langchain/chains/llm.py:321: UserWarning: The predict_and_parse method is deprecated, instead pass an output parser directly to LLMChain.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "from langchain.chat_models import ChatOpenAI\n",
    "\n",
    "llm = ChatOpenAI(\n",
    "    temperature=0,\n",
    "    openai_api_key=api_key,\n",
    "    openai_organization=organization,\n",
    ")\n",
    "\n",
    "tot_chain = ToTChain(\n",
    "    llm = llm,\n",
    "    memory = ConversationBufferWindowMemory(),\n",
    "    checker = checker,\n",
    "    verbose = True,\n",
    "    # verbose_llm = True\n",
    ")\n",
    "\n",
    "tot_chain.run({\"problem_description\": problem_description})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generated Knowledge Prompting\n",
    "\n",
    "LLMs continue to be improved and one popular technique includes the ability to incorporate knowledge or information to help the model make more accurate predictions. \n",
    "\n",
    "Using a similar idea, can the model also be used to generate knowledge before making a prediction? That's what is attempted in the paper by [Liu et al. 2022](https://arxiv.org/pdf/2110.08387.pdf) -- generate knowledge to be used as part of the prompt. In particular, how helpful is this for tasks such as commonsense reasoning?\n",
    "\n",
    "<img src=\"./figures/general-knowledge.png\" >"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's try a simple prompt:\n",
    "\n",
    "*Prompt:*\n",
    "```\n",
    "Part of golf is trying to get a higher point total than others. Yes or No?\n",
    "```\n",
    "\n",
    "*Output:*\n",
    "```\n",
    "Yes.\n",
    "```\n",
    "\n",
    "This type of mistake reveals the limitations of LLMs to perform tasks that require more knowledge about the world. How do we improve this with knowledge generation?\n",
    "\n",
    "First, we generate a few \"knowledges\":\n",
    "\n",
    "*Prompt:*\n",
    "```\n",
    "Input: Greece is larger than mexico.\n",
    "Knowledge: Greece is approximately 131,957 sq km, while Mexico is approximately 1,964,375 sq km, making Mexico 1,389% larger than Greece.\n",
    "\n",
    "Input: Glasses always fog up.\n",
    "Knowledge: Condensation occurs on eyeglass lenses when water vapor from your sweat, breath, and ambient humidity lands on a cold surface, cools, and then changes into tiny drops of liquid, forming a film that you see as fog. Your lenses will be relatively cool compared to your breath, especially when the outside air is cold.\n",
    "\n",
    "Input: A fish is capable of thinking.\n",
    "Knowledge: Fish are more intelligent than they appear. In many areas, such as memory, their cognitive powers match or exceed those of ’higher’ vertebrates including non-human primates. Fish’s long-term memories help them keep track of complex social relationships.\n",
    "\n",
    "Input: A common effect of smoking lots of cigarettes in one’s lifetime is a higher than normal chance of getting lung cancer.\n",
    "Knowledge: Those who consistently averaged less than one cigarette per day over their lifetime had nine times the risk of dying from lung cancer than never smokers. Among people who smoked between one and 10 cigarettes per day, the risk of dying from lung cancer was nearly 12 times higher than that of never smokers.\n",
    "\n",
    "Input: A rock is the same size as a pebble.\n",
    "Knowledge: A pebble is a clast of rock with a particle size of 4 to 64 millimetres based on the Udden-Wentworth scale of sedimentology. Pebbles are generally considered larger than granules (2 to 4 millimetres diameter) and smaller than cobbles (64 to 256 millimetres diameter).\n",
    "\n",
    "Input: Part of golf is trying to get a higher point total than others.\n",
    "Knowledge:\n",
    "```\n",
    "\n",
    "*Knowledge 1:*\n",
    "```\n",
    "The objective of golf is to play a set of holes in the least number of strokes. A round of golf typically consists of 18 holes. Each hole is played once in the round on a standard golf course. Each stroke is counted as one point, and the total number of strokes is used to determine the winner of the game.\n",
    "```\n",
    "\n",
    "*Knowledge 2:*\n",
    "```\n",
    "Golf is a precision club-and-ball sport in which competing players (or golfers) use many types of clubs to hit balls into a series of holes on a course using the fewest number of strokes. The goal is to complete the course with the lowest score, which is calculated by adding up the total number of strokes taken on each hole. The player with the lowest score wins the game.\n",
    "```\n",
    "\n",
    "We are using the prompt provided in the paper by [Liu et al. 2022](https://arxiv.org/pdf/2110.08387.pdf).\n",
    "\n",
    "The next step is to integrate the knowledge and get a prediction. I reformatted the question into QA format to guide the answer format.\n",
    "\n",
    "*Prompt:*\n",
    "```\n",
    "Question: Part of golf is trying to get a higher point total than others. Yes or No?\n",
    "\n",
    "Knowledge: The objective of golf is to play a set of holes in the least number of strokes. A round of golf typically consists of 18 holes. Each hole is played once in the round on a standard golf course. Each stroke is counted as one point, and the total number of strokes is used to determine the winner of the game.\n",
    "\n",
    "Explain and Answer: \n",
    "```\n",
    "\n",
    "*Answer 1 (confidence very high):*\n",
    "```\n",
    "No, the objective of golf is not to get a higher point total than others. Rather, the objective is to play a set of holes in the least number of strokes. The total number of strokes is used to determine the winner of the game, not the total number of points.\n",
    "```\n",
    "\n",
    "\n",
    "```\n",
    "Question: Part of golf is trying to get a higher point total than others. Yes or No?\n",
    "\n",
    "Knowledge: Golf is a precision club-and-ball sport in which competing players (or golfers) use many types of clubs to hit balls into a series of holes on a course using the fewest number of strokes. The goal is to complete the course with the lowest score, which is calculated by adding up the total number of strokes taken on each hole. The player with the lowest score wins the game.\n",
    "\n",
    "Explain and Answer:\n",
    "```\n",
    "\n",
    "*Answer 2 (confidence is a lot lower):*\n",
    "```\n",
    "Yes, part of golf is trying to get a higher point total than others. Each player tries to complete the course with the lowest score, which is calculated by adding up the total number of strokes taken on each hole. The player with the lowest score wins the game.\n",
    "```\n",
    "\n",
    "Some really interesting things happened with this example. In the first answer, the model was very confident but in the second not so much. I simplify the process for demonstration purposes but there are a few more details to consider when arriving at the final answer. Check out the paper for more."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Automatic Prompt Engineer (APE)\n",
    "\n",
    "<img src=\"./figures/ape.jpeg\" >\n",
    "\n",
    "[Zhou et al., (2022)](https://arxiv.org/abs/2211.01910) propose automatic prompt engineer (APE) a framework for automatic instruction generation and selection. The instruction generation problem is framed as natural language synthesis addressed as a black-box optimization problem using LLMs to generate and search over candidate solutions. \n",
    "\n",
    "The first step involves a large language model (as an inference model) that is given output demonstrations to generate instruction candidates for a task. These candidate solutions will guide the search procedure. The instructions are executed using a target model, and then the most appropriate instruction is selected based on computed evaluation scores. \n",
    "\n",
    "APE discovers a better zero-shot CoT prompt than the human engineered \"Let's think step by step\" prompt (Kojima et al., 2022).\n",
    "\n",
    "The prompt \"Let's work this out in a step by step way to be sure we have the right answer.\" elicits chain-of-though reasoning and improves performance on the MultiArith and GSM8K benchmarks:\n",
    "\n",
    "![](../img/ape-zero-shot-cot.png)\n",
    "\n",
    "This paper touches on an important topic related to prompt engineering which is the idea of automatically optimizing prompts. While we don't go deep into this topic in this guide, here are a few key papers if you are interested in the topic:\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Synergizing Reasoning and Acting in Language Models (ReAct)\n",
    "\n",
    "\n",
    "<img src=\"./figures/react.png\" >\n",
    "\n",
    "[Yao et al., (2023)](https://arxiv.org/pdf/2210.03629) designed to improve how LLMs solve tasks by blending thought processes with dynamic actions.\n",
    "\n",
    "REACT stands for Reasoning and Acting, a method that significantly enhances AI capabilities. It enables LLMs to generate thought processes and actions at the same time, allowing for adjustments as tasks evolve. The strengths of REACT include: 1. Allowing AI to interact with external data, enriching its thought process with up-to-date information. 2. Making AI's action plans more adaptable to changes, ensuring swift responses to new tasks or environments. [Read More](https://react-lm.github.io/)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Reference \n",
    "https://medium.com/@astropomeai/implementing-the-tree-of-thoughts-in-langchains-chain-f2ebc5864fac\n",
    "https://github.com/mrspiggot/forestOfThoughts/blob/master/langchain_monoculture.py\n",
    "https://cobusgreyling.medium.com/langchain-langsmith-llm-guided-tree-of-thought-47a2cd5bcfca\n",
    "https://blog.athina.ai/large-language-models-are-human-level-prompt-engineers\n",
    "https://blog.athina.ai/react-synergizing-reasoning-and-acting-in-language-models\n",
    "https://blog.athina.ai/making-large-language-models-better-reasoners-with-step-aware-verifier"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
