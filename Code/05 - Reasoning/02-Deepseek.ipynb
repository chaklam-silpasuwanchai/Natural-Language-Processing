{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# IN PROGRESS"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. Deepseek-V3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.1 Multi-head Latent Attention-(MLA)\n",
    "<img src=\"./figures/mla.webp\" >\n",
    "\n",
    "We first introduce the standard MHA mechanism as background. \n",
    "Let $d$ be the embedding dimension, $n_h$ be the number of attention heads, $d_h$ be the dimension per head, and $\\mathbf{h}_{t} \\in \\mathbb{R}^{d}$ be the attention input of the $t$-th token at an attention layer. \n",
    "Standard MHA first produces $\\mathbf{q}_{t}, \\mathbf{k}_{t}, \\mathbf{v}_{t} \\in \\mathbb{R}^{d_h n_h}$ through three matrices $W^{Q}, W^{K}, W^{V} \\in \\mathbb{R}^{d_h n_h \\times d}$, respectively: \n",
    "\n",
    "$$\n",
    "\\mathbf{q}_{t} = W^{Q} \\mathbf{h}_{t},\n",
    "$$\n",
    "$$\n",
    "\\mathbf{k}_{t} = W^{K} \\mathbf{h}_{t},\n",
    "$$\n",
    "$$\n",
    "\\mathbf{v}_{t} = W^{V} \\mathbf{h}_{t}\n",
    "$$\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.2 Low-Rank Key-Value Joint Compression\n",
    "\n",
    "The core of \\dsattn{} is the low-rank joint compression for keys and values to reduce KV cache:\n",
    "\n",
    "$$\n",
    "\\mathbf{c}_{t}^{KV} = W^{DKV} \\mathbf{h}_{t},\n",
    "$$\n",
    "$$\n",
    "\\mathbf{k}_{t}^{C} = W^{UK} \\mathbf{c}_{t}^{KV},\n",
    "$$\n",
    "$$\n",
    "\\mathbf{v}_{t}^{C} = W^{UV} \\mathbf{c}_{t}^{KV}\n",
    "$$\n",
    "\n",
    "\n",
    "where $\\mathbf{c}_{t}^{KV} \\in \\mathbb{R}^{d_c}$ is the compressed latent vector for keys and values; \n",
    "$d_c (\\ll d_h n_h)$ denotes the KV compression dimension;\n",
    "$W^{DKV} \\in \\mathbb{R}^{d_c \\times d}$ is the down-projection matrix;\n",
    "and $W^{UK},W^{UV} \\in \\mathbb{R}^{d_h n_h \\times d_c}$ are the up-projection matrices for keys and values, respectively. \n",
    "During inference, \\dsattn{} only needs to cache $\\mathbf{c}_{t}^{KV}$, so its KV cache has only $d_{c}l$ elements, where $l$ denotes the number of layers. \n",
    "In addition, during inference, since $W^{UK}$ can be absorbed into $W^{Q}$, and $W^{UV}$ can be absorbed into $W^{O}$, we even do not need to compute keys and values out for attention. \n",
    "Figure~\\ref{fig:dsattn} intuitively illustrates how the KV joint compression in \\dsattn{} reduces the KV cache. \n",
    "\n",
    "Moreover, in order to reduce the activation memory during training, we also perform low-rank compression for the queries, even if it cannot reduce the KV cache:\n",
    "\n",
    "$$\n",
    "\\mathbf{c}_{t}^{Q} = W^{DQ} \\mathbf{h}_{t}, \n",
    "$$\n",
    "$$\n",
    "\\mathbf{q}_{t}^{C} = W^{UQ} \\mathbf{c}_{t}^{Q},\n",
    "$$\n",
    "\n",
    "\n",
    "\n",
    "where $\\mathbf{c}_{t}^{Q} \\in \\mathbb{R}^{d_c^{\\prime}}$ is the compressed latent vector for queries; \n",
    "$d_c^{\\prime} (\\ll d_h n_h)$ denotes the query compression dimension; \n",
    "and $W^{DQ} \\in \\mathbb{R}^{d_c^{\\prime} \\times d}, W^{UQ} \\in \\mathbb{R}^{d_h n_h \\times d_c^{\\prime}}$ are the down-projection and up-projection matrices for queries, respectively. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "class LoxoRankKVCompression(nn.Module):\n",
    "    def __init__(self, d_model=512, n_heads=8, d_head=64, d_compression=32):\n",
    "        super().__init__()\n",
    "        self.d_model = d_model\n",
    "        self.n_heads = n_heads\n",
    "        self.d_head = d_head\n",
    "        self.d_compression = d_compression\n",
    "\n",
    "        # Projection matrices\n",
    "        self.W_DKV = nn.Linear(d_model, d_compression, bias=False)  # Down-projection\n",
    "        self.W_UK = nn.Linear(d_compression, n_heads * d_head, bias=False)  # Key up-projection\n",
    "        self.W_UV = nn.Linear(d_compression, n_heads * d_head, bias=False)  # Value up-projection\n",
    "\n",
    "    def forward(self, h_t, cache=None):\n",
    "        \"\"\"Process one token step, returns compressed KV and reconstructed K/V\"\"\"\n",
    "        # h_t shape: (batch_size, d_model)\n",
    "        \n",
    "        # Step 1: Joint KV compression (Equation 9)\n",
    "        c_t_KV = self.W_DKV(h_t)  # (batch_size, d_compression)\n",
    "        \n",
    "        # Step 2: Cache management (store compressed representation)\n",
    "        if cache is not None:\n",
    "            cache.append(c_t_KV.detach())\n",
    "        \n",
    "        # Step 3: Up-projection to original dimensions (Equations 10-11)\n",
    "        k_t_C = self.W_UK(c_t_KV).view(-1, self.n_heads, self.d_head)  # (batch_size, n_heads, d_head)\n",
    "        v_t_C = self.W_UV(c_t_KV).view(-1, self.n_heads, self.d_head)  # (batch_size, n_heads, d_head)\n",
    "        \n",
    "        return k_t_C, v_t_C, c_t_KV\n",
    "\n",
    "# Example usage\n",
    "batch_size = 1\n",
    "d_model = 512\n",
    "d_compression = 32\n",
    "\n",
    "# Initialize module\n",
    "compressor = LoxoRankKVCompression(d_model=d_model, d_compression=d_compression)\n",
    "\n",
    "# Simulate hidden state for one token\n",
    "h_t = torch.randn(batch_size, d_model)  # (1, 512)\n",
    "\n",
    "# Forward pass\n",
    "compressed_k, compressed_v, c_t_KV = compressor(h_t)\n",
    "\n",
    "# During inference, we would only cache c_t_KV\n",
    "kv_cache = [c_t_KV.detach()]\n",
    "\n",
    "print(\"Original hidden state size:\", h_t.shape)\n",
    "print(\"Compressed KV cache size:\", c_t_KV.shape)\n",
    "print(\"Reconstructed keys shape:\", compressed_k.shape)\n",
    "print(\"Reconstructed values shape:\", compressed_v.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.3 Decoupled Rotary Position Embedding\n",
    "\n",
    "Standard RoPE is incompatible with low-rank KV compression as done above. Decoupled RoPE strategy uses additional multi-head queries `q_t` and a shared key `k_t` to carry RoPE. This sums up the complete MLA computation as:\n",
    "\n",
    "$$\n",
    "\\begin{aligned}\n",
    "    [\\mathbf{q}_{t, 1}^{R};\\mathbf{q}_{t, 2}^{R};...;\\mathbf{q}_{t, n_{h}}^{R}] = \\mathbf{q}_{t}^{R} &= \\operatorname{RoPE}({W^{QR}} \\mathbf{c}_{t}^{Q}), \\\\\n",
    "    \\mathbf{k}_{t}^{R} &= \\operatorname{RoPE}({W^{KR}} \\mathbf{h}_{t}), \\\\\n",
    "    \\mathbf{q}_{t, i} &= [\\mathbf{q}_{t, i}^{C}; \\mathbf{q}_{t, i}^{R}], \\\\\n",
    "    \\mathbf{k}_{t, i} &= [\\mathbf{k}_{t, i}^{C}; \\mathbf{k}_{t}^{R}], \\\\\n",
    "    \\mathbf{o}_{t, i} &= \\sum_{j=1}^{t} \\operatorname{Softmax}_j\\left(\\frac{\\mathbf{q}_{t, i}^T \\mathbf{k}_{j, i}}{\\sqrt{d_{h} + d_{h}^{R}}}\\right) \\mathbf{v}_{j, i}^{C}, \\\\ \n",
    "    \\mathbf{u}_{t} &= W^{O} [\\mathbf{o}_{t, 1};\\mathbf{o}_{t, 2};...;\\mathbf{o}_{t, n_{h}}].\n",
    "\\end{aligned}\n",
    "$$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.4 KV Cache"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Simplified generation loop with KV cache\n",
    "def generate(input_ids, max_length=50):\n",
    "    kv_cache = []  # Stores compressed KV states\n",
    "    for _ in range(max_length):\n",
    "        # Forward pass: compute logits and update cache\n",
    "        logits, kv_cache = model(input_ids, kv_cache=kv_cache)\n",
    "        # Sample next token\n",
    "        next_token = sample(logits)\n",
    "        input_ids = torch.cat([input_ids, next_token], dim=-1)\n",
    "    return input_ids"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. DeepSeek-R1 "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.1 Gate Implementation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.2 Cold-start"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.3 Reasoning-Oriented Reinforcement Learning"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
